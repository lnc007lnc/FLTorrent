"""
åŸºäºæ‚¨æä¾›ç®—æ³•çš„æ¨¡å‹åˆ†å—ç®¡ç†ç³»ç»Ÿ
ä½¿ç”¨æ‰å¹³ç´¢å¼•è®°å½•å‚æ•°chunkä¿¡æ¯ï¼ŒæŒ‰èŠ‚ç‚¹åå»ºç«‹æœ¬åœ°æ•°æ®åº“å­˜å‚¨chunk
æ”¯æŒå®æ—¶å˜åŒ–ç›‘æ§å’Œchunkä¿¡æ¯ä¸ŠæŠ¥
"""

import os
import json
import hashlib
import pickle
import sqlite3
import threading
import time
from typing import Dict, List, Optional, Tuple, Any, Callable
import numpy as np
import torch
import torch.nn as nn
from datetime import datetime
import logging

# Import ChunkInfo for change monitoring
from federatedscope.core.chunk_tracker import ChunkInfo, ChunkAction

logger = logging.getLogger(__name__)


class ChunkManager:
    """
    ç»Ÿä¸€ç®¡ç†æ¨¡å‹åˆ†å—é€»è¾‘ï¼Œä½¿ç”¨æ‰å¹³ç´¢å¼•è®°å½•å‚æ•°chunkä¿¡æ¯ã€‚
    æ¯ä¸ªchunkçš„å®šä¹‰æ ¼å¼ä¸ºï¼š
      {
          'chunk_id': int,
          'parts': { key: [ (flat_start, flat_end, shape), ... ] },
          'flat_size': int
      }
    """
    
    def __init__(self, client_id: int, change_callback: Optional[Callable[[ChunkInfo], None]] = None):
        """
        åˆå§‹åŒ–ChunkManagerï¼Œä¸ºæŒ‡å®šå®¢æˆ·ç«¯åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“
        
        Args:
            client_id: å®¢æˆ·ç«¯IDï¼Œç”¨äºåˆ›å»ºèŠ‚ç‚¹ç‰¹å®šçš„æ•°æ®åº“æ–‡ä»¶
            change_callback: æ•°æ®åº“å˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºå‘æœåŠ¡å™¨æŠ¥å‘Šchunkå˜åŒ–
        """
        self.client_id = client_id
        self.change_callback = change_callback
        
        # æŒ‰èŠ‚ç‚¹ååˆ›å»ºæ•°æ®åº“æ–‡ä»¶è·¯å¾„: /tmp/client_X/client_X_chunks.db
        client_name = f"client_{client_id}"
        db_dir = os.path.join(os.getcwd(), "tmp", client_name)
        os.makedirs(db_dir, exist_ok=True)
        
        self.db_path = os.path.join(db_dir, f"{client_name}_chunks.db")
        self._init_database()
        
        # å˜åŒ–ç›‘æ§ç›¸å…³
        self.monitoring_enabled = False
        self.monitoring_thread = None
        self.stop_monitoring = threading.Event()
        self.last_db_mtime = 0
        
        logger.info(f"ğŸ“Š åˆå§‹åŒ–èŠ‚ç‚¹ {client_id} çš„chunkæ•°æ®åº“: {self.db_path}")
        
        # å¦‚æœæä¾›äº†å›è°ƒå‡½æ•°ï¼Œå¯åŠ¨ç›‘æ§
        if change_callback:
            self.start_monitoring()
        
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºchunkå…ƒæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunk_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                round_num INTEGER NOT NULL,
                chunk_id INTEGER NOT NULL,
                chunk_hash TEXT NOT NULL,
                parts_info TEXT NOT NULL,
                flat_size INTEGER NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(round_num, chunk_id)
            )
        ''')
        
        # åˆ›å»ºchunkæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunk_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chunk_hash TEXT UNIQUE NOT NULL,
                data BLOB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_metadata_round 
            ON chunk_metadata(round_num)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_metadata_hash 
            ON chunk_metadata(chunk_hash)
        ''')
        
        conn.commit()
        conn.close()
    
    @staticmethod
    def model_to_params(model: nn.Module) -> Dict[str, np.ndarray]:
        """å°†æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°åŠç¼“å†²åŒºè½¬ä¸º numpy æ•°ç»„"""
        params = {name: param.data.cpu().numpy() for name, param in model.named_parameters()}
        for name, buffer in model.named_buffers():
            params[name] = buffer.data.cpu().numpy()
        return params

    @staticmethod
    def params_to_model(params: Dict[str, np.ndarray], model: nn.Module):
        """å°†å‚æ•°å­—å…¸åŠ è½½å›æ¨¡å‹"""
        for name, param in model.named_parameters():
            if name in params:
                param.data = torch.from_numpy(params[name]).to(param.device)
    
    @staticmethod
    def split_model(params: Dict[str, np.ndarray], num_chunks: int) -> List[Dict]:
        """
        å°†æ¨¡å‹å‚æ•°å‡åŒ€åˆ†å‰²ä¸ºæŒ‡å®šæ•°é‡çš„chunkï¼Œè®°å½•æ¯ä¸ªchunkä¸­å„å‚æ•°çš„æ‰å¹³ç´¢å¼•åŒºé—´ã€‚
        è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼ä¸ºï¼š
          {
              'chunk_id': int,
              'parts': { key: [ (flat_start, flat_end, shape), ... ] },
              'flat_size': int
          }
        """
        total_elements = sum(np.prod(v.shape) for v in params.values())
        elements_per_chunk = total_elements // num_chunks

        chunks = []
        current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
        
        # å¯¹æ¯ä¸ªå‚æ•°ï¼ŒæŒ‰ç…§æ‰å¹³é¡ºåºè¿›è¡Œåˆ‡åˆ†
        for key in sorted(params.keys()):
            arr = params[key]
            n = int(np.prod(arr.shape))
            ptr = 0
            while ptr < n:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°çš„chunk
                if current_chunk['flat_size'] >= elements_per_chunk and len(chunks) < num_chunks - 1:
                    chunks.append(current_chunk)
                    current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
                
                # è®¡ç®—å¯ä»¥æ”¾å…¥å½“å‰chunkçš„å…ƒç´ æ•°é‡
                if len(chunks) < num_chunks - 1:
                    remaining = elements_per_chunk - current_chunk['flat_size']
                    take = min(remaining, n - ptr)
                else:
                    # æœ€åä¸€ä¸ªchunkåŒ…å«æ‰€æœ‰å‰©ä½™å…ƒç´ 
                    take = n - ptr
                    
                # ä¸ºå½“å‰chunkä¸­å‚æ•° key æ·»åŠ è¿™ä¸€æ®µä¿¡æ¯
                if key not in current_chunk['parts']:
                    current_chunk['parts'][key] = []
                current_chunk['parts'][key].append((int(ptr), int(ptr + take), arr.shape))
                current_chunk['flat_size'] += take
                ptr += take
                
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk['flat_size'] > 0:
            chunks.append(current_chunk)
            
        return chunks
    
    def extract_chunk_data(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> np.ndarray:
        """
        æ ¹æ®chunkä¿¡æ¯ä»æ¨¡å‹å‚æ•°ä¸­æå–å¯¹åº”çš„æ•°æ®
        
        Args:
            params: æ¨¡å‹å‚æ•°å­—å…¸
            chunk_info: chunkçš„å…ƒæ•°æ®ä¿¡æ¯
            
        Returns:
            æ‰å¹³åŒ–çš„chunkæ•°æ®æ•°ç»„
        """
        chunk_data = []
        
        for key, parts in chunk_info['parts'].items():
            if key not in params:
                logger.warning(f"âš ï¸ å‚æ•° {key} åœ¨æ¨¡å‹ä¸­æœªæ‰¾åˆ°")
                continue
                
            arr_flat = params[key].flatten()
            for flat_start, flat_end, shape in parts:
                chunk_data.append(arr_flat[flat_start:flat_end])
                
        if chunk_data:
            return np.concatenate(chunk_data)
        else:
            return np.array([])
    
    def save_model_chunks(self, model: nn.Module, round_num: int, num_chunks: int = 10) -> List[str]:
        """
        å°†æ¨¡å‹åˆ†å‰²æˆchunkså¹¶ä¿å­˜åˆ°èŠ‚ç‚¹ç‰¹å®šçš„æ•°æ®åº“
        
        Args:
            model: PyTorchæ¨¡å‹
            round_num: è®­ç»ƒè½®æ¬¡
            num_chunks: åˆ†å‰²çš„chunkæ•°é‡
            
        Returns:
            ä¿å­˜çš„chunkå“ˆå¸Œåˆ—è¡¨
        """
        try:
            # å°†æ¨¡å‹è½¬æ¢ä¸ºå‚æ•°å­—å…¸
            params = self.model_to_params(model)
            
            # åˆ†å‰²æ¨¡å‹
            chunks_info = self.split_model(params, num_chunks)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            saved_hashes = []
            
            for chunk_info in chunks_info:
                # æå–chunkæ•°æ®
                chunk_data = self.extract_chunk_data(params, chunk_info)
                
                # è®¡ç®—chunkå“ˆå¸Œ
                chunk_bytes = pickle.dumps(chunk_data)
                chunk_hash = hashlib.sha256(chunk_bytes).hexdigest()
                
                # ä¿å­˜chunkæ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                cursor.execute(
                    "INSERT OR IGNORE INTO chunk_data (chunk_hash, data) VALUES (?, ?)",
                    (chunk_hash, chunk_bytes)
                )
                
                # ä¿å­˜chunkå…ƒæ•°æ®
                parts_json = json.dumps(chunk_info['parts'])
                cursor.execute('''
                    INSERT OR REPLACE INTO chunk_metadata 
                    (round_num, chunk_id, chunk_hash, parts_info, flat_size)
                    VALUES (?, ?, ?, ?, ?)
                ''', (round_num, chunk_info['chunk_id'], chunk_hash, 
                     parts_json, chunk_info['flat_size']))
                
                saved_hashes.append(chunk_hash)
                
                # æŠ¥å‘Šchunkå˜åŒ–
                if self.change_callback:
                    self.report_chunk_change(
                        round_num=round_num,
                        chunk_id=chunk_info['chunk_id'],
                        action=ChunkAction.ADD.value,
                        chunk_hash=chunk_hash,
                        chunk_size=chunk_info['flat_size']
                    )
                
            conn.commit()
            conn.close()
            
            logger.info(f"ğŸ’¾ èŠ‚ç‚¹ {self.client_id}: ç¬¬{round_num}è½®ä¿å­˜äº† {len(saved_hashes)} ä¸ªchunks")
            return saved_hashes
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¨¡å‹chunkså¤±è´¥: {e}")
            return []
    
    def load_chunks_by_round(self, round_num: int) -> List[Tuple[Dict, np.ndarray]]:
        """
        åŠ è½½æŒ‡å®šè½®æ¬¡çš„æ‰€æœ‰chunks
        
        Args:
            round_num: è®­ç»ƒè½®æ¬¡
            
        Returns:
            (chunk_info, chunk_data) å…ƒç»„åˆ—è¡¨
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æŸ¥è¯¢chunkå…ƒæ•°æ®
            cursor.execute('''
                SELECT chunk_id, chunk_hash, parts_info, flat_size
                FROM chunk_metadata
                WHERE round_num = ?
                ORDER BY chunk_id
            ''', (round_num,))
            
            metadata_rows = cursor.fetchall()
            
            chunks = []
            for chunk_id, chunk_hash, parts_json, flat_size in metadata_rows:
                # åŠ è½½chunkæ•°æ®
                cursor.execute(
                    "SELECT data FROM chunk_data WHERE chunk_hash = ?",
                    (chunk_hash,)
                )
                data_row = cursor.fetchone()
                
                if data_row:
                    chunk_data = pickle.loads(data_row[0])
                    chunk_info = {
                        'chunk_id': chunk_id,
                        'parts': json.loads(parts_json),
                        'flat_size': flat_size
                    }
                    chunks.append((chunk_info, chunk_data))
                    
            conn.close()
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½chunkså¤±è´¥: {e}")
            return []
    
    def get_chunk_by_id(self, round_num: int, chunk_id: int) -> Optional[Tuple[Dict, np.ndarray]]:
        """
        è·å–æŒ‡å®šè½®æ¬¡å’Œchunk_idçš„chunkæ•°æ®
        
        Args:
            round_num: è®­ç»ƒè½®æ¬¡
            chunk_id: chunk ID
            
        Returns:
            (chunk_info, chunk_data) å…ƒç»„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT chunk_hash, parts_info, flat_size
                FROM chunk_metadata
                WHERE round_num = ? AND chunk_id = ?
            ''', (round_num, chunk_id))
            
            row = cursor.fetchone()
            if row:
                chunk_hash, parts_json, flat_size = row
                
                # åŠ è½½chunkæ•°æ®
                cursor.execute(
                    "SELECT data FROM chunk_data WHERE chunk_hash = ?",
                    (chunk_hash,)
                )
                data_row = cursor.fetchone()
                
                if data_row:
                    chunk_data = pickle.loads(data_row[0])
                    chunk_info = {
                        'chunk_id': chunk_id,
                        'parts': json.loads(parts_json),
                        'flat_size': flat_size
                    }
                    conn.close()
                    return (chunk_info, chunk_data)
                    
            conn.close()
            return None
            
        except Exception as e:
            logger.error(f"âŒ è·å–chunkå¤±è´¥: {e}")
            return None
    
    def get_storage_stats(self) -> Dict:
        """è·å–æ•°æ®åº“å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç»Ÿè®¡chunkå…ƒæ•°æ®
            cursor.execute("SELECT COUNT(*) FROM chunk_metadata")
            total_metadata = cursor.fetchone()[0]
            
            # ç»Ÿè®¡chunkæ•°æ®
            cursor.execute("SELECT COUNT(*) FROM chunk_data")
            total_chunks = cursor.fetchone()[0]
            
            # ç»Ÿè®¡å­˜å‚¨å¤§å°
            cursor.execute("SELECT SUM(LENGTH(data)) FROM chunk_data")
            total_size = cursor.fetchone()[0] or 0
            
            # ç»Ÿè®¡è½®æ¬¡èŒƒå›´
            cursor.execute("SELECT MIN(round_num), MAX(round_num) FROM chunk_metadata")
            min_round, max_round = cursor.fetchone()
            
            conn.close()
            
            return {
                'client_id': self.client_id,
                'db_path': self.db_path,
                'total_metadata_entries': total_metadata,
                'unique_chunks': total_chunks,
                'storage_size_bytes': total_size,
                'storage_size_mb': total_size / (1024 * 1024) if total_size else 0,
                'round_range': (min_round, max_round) if min_round is not None else (None, None)
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def cleanup_old_rounds(self, keep_rounds: int = 5):
        """
        æ¸…ç†æ—§è½®æ¬¡çš„chunksï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ è½®
        
        Args:
            keep_rounds: ä¿ç•™æœ€è¿‘å‡ è½®çš„æ•°æ®
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ‰¾åˆ°è¦ä¿ç•™çš„æœ€å°è½®æ¬¡
            cursor.execute("SELECT MAX(round_num) FROM chunk_metadata")
            max_round = cursor.fetchone()[0]
            
            if max_round is not None:
                min_keep_round = max_round - keep_rounds + 1
                
                # åˆ é™¤æ—§çš„å…ƒæ•°æ®
                cursor.execute(
                    "DELETE FROM chunk_metadata WHERE round_num < ?",
                    (min_keep_round,)
                )
                
                # åˆ é™¤ä¸å†è¢«å¼•ç”¨çš„chunkæ•°æ®
                cursor.execute('''
                    DELETE FROM chunk_data 
                    WHERE chunk_hash NOT IN (
                        SELECT DISTINCT chunk_hash FROM chunk_metadata
                    )
                ''')
                
                conn.commit()
                logger.info(f"ğŸ§¹ èŠ‚ç‚¹ {self.client_id}: æ¸…ç†äº†ç¬¬{min_keep_round}è½®ä¹‹å‰çš„chunks")
                
            conn.close()
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§chunkså¤±è´¥: {e}")
    
    def reconstruct_model_from_chunks(self, round_num: int, target_model: nn.Module) -> bool:
        """
        ä»chunksé‡æ„æ¨¡å‹å‚æ•°
        
        Args:
            round_num: è¦é‡æ„çš„è½®æ¬¡
            target_model: ç›®æ ‡æ¨¡å‹ï¼Œå‚æ•°å°†è¢«é‡æ„çš„å€¼æ›¿æ¢
            
        Returns:
            æ˜¯å¦é‡æ„æˆåŠŸ
        """
        try:
            chunks = self.load_chunks_by_round(round_num)
            if not chunks:
                logger.warning(f"âš ï¸ ç¬¬{round_num}è½®æ²¡æœ‰æ‰¾åˆ°chunks")
                return False
            
            # è·å–ç›®æ ‡æ¨¡å‹çš„å‚æ•°å½¢çŠ¶ä¿¡æ¯
            model_params = self.model_to_params(target_model)
            
            # åˆå§‹åŒ–é‡æ„å‚æ•°å­—å…¸
            reconstructed_params = {}
            for param_name, param_array in model_params.items():
                reconstructed_params[param_name] = np.zeros_like(param_array)
            
            # ä»chunksé‡æ„å‚æ•°
            for chunk_info, chunk_data in chunks:
                data_ptr = 0
                
                for param_name, parts in chunk_info['parts'].items():
                    if param_name not in reconstructed_params:
                        logger.warning(f"âš ï¸ å‚æ•° {param_name} åœ¨ç›®æ ‡æ¨¡å‹ä¸­ä¸å­˜åœ¨")
                        continue
                        
                    for flat_start, flat_end, shape in parts:
                        chunk_size = flat_end - flat_start
                        part_data = chunk_data[data_ptr:data_ptr + chunk_size]
                        
                        # ç›´æ¥å¯¹æ‰å¹³åŒ–çš„å‚æ•°æ•°ç»„è¿›è¡Œèµ‹å€¼
                        param_flat = reconstructed_params[param_name].reshape(-1)
                        param_flat[flat_start:flat_end] = part_data
                        
                        data_ptr += chunk_size
            
            # å°†é‡æ„çš„å‚æ•°åŠ è½½åˆ°æ¨¡å‹
            self.params_to_model(reconstructed_params, target_model)
            
            logger.info(f"ğŸ“¦ èŠ‚ç‚¹ {self.client_id}: æˆåŠŸä»chunksé‡æ„ç¬¬{round_num}è½®çš„æ¨¡å‹")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‡æ„æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return False
    
    def start_monitoring(self):
        """å¯åŠ¨æ•°æ®åº“å˜åŒ–ç›‘æ§"""
        if self.monitoring_enabled:
            logger.warning(f"âš ï¸ èŠ‚ç‚¹ {self.client_id}: ç›‘æ§å·²ç»å¯åŠ¨")
            return
            
        self.monitoring_enabled = True
        self.stop_monitoring.clear()
        self.last_db_mtime = self._get_db_mtime()
        
        self.monitoring_thread = threading.Thread(
            target=self._monitor_database_changes,
            daemon=True,
            name=f"ChunkMonitor-{self.client_id}"
        )
        self.monitoring_thread.start()
        
        logger.info(f"ğŸ” èŠ‚ç‚¹ {self.client_id}: å¯åŠ¨chunkæ•°æ®åº“å˜åŒ–ç›‘æ§")
    
    def stop_monitoring_thread(self):
        """åœæ­¢æ•°æ®åº“å˜åŒ–ç›‘æ§"""
        if not self.monitoring_enabled:
            return
            
        self.monitoring_enabled = False
        self.stop_monitoring.set()
        
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=2.0)
            
        logger.info(f"ğŸ›‘ èŠ‚ç‚¹ {self.client_id}: åœæ­¢chunkæ•°æ®åº“å˜åŒ–ç›‘æ§")
    
    def _get_db_mtime(self) -> float:
        """è·å–æ•°æ®åº“æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´"""
        try:
            return os.path.getmtime(self.db_path) if os.path.exists(self.db_path) else 0
        except OSError:
            return 0
    
    def _monitor_database_changes(self):
        """ç›‘æ§æ•°æ®åº“å˜åŒ–çš„åå°çº¿ç¨‹"""
        logger.debug(f"ğŸ” èŠ‚ç‚¹ {self.client_id}: å¼€å§‹ç›‘æ§æ•°æ®åº“å˜åŒ–")
        
        while not self.stop_monitoring.is_set():
            try:
                current_mtime = self._get_db_mtime()
                
                if current_mtime > self.last_db_mtime:
                    # æ•°æ®åº“å‘ç”Ÿå˜åŒ–ï¼Œæ£€æµ‹å…·ä½“å˜åŒ–
                    self._detect_and_report_changes()
                    self.last_db_mtime = current_mtime
                
                # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                self.stop_monitoring.wait(1.0)
                
            except Exception as e:
                logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: ç›‘æ§æ•°æ®åº“å˜åŒ–å¤±è´¥: {e}")
                self.stop_monitoring.wait(5.0)  # é”™è¯¯åç­‰å¾…5ç§’å†é‡è¯•
        
        logger.debug(f"ğŸ” èŠ‚ç‚¹ {self.client_id}: æ•°æ®åº“å˜åŒ–ç›‘æ§çº¿ç¨‹é€€å‡º")
    
    def _detect_and_report_changes(self):
        """æ£€æµ‹å¹¶æŠ¥å‘Šæ•°æ®åº“å˜åŒ–"""
        if not self.change_callback:
            return
            
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # è·å–æœ€è¿‘æ·»åŠ çš„chunkä¿¡æ¯ï¼ˆåŸºäºåˆ›å»ºæ—¶é—´ï¼‰
            cursor.execute('''
                SELECT round_num, chunk_id, chunk_hash, flat_size, created_at
                FROM chunk_metadata
                ORDER BY created_at DESC
                LIMIT 10
            ''')
            
            recent_chunks = cursor.fetchall()
            conn.close()
            
            # æŠ¥å‘Šæœ€è¿‘çš„å˜åŒ–
            for round_num, chunk_id, chunk_hash, flat_size, created_at in recent_chunks:
                chunk_info = ChunkInfo(
                    client_id=self.client_id,
                    round_num=round_num,
                    chunk_id=chunk_id,
                    action=ChunkAction.ADD.value,
                    chunk_hash=chunk_hash,
                    chunk_size=flat_size,
                    timestamp=time.time()
                )
                
                # è°ƒç”¨å›è°ƒå‡½æ•°æŠ¥å‘Šå˜åŒ–
                try:
                    self.change_callback(chunk_info)
                    logger.debug(f"ğŸ“¤ èŠ‚ç‚¹ {self.client_id}: æŠ¥å‘Šchunkå˜åŒ– - è½®æ¬¡{round_num}, chunk{chunk_id}")
                except Exception as e:
                    logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: æŠ¥å‘Šchunkå˜åŒ–å¤±è´¥: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: æ£€æµ‹æ•°æ®åº“å˜åŒ–å¤±è´¥: {e}")
    
    def report_chunk_change(self, round_num: int, chunk_id: int, action: str, chunk_hash: str, chunk_size: int):
        """æ‰‹åŠ¨æŠ¥å‘Šchunkå˜åŒ–"""
        if not self.change_callback:
            return
            
        chunk_info = ChunkInfo(
            client_id=self.client_id,
            round_num=round_num,
            chunk_id=chunk_id,
            action=action,
            chunk_hash=chunk_hash,
            chunk_size=chunk_size,
            timestamp=time.time()
        )
        
        try:
            self.change_callback(chunk_info)
            logger.debug(f"ğŸ“¤ èŠ‚ç‚¹ {self.client_id}: æ‰‹åŠ¨æŠ¥å‘Šchunkå˜åŒ– - {action} è½®æ¬¡{round_num}, chunk{chunk_id}")
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: æ‰‹åŠ¨æŠ¥å‘Šchunkå˜åŒ–å¤±è´¥: {e}")
    
    def set_change_callback(self, callback: Callable[[ChunkInfo], None]):
        """è®¾ç½®å˜åŒ–å›è°ƒå‡½æ•°"""
        self.change_callback = callback
        
        # å¦‚æœç›‘æ§æœªå¯åŠ¨ä¸”è®¾ç½®äº†å›è°ƒï¼Œå¯åŠ¨ç›‘æ§
        if callback and not self.monitoring_enabled:
            self.start_monitoring()
        # å¦‚æœå–æ¶ˆå›è°ƒï¼Œåœæ­¢ç›‘æ§
        elif not callback and self.monitoring_enabled:
            self.stop_monitoring_thread()
            
        logger.info(f"ğŸ”„ èŠ‚ç‚¹ {self.client_id}: æ›´æ–°å˜åŒ–å›è°ƒå‡½æ•°")
    
    def get_all_chunks_info(self) -> List[ChunkInfo]:
        """è·å–æ‰€æœ‰chunkä¿¡æ¯ç”¨äºåˆå§‹åŒ–æŠ¥å‘Š"""
        chunk_infos = []
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT round_num, chunk_id, chunk_hash, flat_size, created_at
                FROM chunk_metadata
                ORDER BY round_num, chunk_id
            ''')
            
            rows = cursor.fetchall()
            conn.close()
            
            for round_num, chunk_id, chunk_hash, flat_size, created_at in rows:
                chunk_info = ChunkInfo(
                    client_id=self.client_id,
                    round_num=round_num,
                    chunk_id=chunk_id,
                    action=ChunkAction.ADD.value,
                    chunk_hash=chunk_hash,
                    chunk_size=flat_size,
                    timestamp=time.time()
                )
                chunk_infos.append(chunk_info)
                
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: è·å–æ‰€æœ‰chunkä¿¡æ¯å¤±è´¥: {e}")
            
        return chunk_infos