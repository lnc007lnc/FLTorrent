"""
åŸºäºæ‚¨æä¾›ç®—æ³•çš„æ¨¡å‹åˆ†å—ç®¡ç†ç³»ç»Ÿ
ä½¿ç”¨æ‰å¹³ç´¢å¼•è®°å½•å‚æ•°chunkä¿¡æ¯ï¼ŒæŒ‰èŠ‚ç‚¹åå»ºç«‹æœ¬åœ°æ•°æ®åº“å­˜å‚¨chunk
æ”¯æŒå®æ—¶å˜åŒ–ç›‘æ§å’Œchunkä¿¡æ¯ä¸ŠæŠ¥
"""

import os
import json
import hashlib
import pickle
import sqlite3
import time
import threading
import time
from typing import Dict, List, Optional, Tuple, Any, Callable
import numpy as np
import torch
import torch.nn as nn
from datetime import datetime
import logging

# Import ChunkInfo for change monitoring
from federatedscope.core.chunk_tracker import ChunkInfo, ChunkAction

logger = logging.getLogger(__name__)


class ChunkManager:
    """
    ç»Ÿä¸€ç®¡ç†æ¨¡å‹åˆ†å—é€»è¾‘ï¼Œä½¿ç”¨æ‰å¹³ç´¢å¼•è®°å½•å‚æ•°chunkä¿¡æ¯ã€‚
    æ¯ä¸ªchunkçš„å®šä¹‰æ ¼å¼ä¸ºï¼š
      {
          'chunk_id': int,
          'parts': { key: [ (flat_start, flat_end, shape), ... ] },
          'flat_size': int
      }
    """
    
    def __init__(self, client_id: int, change_callback: Optional[Callable[[ChunkInfo], None]] = None):
        """
        åˆå§‹åŒ–ChunkManagerï¼Œä¸ºæŒ‡å®šå®¢æˆ·ç«¯åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“
        
        Args:
            client_id: å®¢æˆ·ç«¯IDï¼Œç”¨äºåˆ›å»ºèŠ‚ç‚¹ç‰¹å®šçš„æ•°æ®åº“æ–‡ä»¶
            change_callback: æ•°æ®åº“å˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºå‘æœåŠ¡å™¨æŠ¥å‘Šchunkå˜åŒ–
        """
        self.client_id = client_id
        self.change_callback = change_callback
        
        # æŒ‰èŠ‚ç‚¹ååˆ›å»ºæ•°æ®åº“æ–‡ä»¶è·¯å¾„: /tmp/client_X/client_X_chunks.db
        client_name = f"client_{client_id}"
        db_dir = os.path.join(os.getcwd(), "tmp", client_name)
        os.makedirs(db_dir, exist_ok=True)
        
        self.db_path = os.path.join(db_dir, f"{client_name}_chunks.db")
        self._init_database()
        
        # å˜åŒ–ç›‘æ§ç›¸å…³
        self.monitoring_enabled = False
        self.monitoring_thread = None
        self.stop_monitoring = threading.Event()
        self.last_db_mtime = 0
        
        logger.info(f"ğŸ“Š åˆå§‹åŒ–èŠ‚ç‚¹ {client_id} çš„chunkæ•°æ®åº“: {self.db_path}")
        
        # å¦‚æœæä¾›äº†å›è°ƒå‡½æ•°ï¼Œå¯åŠ¨ç›‘æ§
        if change_callback:
            self.start_monitoring()
        
    def _get_optimized_connection(self):
        """è·å–ä¼˜åŒ–çš„æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(self.db_path, timeout=30.0, check_same_thread=False)
        cursor = conn.cursor()
        
        # å¯ç”¨ä¼˜åŒ–è®¾ç½®
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL") 
        cursor.execute("PRAGMA cache_size=10000")
        cursor.execute("PRAGMA temp_store=MEMORY")
        cursor.execute("PRAGMA busy_timeout=30000")  # 30ç§’è¶…æ—¶
        
        return conn
        
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“è¡¨ç»“æ„"""
        conn = self._get_optimized_connection()
        cursor = conn.cursor()
        
        # åˆ›å»ºchunkå…ƒæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunk_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                round_num INTEGER NOT NULL,
                chunk_id INTEGER NOT NULL,
                chunk_hash TEXT NOT NULL,
                parts_info TEXT NOT NULL,
                flat_size INTEGER NOT NULL,
                importance_score REAL DEFAULT 0.0,
                pruning_method TEXT DEFAULT 'magnitude',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(round_num, chunk_id)
            )
        ''')
        
        # å‡çº§ç°æœ‰è¡¨ç»“æ„ï¼ˆå¦‚æœéœ€è¦ï¼‰
        try:
            cursor.execute("ALTER TABLE chunk_metadata ADD COLUMN importance_score REAL DEFAULT 0.0")
            logger.info("[ChunkManager] Added importance_score column to chunk_metadata table")
        except sqlite3.OperationalError:
            # åˆ—å·²å­˜åœ¨ï¼Œå¿½ç•¥
            pass
            
        try:
            cursor.execute("ALTER TABLE chunk_metadata ADD COLUMN pruning_method TEXT DEFAULT 'magnitude'")
            logger.info("[ChunkManager] Added pruning_method column to chunk_metadata table")
        except sqlite3.OperationalError:
            # åˆ—å·²å­˜åœ¨ï¼Œå¿½ç•¥
            pass
        
        # åˆ›å»ºchunkæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunk_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chunk_hash TEXT UNIQUE NOT NULL,
                data BLOB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_metadata_round 
            ON chunk_metadata(round_num)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_metadata_hash 
            ON chunk_metadata(chunk_hash)
        ''')
        
        conn.commit()
        conn.close()
    
    @staticmethod
    def model_to_params(model: nn.Module) -> Dict[str, np.ndarray]:
        """å°†æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°åŠç¼“å†²åŒºè½¬ä¸º numpy æ•°ç»„"""
        params = {name: param.data.cpu().numpy() for name, param in model.named_parameters()}
        for name, buffer in model.named_buffers():
            params[name] = buffer.data.cpu().numpy()
        return params

    @staticmethod
    def params_to_model(params: Dict[str, np.ndarray], model: nn.Module):
        """å°†å‚æ•°å­—å…¸åŠ è½½å›æ¨¡å‹"""
        for name, param in model.named_parameters():
            if name in params:
                param.data = torch.from_numpy(params[name]).to(param.device)
    
    def compute_chunk_importance(self, params: Dict[str, np.ndarray], chunks_info: List[Dict], 
                                method: str = 'magnitude') -> List[float]:
        """
        è®¡ç®—æ¯ä¸ªchunkçš„é‡è¦åº¦åˆ†æ•°
        
        Args:
            params: æ¨¡å‹å‚æ•°å­—å…¸
            chunks_info: chunkä¿¡æ¯åˆ—è¡¨
            method: é‡è¦åº¦è®¡ç®—æ–¹æ³• ('magnitude', 'l2_norm', 'gradient_norm', 'snip')
            
        Returns:
            List[float]: æ¯ä¸ªchunkçš„é‡è¦åº¦åˆ†æ•°
        """
        importance_scores = []
        
        for chunk_info in chunks_info:
            if method == 'magnitude':
                score = self._compute_magnitude_importance(params, chunk_info)
            elif method == 'l2_norm':
                score = self._compute_l2_norm_importance(params, chunk_info)
            elif method == 'gradient_norm':
                score = self._compute_gradient_norm_importance(params, chunk_info)
            elif method == 'snip':
                score = self._compute_snip_importance(params, chunk_info)
            elif method == 'fisher':
                score = self._compute_fisher_importance(params, chunk_info)
            else:
                logger.warning(f"[ChunkManager] Unknown importance method: {method}, using magnitude")
                score = self._compute_magnitude_importance(params, chunk_info)
                
            importance_scores.append(float(score))
            
        # ä½¿ç”¨L1å½’ä¸€åŒ–ï¼ˆæ€»å’Œå½’ä¸€åŒ–ï¼‰ï¼Œä¿æŒåŸå§‹æ¯”ä¾‹å…³ç³»
        if importance_scores:
            total_score = sum(importance_scores)
            if total_score > 0:
                importance_scores = [s / total_score for s in importance_scores]
            else:
                importance_scores = [1.0 / len(importance_scores)] * len(importance_scores)  # å¹³å‡åˆ†é…
                
        logger.info(f"[ChunkManager] Computed chunk importance scores: {[f'{s:.4f}' for s in importance_scores]}")
        return importance_scores
    
    def _compute_magnitude_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºå‚æ•°å¹…åº¦çš„é‡è¦åº¦è®¡ç®—"""
        total_magnitude = 0.0
        total_elements = 0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    total_magnitude += np.sum(np.abs(chunk_slice))
                    total_elements += len(chunk_slice)
        
        return total_magnitude / max(total_elements, 1)
    
    def _compute_l2_norm_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºL2èŒƒæ•°çš„é‡è¦åº¦è®¡ç®—"""
        total_l2_norm = 0.0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    total_l2_norm += np.linalg.norm(chunk_slice) ** 2
        
        return np.sqrt(total_l2_norm)
    
    def _compute_gradient_norm_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºæ¢¯åº¦èŒƒæ•°çš„é‡è¦åº¦è®¡ç®—ï¼ˆéœ€è¦æ¢¯åº¦ä¿¡æ¯ï¼‰"""
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ¢¯åº¦ä¿¡æ¯
        # ä½œä¸ºfallbackä½¿ç”¨magnitudeæ–¹æ³•
        return self._compute_magnitude_importance(params, chunk_info)
    
    def _compute_snip_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºSNIP (Single-shot Network Pruning)çš„é‡è¦åº¦è®¡ç®—"""
        # æ”¹è¿›çš„SNIPå®ç°ï¼šè€ƒè™‘å‚æ•°å±‚çº§é‡è¦æ€§
        total_snip_score = 0.0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                
                # æ ¹æ®å‚æ•°ç±»å‹è®¾ç½®æƒé‡å› å­
                layer_weight = 1.0
                if 'weight' in param_name:
                    layer_weight = 2.0  # æƒé‡æ¯”åç½®æ›´é‡è¦
                if 'fc' in param_name or '4.' in param_name:  # è¾“å‡ºå±‚
                    layer_weight *= 1.5  # è¾“å‡ºå±‚æ›´é‡è¦
                
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    if len(chunk_slice) > 0:
                        # è®¡ç®—å‚æ•°çš„æ•æ„Ÿåº¦æŒ‡æ ‡
                        abs_values = np.abs(chunk_slice)
                        
                        # 1. å¤§å¹…åº¦å‚æ•°çš„é‡è¦æ€§
                        magnitude_score = np.sum(abs_values)
                        
                        # 2. å‚æ•°åˆ†æ•£ç¨‹åº¦ï¼ˆæ–¹å·®ï¼‰
                        variance_score = np.var(abs_values) + 1e-8
                        
                        # 3. éé›¶å‚æ•°æ¯”ä¾‹ï¼ˆç¨€ç–æ€§è€ƒè™‘ï¼‰
                        non_zero_ratio = np.count_nonzero(abs_values) / len(abs_values)
                        
                        # SNIPç»¼åˆè¯„åˆ†ï¼šç»“åˆå¹…åº¦ã€æ–¹å·®å’Œç¨€ç–æ€§
                        chunk_score = magnitude_score * (1 + np.sqrt(variance_score)) * (0.5 + non_zero_ratio)
                        total_snip_score += chunk_score * layer_weight
        
        return total_snip_score
    
    def _compute_fisher_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºFisherä¿¡æ¯çŸ©é˜µçš„é‡è¦åº¦è®¡ç®—"""
        # Fisherä¿¡æ¯çŸ©é˜µçš„ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å‚æ•°æ–¹å·®ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
        total_variance = 0.0
        total_chunks = 0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    if len(chunk_slice) > 1:
                        total_variance += np.var(chunk_slice)
                        total_chunks += 1
        
        return total_variance / max(total_chunks, 1)
    
    def get_chunk_importance_scores(self, round_num: int) -> Dict[int, Dict]:
        """
        è·å–æŒ‡å®šè½®æ¬¡æ‰€æœ‰chunkçš„é‡è¦åº¦åˆ†æ•°
        
        Args:
            round_num: ç›®æ ‡è½®æ¬¡
            
        Returns:
            Dict[chunk_id, {'importance_score': float, 'pruning_method': str, 'flat_size': int}]
        """
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT chunk_id, importance_score, pruning_method, flat_size, chunk_hash
                FROM chunk_metadata 
                WHERE round_num = ?
                ORDER BY chunk_id
            ''', (round_num,))
            
            results = cursor.fetchall()
            conn.close()
            
            chunk_scores = {}
            for chunk_id, importance_score, pruning_method, flat_size, chunk_hash in results:
                chunk_scores[chunk_id] = {
                    'importance_score': float(importance_score) if importance_score is not None else 0.0,
                    'pruning_method': pruning_method or 'unknown',
                    'flat_size': flat_size,
                    'chunk_hash': chunk_hash[:8] + '...'  # æ˜¾ç¤ºç®€çŸ­hash
                }
            
            return chunk_scores
            
        except Exception as e:
            logger.error(f"[ChunkManager] Failed to get chunk importance scores for round {round_num}: {e}")
            return {}

    @staticmethod
    def split_model(params: Dict[str, np.ndarray], num_chunks: int) -> List[Dict]:
        """
        å°†æ¨¡å‹å‚æ•°å‡åŒ€åˆ†å‰²ä¸ºæŒ‡å®šæ•°é‡çš„chunkï¼Œè®°å½•æ¯ä¸ªchunkä¸­å„å‚æ•°çš„æ‰å¹³ç´¢å¼•åŒºé—´ã€‚
        è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼ä¸ºï¼š
          {
              'chunk_id': int,
              'parts': { key: [ (flat_start, flat_end, shape), ... ] },
              'flat_size': int
          }
        """
        total_elements = sum(np.prod(v.shape) for v in params.values())
        elements_per_chunk = total_elements // num_chunks

        chunks = []
        current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
        
        # å¯¹æ¯ä¸ªå‚æ•°ï¼ŒæŒ‰ç…§æ‰å¹³é¡ºåºè¿›è¡Œåˆ‡åˆ†
        for key in sorted(params.keys()):
            arr = params[key]
            n = int(np.prod(arr.shape))
            ptr = 0
            while ptr < n:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°çš„chunk
                if current_chunk['flat_size'] >= elements_per_chunk and len(chunks) < num_chunks - 1:
                    chunks.append(current_chunk)
                    current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
                
                # è®¡ç®—å¯ä»¥æ”¾å…¥å½“å‰chunkçš„å…ƒç´ æ•°é‡
                if len(chunks) < num_chunks - 1:
                    remaining = elements_per_chunk - current_chunk['flat_size']
                    take = min(remaining, n - ptr)
                else:
                    # æœ€åä¸€ä¸ªchunkåŒ…å«æ‰€æœ‰å‰©ä½™å…ƒç´ 
                    take = n - ptr
                    
                # ä¸ºå½“å‰chunkä¸­å‚æ•° key æ·»åŠ è¿™ä¸€æ®µä¿¡æ¯
                if key not in current_chunk['parts']:
                    current_chunk['parts'][key] = []
                current_chunk['parts'][key].append((int(ptr), int(ptr + take), arr.shape))
                current_chunk['flat_size'] += take
                ptr += take
                
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk['flat_size'] > 0:
            chunks.append(current_chunk)
            
        return chunks
    
    def extract_chunk_data(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> np.ndarray:
        """
        æ ¹æ®chunkä¿¡æ¯ä»æ¨¡å‹å‚æ•°ä¸­æå–å¯¹åº”çš„æ•°æ®
        
        Args:
            params: æ¨¡å‹å‚æ•°å­—å…¸
            chunk_info: chunkçš„å…ƒæ•°æ®ä¿¡æ¯
            
        Returns:
            æ‰å¹³åŒ–çš„chunkæ•°æ®æ•°ç»„
        """
        chunk_data = []
        
        for key, parts in chunk_info['parts'].items():
            if key not in params:
                logger.warning(f"âš ï¸ å‚æ•° {key} åœ¨æ¨¡å‹ä¸­æœªæ‰¾åˆ°")
                continue
                
            arr_flat = params[key].flatten()
            for flat_start, flat_end, shape in parts:
                chunk_data.append(arr_flat[flat_start:flat_end])
                
        if chunk_data:
            return np.concatenate(chunk_data)
        else:
            return np.array([])
    
    def save_model_chunks(self, model: nn.Module, round_num: int, num_chunks: int = 10, keep_rounds: int = 2, 
                         importance_method: str = 'magnitude') -> List[str]:
        """
        å°†æ¨¡å‹åˆ†å‰²æˆchunkså¹¶ä¿å­˜åˆ°èŠ‚ç‚¹ç‰¹å®šçš„æ•°æ®åº“
        
        Args:
            model: PyTorchæ¨¡å‹
            round_num: è®­ç»ƒè½®æ¬¡
            num_chunks: åˆ†å‰²çš„chunkæ•°é‡
            keep_rounds: ä¿ç•™æœ€è¿‘å‡ è½®çš„æ•°æ®ï¼Œé»˜è®¤2è½®
            importance_method: chunké‡è¦åº¦è®¡ç®—æ–¹æ³• ('magnitude', 'l2_norm', 'snip', 'fisher')
            
        Returns:
            ä¿å­˜çš„chunkå“ˆå¸Œåˆ—è¡¨
        """
        try:
            # å°†æ¨¡å‹è½¬æ¢ä¸ºå‚æ•°å­—å…¸
            params = self.model_to_params(model)
            
            # åˆ†å‰²æ¨¡å‹
            chunks_info = self.split_model(params, num_chunks)
            
            # ğŸ§  è®¡ç®—chunké‡è¦åº¦åˆ†æ•°
            logger.info(f"[ChunkManager] Computing chunk importance using method: {importance_method}")
            importance_scores = self.compute_chunk_importance(params, chunks_info, importance_method)
            
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            saved_hashes = []
            
            for i, chunk_info in enumerate(chunks_info):
                # æå–chunkæ•°æ®
                chunk_data = self.extract_chunk_data(params, chunk_info)
                
                # è®¡ç®—chunkå“ˆå¸Œ
                chunk_bytes = pickle.dumps(chunk_data)
                chunk_hash = hashlib.sha256(chunk_bytes).hexdigest()
                
                # ä¿å­˜chunkæ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                cursor.execute(
                    "INSERT OR IGNORE INTO chunk_data (chunk_hash, data) VALUES (?, ?)",
                    (chunk_hash, chunk_bytes)
                )
                
                # ä¿å­˜chunkå…ƒæ•°æ®ï¼ˆåŒ…å«é‡è¦åº¦åˆ†æ•°ï¼‰
                parts_json = json.dumps(chunk_info['parts'])
                importance_score = importance_scores[i] if i < len(importance_scores) else 0.0
                
                cursor.execute('''
                    INSERT OR REPLACE INTO chunk_metadata 
                    (round_num, chunk_id, chunk_hash, parts_info, flat_size, importance_score, pruning_method)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (round_num, chunk_info['chunk_id'], chunk_hash, 
                     parts_json, chunk_info['flat_size'], importance_score, importance_method))
                
                saved_hashes.append(chunk_hash)
                
                # æŠ¥å‘Šchunkå˜åŒ–
                if self.change_callback:
                    self.report_chunk_change(
                        round_num=round_num,
                        chunk_id=chunk_info['chunk_id'],
                        action=ChunkAction.ADD.value,
                        chunk_hash=chunk_hash,
                        chunk_size=chunk_info['flat_size']
                    )
                
            conn.commit()
            conn.close()
            
            # è‡ªåŠ¨æ¸…ç†æ—§è½®æ¬¡æ•°æ®ï¼Œä¿ç•™æœ€è¿‘å‡ è½®
            self.cleanup_old_rounds(keep_rounds=keep_rounds)
            
            logger.debug(f"ğŸ’¾ èŠ‚ç‚¹ {self.client_id}: ç¬¬{round_num}è½®ä¿å­˜äº† {len(saved_hashes)} ä¸ªchunks")
            return saved_hashes
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¨¡å‹chunkså¤±è´¥: {e}")
            return []
    
    def load_chunks_by_round(self, round_num: int) -> List[Tuple[Dict, np.ndarray]]:
        """
        åŠ è½½æŒ‡å®šè½®æ¬¡çš„æ‰€æœ‰chunks
        
        Args:
            round_num: è®­ç»ƒè½®æ¬¡
            
        Returns:
            (chunk_info, chunk_data) å…ƒç»„åˆ—è¡¨
        """
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # æŸ¥è¯¢chunkå…ƒæ•°æ®
            cursor.execute('''
                SELECT chunk_id, chunk_hash, parts_info, flat_size
                FROM chunk_metadata
                WHERE round_num = ?
                ORDER BY chunk_id
            ''', (round_num,))
            
            metadata_rows = cursor.fetchall()
            
            chunks = []
            for chunk_id, chunk_hash, parts_json, flat_size in metadata_rows:
                # åŠ è½½chunkæ•°æ®
                cursor.execute(
                    "SELECT data FROM chunk_data WHERE chunk_hash = ?",
                    (chunk_hash,)
                )
                data_row = cursor.fetchone()
                
                if data_row:
                    chunk_data = pickle.loads(data_row[0])
                    chunk_info = {
                        'chunk_id': chunk_id,
                        'parts': json.loads(parts_json),
                        'flat_size': flat_size
                    }
                    chunks.append((chunk_info, chunk_data))
                    
            conn.close()
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½chunkså¤±è´¥: {e}")
            return []
    
    def get_chunk_by_id(self, round_num: int, chunk_id: int) -> Optional[Tuple[Dict, np.ndarray]]:
        """
        è·å–æŒ‡å®šè½®æ¬¡å’Œchunk_idçš„chunkæ•°æ®
        
        Args:
            round_num: è®­ç»ƒè½®æ¬¡
            chunk_id: chunk ID
            
        Returns:
            (chunk_info, chunk_data) å…ƒç»„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT chunk_hash, parts_info, flat_size
                FROM chunk_metadata
                WHERE round_num = ? AND chunk_id = ?
            ''', (round_num, chunk_id))
            
            row = cursor.fetchone()
            if row:
                chunk_hash, parts_json, flat_size = row
                
                # åŠ è½½chunkæ•°æ®
                cursor.execute(
                    "SELECT data FROM chunk_data WHERE chunk_hash = ?",
                    (chunk_hash,)
                )
                data_row = cursor.fetchone()
                
                if data_row:
                    chunk_data = pickle.loads(data_row[0])
                    chunk_info = {
                        'chunk_id': chunk_id,
                        'parts': json.loads(parts_json),
                        'flat_size': flat_size
                    }
                    conn.close()
                    return (chunk_info, chunk_data)
                    
            conn.close()
            return None
            
        except Exception as e:
            logger.error(f"âŒ è·å–chunkå¤±è´¥: {e}")
            return None
    
    def get_storage_stats(self) -> Dict:
        """è·å–æ•°æ®åº“å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # ç»Ÿè®¡chunkå…ƒæ•°æ®
            cursor.execute("SELECT COUNT(*) FROM chunk_metadata")
            total_metadata = cursor.fetchone()[0]
            
            # ç»Ÿè®¡chunkæ•°æ®
            cursor.execute("SELECT COUNT(*) FROM chunk_data")
            total_chunks = cursor.fetchone()[0]
            
            # ç»Ÿè®¡å­˜å‚¨å¤§å°
            cursor.execute("SELECT SUM(LENGTH(data)) FROM chunk_data")
            total_size = cursor.fetchone()[0] or 0
            
            # ç»Ÿè®¡è½®æ¬¡èŒƒå›´
            cursor.execute("SELECT MIN(round_num), MAX(round_num) FROM chunk_metadata")
            min_round, max_round = cursor.fetchone()
            
            conn.close()
            
            return {
                'client_id': self.client_id,
                'db_path': self.db_path,
                'total_metadata_entries': total_metadata,
                'unique_chunks': total_chunks,
                'storage_size_bytes': total_size,
                'storage_size_mb': total_size / (1024 * 1024) if total_size else 0,
                'round_range': (min_round, max_round) if min_round is not None else (None, None)
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def cleanup_old_rounds(self, keep_rounds: int = 2):
        """
        æ¸…ç†æ—§è½®æ¬¡çš„chunksï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ è½®
        
        Args:
            keep_rounds: ä¿ç•™æœ€è¿‘å‡ è½®çš„æ•°æ®ï¼Œé»˜è®¤åªä¿ç•™2è½®
        """
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # æ‰¾åˆ°è¦ä¿ç•™çš„æœ€å°è½®æ¬¡ï¼ˆåŒæ—¶è€ƒè™‘æœ¬åœ°å’Œæ¥æ”¶çš„chunkï¼‰
            cursor.execute("SELECT MAX(round_num) FROM chunk_metadata")
            max_local_round = cursor.fetchone()[0]
            
            cursor.execute("SELECT MAX(round_num) FROM bt_chunks")
            max_bt_round = cursor.fetchone()[0]
            
            # ä½¿ç”¨ä¸¤ä¸ªè¡¨ä¸­è¾ƒå¤§çš„è½®æ¬¡ä½œä¸ºåŸºå‡†
            max_round = max_local_round
            if max_bt_round is not None:
                if max_round is None:
                    max_round = max_bt_round
                else:
                    max_round = max(max_round, max_bt_round)
            
            if max_round is not None:
                min_keep_round = max_round - keep_rounds + 1
                
                # åˆ é™¤æ—§çš„æœ¬åœ°chunkå…ƒæ•°æ®
                deleted_local = cursor.execute(
                    "DELETE FROM chunk_metadata WHERE round_num < ?",
                    (min_keep_round,)
                ).rowcount
                
                # ğŸ”§ æ–°å¢ï¼šåˆ é™¤æ—§çš„BitTorrent chunkè®°å½•
                deleted_bt = cursor.execute(
                    "DELETE FROM bt_chunks WHERE round_num < ?",
                    (min_keep_round,)
                ).rowcount
                
                # åˆ é™¤ä¸å†è¢«å¼•ç”¨çš„chunkæ•°æ®
                # ç°åœ¨bt_chunksè¡¨ä¹Ÿä¼šè¢«æ¸…ç†ï¼Œæ‰€ä»¥ä¸ä¼šæœ‰æ°¸ä¹…å¼•ç”¨çš„é—®é¢˜
                deleted_data = cursor.execute('''
                    DELETE FROM chunk_data 
                    WHERE chunk_hash NOT IN (
                        SELECT DISTINCT chunk_hash FROM chunk_metadata
                        UNION
                        SELECT DISTINCT chunk_hash FROM bt_chunks
                    )
                ''').rowcount
                
                conn.commit()
                logger.info(f"ğŸ§¹ èŠ‚ç‚¹ {self.client_id}: æ¸…ç†äº†ç¬¬{min_keep_round}è½®ä¹‹å‰çš„æ•°æ®")
                logger.info(f"   - åˆ é™¤æœ¬åœ°chunkå…ƒæ•°æ®: {deleted_local}æ¡")
                logger.info(f"   - åˆ é™¤æ¥æ”¶chunkè®°å½•: {deleted_bt}æ¡") 
                logger.info(f"   - åˆ é™¤æ— å¼•ç”¨chunkæ•°æ®: {deleted_data}æ¡")
                
            conn.close()
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§chunkså¤±è´¥: {e}")
    
    def reconstruct_model_from_chunks(self, round_num: int, target_model: nn.Module) -> bool:
        """
        ä»chunksé‡æ„æ¨¡å‹å‚æ•°
        
        Args:
            round_num: è¦é‡æ„çš„è½®æ¬¡
            target_model: ç›®æ ‡æ¨¡å‹ï¼Œå‚æ•°å°†è¢«é‡æ„çš„å€¼æ›¿æ¢
            
        Returns:
            æ˜¯å¦é‡æ„æˆåŠŸ
        """
        try:
            chunks = self.load_chunks_by_round(round_num)
            if not chunks:
                logger.warning(f"âš ï¸ ç¬¬{round_num}è½®æ²¡æœ‰æ‰¾åˆ°chunks")
                return False
            
            # è·å–ç›®æ ‡æ¨¡å‹çš„å‚æ•°å½¢çŠ¶ä¿¡æ¯
            model_params = self.model_to_params(target_model)
            
            # åˆå§‹åŒ–é‡æ„å‚æ•°å­—å…¸
            reconstructed_params = {}
            for param_name, param_array in model_params.items():
                reconstructed_params[param_name] = np.zeros_like(param_array)
            
            # ä»chunksé‡æ„å‚æ•°
            for chunk_info, chunk_data in chunks:
                data_ptr = 0
                
                for param_name, parts in chunk_info['parts'].items():
                    if param_name not in reconstructed_params:
                        logger.warning(f"âš ï¸ å‚æ•° {param_name} åœ¨ç›®æ ‡æ¨¡å‹ä¸­ä¸å­˜åœ¨")
                        continue
                        
                    for flat_start, flat_end, shape in parts:
                        chunk_size = flat_end - flat_start
                        part_data = chunk_data[data_ptr:data_ptr + chunk_size]
                        
                        # ç›´æ¥å¯¹æ‰å¹³åŒ–çš„å‚æ•°æ•°ç»„è¿›è¡Œèµ‹å€¼
                        param_flat = reconstructed_params[param_name].reshape(-1)
                        param_flat[flat_start:flat_end] = part_data
                        
                        data_ptr += chunk_size
            
            # å°†é‡æ„çš„å‚æ•°åŠ è½½åˆ°æ¨¡å‹
            self.params_to_model(reconstructed_params, target_model)
            
            logger.info(f"ğŸ“¦ èŠ‚ç‚¹ {self.client_id}: æˆåŠŸä»chunksé‡æ„ç¬¬{round_num}è½®çš„æ¨¡å‹")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‡æ„æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return False
    
    def start_monitoring(self):
        """å¯åŠ¨æ•°æ®åº“å˜åŒ–ç›‘æ§"""
        if self.monitoring_enabled:
            logger.warning(f"âš ï¸ èŠ‚ç‚¹ {self.client_id}: ç›‘æ§å·²ç»å¯åŠ¨")
            return
            
        self.monitoring_enabled = True
        self.stop_monitoring.clear()
        self.last_db_mtime = self._get_db_mtime()
        
        self.monitoring_thread = threading.Thread(
            target=self._monitor_database_changes,
            daemon=True,
            name=f"ChunkMonitor-{self.client_id}"
        )
        self.monitoring_thread.start()
        
        logger.info(f"ğŸ” èŠ‚ç‚¹ {self.client_id}: å¯åŠ¨chunkæ•°æ®åº“å˜åŒ–ç›‘æ§")
    
    def stop_monitoring_thread(self):
        """åœæ­¢æ•°æ®åº“å˜åŒ–ç›‘æ§"""
        if not self.monitoring_enabled:
            return
            
        self.monitoring_enabled = False
        self.stop_monitoring.set()
        
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=2.0)
            
        logger.info(f"ğŸ›‘ èŠ‚ç‚¹ {self.client_id}: åœæ­¢chunkæ•°æ®åº“å˜åŒ–ç›‘æ§")
    
    def _get_db_mtime(self) -> float:
        """è·å–æ•°æ®åº“æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´"""
        try:
            return os.path.getmtime(self.db_path) if os.path.exists(self.db_path) else 0
        except OSError:
            return 0
    
    def _monitor_database_changes(self):
        """ç›‘æ§æ•°æ®åº“å˜åŒ–çš„åå°çº¿ç¨‹"""
        logger.debug(f"ğŸ” èŠ‚ç‚¹ {self.client_id}: å¼€å§‹ç›‘æ§æ•°æ®åº“å˜åŒ–")
        
        while not self.stop_monitoring.is_set():
            try:
                current_mtime = self._get_db_mtime()
                
                if current_mtime > self.last_db_mtime:
                    # æ•°æ®åº“å‘ç”Ÿå˜åŒ–ï¼Œæ£€æµ‹å…·ä½“å˜åŒ–
                    self._detect_and_report_changes()
                    self.last_db_mtime = current_mtime
                
                # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                self.stop_monitoring.wait(1.0)
                
            except Exception as e:
                logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: ç›‘æ§æ•°æ®åº“å˜åŒ–å¤±è´¥: {e}")
                self.stop_monitoring.wait(5.0)  # é”™è¯¯åç­‰å¾…5ç§’å†é‡è¯•
        
        logger.debug(f"ğŸ” èŠ‚ç‚¹ {self.client_id}: æ•°æ®åº“å˜åŒ–ç›‘æ§çº¿ç¨‹é€€å‡º")
    
    def _detect_and_report_changes(self):
        """æ£€æµ‹å¹¶æŠ¥å‘Šæ•°æ®åº“å˜åŒ–"""
        if not self.change_callback:
            return
            
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # è·å–æœ€è¿‘æ·»åŠ çš„chunkä¿¡æ¯ï¼ˆåŸºäºåˆ›å»ºæ—¶é—´ï¼‰
            cursor.execute('''
                SELECT round_num, chunk_id, chunk_hash, flat_size, created_at
                FROM chunk_metadata
                ORDER BY created_at DESC
                LIMIT 10
            ''')
            
            recent_chunks = cursor.fetchall()
            conn.close()
            
            # æŠ¥å‘Šæœ€è¿‘çš„å˜åŒ–
            for round_num, chunk_id, chunk_hash, flat_size, created_at in recent_chunks:
                chunk_info = ChunkInfo(
                    client_id=self.client_id,
                    round_num=round_num,
                    chunk_id=chunk_id,
                    action=ChunkAction.ADD.value,
                    chunk_hash=chunk_hash,
                    chunk_size=flat_size,
                    timestamp=time.time()
                )
                
                # è°ƒç”¨å›è°ƒå‡½æ•°æŠ¥å‘Šå˜åŒ–
                try:
                    self.change_callback(chunk_info)
                    logger.debug(f"ğŸ“¤ èŠ‚ç‚¹ {self.client_id}: æŠ¥å‘Šchunkå˜åŒ– - è½®æ¬¡{round_num}, chunk{chunk_id}")
                except Exception as e:
                    logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: æŠ¥å‘Šchunkå˜åŒ–å¤±è´¥: {e}")
                    
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: æ£€æµ‹æ•°æ®åº“å˜åŒ–å¤±è´¥: {e}")
    
    def report_chunk_change(self, round_num: int, chunk_id: int, action: str, chunk_hash: str, chunk_size: int):
        """æ‰‹åŠ¨æŠ¥å‘Šchunkå˜åŒ–"""
        if not self.change_callback:
            return
            
        chunk_info = ChunkInfo(
            client_id=self.client_id,
            round_num=round_num,
            chunk_id=chunk_id,
            action=action,
            chunk_hash=chunk_hash,
            chunk_size=chunk_size,
            timestamp=time.time()
        )
        
        try:
            self.change_callback(chunk_info)
            logger.debug(f"ğŸ“¤ èŠ‚ç‚¹ {self.client_id}: æ‰‹åŠ¨æŠ¥å‘Šchunkå˜åŒ– - {action} è½®æ¬¡{round_num}, chunk{chunk_id}")
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: æ‰‹åŠ¨æŠ¥å‘Šchunkå˜åŒ–å¤±è´¥: {e}")
    
    def set_change_callback(self, callback: Callable[[ChunkInfo], None]):
        """è®¾ç½®å˜åŒ–å›è°ƒå‡½æ•°"""
        self.change_callback = callback
        
        # å¦‚æœç›‘æ§æœªå¯åŠ¨ä¸”è®¾ç½®äº†å›è°ƒï¼Œå¯åŠ¨ç›‘æ§
        if callback and not self.monitoring_enabled:
            self.start_monitoring()
        # å¦‚æœå–æ¶ˆå›è°ƒï¼Œåœæ­¢ç›‘æ§
        elif not callback and self.monitoring_enabled:
            self.stop_monitoring_thread()
            
        logger.info(f"ğŸ”„ èŠ‚ç‚¹ {self.client_id}: æ›´æ–°å˜åŒ–å›è°ƒå‡½æ•°")
    
    def get_all_chunks_info(self) -> List[ChunkInfo]:
        """è·å–æ‰€æœ‰chunkä¿¡æ¯ç”¨äºåˆå§‹åŒ–æŠ¥å‘Š"""
        chunk_infos = []
        
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT round_num, chunk_id, chunk_hash, flat_size, created_at
                FROM chunk_metadata
                ORDER BY round_num, chunk_id
            ''')
            
            rows = cursor.fetchall()
            conn.close()
            
            for round_num, chunk_id, chunk_hash, flat_size, created_at in rows:
                chunk_info = ChunkInfo(
                    client_id=self.client_id,
                    round_num=round_num,
                    chunk_id=chunk_id,
                    action=ChunkAction.ADD.value,
                    chunk_hash=chunk_hash,
                    chunk_size=flat_size,
                    timestamp=time.time()
                )
                chunk_infos.append(chunk_info)
                
        except Exception as e:
            logger.error(f"âŒ èŠ‚ç‚¹ {self.client_id}: è·å–æ‰€æœ‰chunkä¿¡æ¯å¤±è´¥: {e}")
            
        return chunk_infos
    
    # =================== BitTorrentæ‰©å±•æ–¹æ³• ===================
    
    def _init_bittorrent_tables(self):
        """åˆå§‹åŒ–BitTorrentç›¸å…³çš„æ•°æ®åº“è¡¨"""
        conn = self._get_optimized_connection()
        cursor = conn.cursor()
        
        # åˆ›å»ºBitTorrent chunksè¡¨ï¼ˆç‹¬ç«‹äºåŸæœ‰è¡¨ï¼Œé¿å…å†²çªï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS bt_chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                round_num INTEGER NOT NULL,
                source_client_id INTEGER NOT NULL,
                chunk_id INTEGER NOT NULL,
                chunk_hash TEXT NOT NULL,
                holder_client_id INTEGER NOT NULL,
                received_time REAL DEFAULT (strftime('%s', 'now')),
                is_verified INTEGER DEFAULT 0,
                UNIQUE(round_num, source_client_id, chunk_id, holder_client_id)
            )
        ''')
        
        # åˆ›å»ºBitTorrentäº¤æ¢çŠ¶æ€è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS bt_exchange_status (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                round_num INTEGER NOT NULL,
                peer_id INTEGER NOT NULL,
                chunk_key TEXT NOT NULL,
                status TEXT NOT NULL,
                request_time REAL,
                complete_time REAL,
                retry_count INTEGER DEFAULT 0,
                error_msg TEXT,
                size INTEGER,
                UNIQUE(round_num, peer_id, chunk_key)
            )
        ''')
        
        # åˆ›å»ºBitTorrentä¼šè¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS bt_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                round_num INTEGER NOT NULL,
                start_time REAL NOT NULL,
                end_time REAL,
                status TEXT NOT NULL,
                total_chunks_expected INTEGER,
                total_chunks_received INTEGER DEFAULT 0,
                bytes_uploaded INTEGER DEFAULT 0,
                bytes_downloaded INTEGER DEFAULT 0,
                UNIQUE(round_num)
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_bt_round_holder ON bt_chunks(round_num, holder_client_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_bt_source ON bt_chunks(round_num, source_client_id, chunk_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_bt_hash ON bt_chunks(chunk_hash)')
        
        conn.commit()
        conn.close()
        logger.debug(f"[ChunkManager] BitTorrent tables initialized for client {self.client_id}")
    
    def get_global_bitfield(self, round_num=None):
        """
        ğŸ”§ ä¿®å¤ï¼šå…¼å®¹æ—§ä»£ç ï¼Œæ”¯æŒå¯é€‰round_numå‚æ•°
        è·å–æŒ‡å®šè½®æ¬¡çš„å…¨å±€chunkæ‹¥æœ‰æƒ…å†µçš„bitfield
        """
        # å¦‚æœæ²¡æœ‰ä¼ å…¥round_numï¼Œä½¿ç”¨å½“å‰è½®æ¬¡
        if round_num is None:
            round_num = getattr(self, 'current_round', 0)
            
        bitfield = {}
        
        # æŸ¥è¯¢æœ¬åœ°chunksï¼ˆåŸæœ‰è¡¨ï¼‰
        conn = self._get_optimized_connection()
        cursor = conn.cursor()
        
        try:
            # æŸ¥è¯¢æœ¬åœ°ä¿å­˜çš„chunks
            cursor.execute('''
                SELECT chunk_id FROM chunk_metadata
                WHERE round_num = ?
            ''', (round_num,))
            
            local_chunks = cursor.fetchall()
            logger.debug(f"[ChunkManager] Client {self.client_id}: Found {len(local_chunks)} local chunks for round {round_num}")
            
            for (chunk_id,) in local_chunks:
                # æœ¬åœ°chunks
                bitfield[(round_num, self.client_id, chunk_id)] = True
                # é™é»˜æ·»åŠ æœ¬åœ°chunkåˆ°bitfield
                pass
            
            # æŸ¥è¯¢BitTorrentäº¤æ¢çš„chunksï¼ˆæ–°è¡¨ï¼‰
            cursor.execute('''
                SELECT source_client_id, chunk_id FROM bt_chunks
                WHERE round_num = ? AND holder_client_id = ?
            ''', (round_num, self.client_id))
            
            for source_id, chunk_id in cursor.fetchall():
                bitfield[(round_num, source_id, chunk_id)] = True
                
        except sqlite3.OperationalError:
            # å¦‚æœbt_chunksè¡¨ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–å®ƒ
            logger.warning(f"[ChunkManager] BitTorrent tables not found, initializing...")
            conn.close()
            self._init_bittorrent_tables()
            return self.get_global_bitfield(round_num)
        
        conn.close()
        return bitfield
    
    def save_remote_chunk(self, round_num, source_client_id, chunk_id, chunk_data):
        """
        ğŸ”§ ä¿®å¤ï¼šä¿å­˜BitTorrentäº¤æ¢çš„chunkåˆ°æ–°è¡¨ï¼Œé¿å…schemaå†²çª
        """
        import hashlib
        chunk_hash = hashlib.sha256(chunk_data).hexdigest()
        
        # ç¡®ä¿BitTorrentè¡¨å­˜åœ¨
        try:
            # ç›´æ¥å†™å…¥bt_chunksè¡¨ï¼ˆé¿å…ä¸ç°æœ‰chunk_metadataè¡¨å†²çªï¼‰
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # å†™å…¥bt_chunksè¡¨
            cursor.execute('''
                INSERT OR REPLACE INTO bt_chunks 
                (round_num, source_client_id, chunk_id, chunk_hash, holder_client_id, is_verified)
                VALUES (?, ?, ?, ?, ?, 1)
            ''', (round_num, source_client_id, chunk_id, chunk_hash, self.client_id))
            
            # å†™å…¥chunk_dataè¡¨ï¼ˆå…±äº«å­˜å‚¨ï¼‰- ä½¿ç”¨REPLACEå¤„ç†é‡å¤å“ˆå¸Œ
            cursor.execute('''
                INSERT OR REPLACE INTO chunk_data (chunk_hash, data)
                VALUES (?, ?)
            ''', (chunk_hash, pickle.dumps(chunk_data)))
            
            conn.commit()
            conn.close()
            
        except sqlite3.OperationalError as e:
            if "no such table" in str(e):
                # åˆå§‹åŒ–BitTorrentè¡¨
                self._init_bittorrent_tables()
                # é‡è¯•
                return self.save_remote_chunk(round_num, source_client_id, chunk_id, chunk_data)
            else:
                raise e
        
        
        # è§¦å‘å˜åŒ–å›è°ƒ
        if self.change_callback:
            # åˆ›å»ºChunkInfoå¯¹è±¡æ¥æŠ¥å‘Šè¿œç¨‹chunkä¿å­˜äº‹ä»¶
            chunk_info = ChunkInfo(
                client_id=self.client_id,
                round_num=round_num,
                chunk_id=chunk_id,
                action='remote_chunk_saved',
                chunk_hash=chunk_hash,
                chunk_size=len(chunk_data) if hasattr(chunk_data, '__len__') else 0,
                timestamp=time.time()
            )
            self.change_callback(chunk_info)
    
    def get_chunk_data(self, round_num, source_client_id, chunk_id):
        """
        ğŸ†• æ–°å¢ï¼šè·å–chunkæ•°æ®ï¼ˆç”¨äºå‘é€ç»™å…¶ä»–peersï¼‰
        """
        conn = self._get_optimized_connection()
        cursor = conn.cursor()
        
        try:
            # å…ˆæŸ¥è¯¢æœ¬åœ°chunks
            if source_client_id == self.client_id:
                cursor.execute('''
                    SELECT cd.data FROM chunk_metadata cm
                    JOIN chunk_data cd ON cm.chunk_hash = cd.chunk_hash
                    WHERE cm.round_num = ? AND cm.chunk_id = ?
                ''', (round_num, chunk_id))
            else:
                # æŸ¥è¯¢BitTorrentäº¤æ¢çš„chunks
                cursor.execute('''
                    SELECT cd.data FROM bt_chunks bc
                    JOIN chunk_data cd ON bc.chunk_hash = cd.chunk_hash
                    WHERE bc.round_num = ? AND bc.source_client_id = ? 
                    AND bc.chunk_id = ? AND bc.holder_client_id = ?
                ''', (round_num, source_client_id, chunk_id, self.client_id))
            
            result = cursor.fetchone()
            logger.debug(f"[ChunkManager] Client {self.client_id}: Query result for chunk ({round_num}, {source_client_id}, {chunk_id}): {result is not None}")
            if result:
                try:
                    chunk_data = pickle.loads(result[0])
                    logger.debug(f"[ChunkManager] Client {self.client_id}: Successfully unpickled chunk data, size: {len(result[0])} bytes")
                    return chunk_data
                except Exception as pickle_error:
                    logger.error(f"[ChunkManager] Client {self.client_id}: Failed to unpickle chunk data: {pickle_error}")
                    return None
            else:
                logger.debug(f"[ChunkManager] Client {self.client_id}: No result found for chunk ({round_num}, {source_client_id}, {chunk_id})")
                return None
            
        except sqlite3.OperationalError as e:
            # æ•°æ®åº“æ“ä½œé”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
            logger.error(f"[ChunkManager] Client {self.client_id}: SQLite OperationalError in get_chunk_data: {e}")
            logger.error(f"[ChunkManager] Client {self.client_id}: Query params: round_num={round_num}, source_client_id={source_client_id}, chunk_id={chunk_id}")
            return None
        except sqlite3.DatabaseError as e:
            # æ•°æ®åº“é”™è¯¯
            logger.error(f"[ChunkManager] Client {self.client_id}: SQLite DatabaseError in get_chunk_data: {e}")
            return None
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸
            logger.error(f"[ChunkManager] Client {self.client_id}: Unexpected error in get_chunk_data: {e}")
            return None
        finally:
            conn.close()
    
    def start_bittorrent_session(self, round_num, expected_chunks):
        """å¼€å§‹BitTorrentäº¤æ¢ä¼šè¯"""
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO bt_sessions 
                (round_num, start_time, status, total_chunks_expected)
                VALUES (?, ?, 'active', ?)
            ''', (round_num, time.time(), expected_chunks))
            
            conn.commit()
            conn.close()
            
        except sqlite3.OperationalError:
            # åˆå§‹åŒ–è¡¨å¹¶é‡è¯•
            self._init_bittorrent_tables()
            return self.start_bittorrent_session(round_num, expected_chunks)
        
        logger.info(f"[ChunkManager] Started BitTorrent session for round {round_num}")
    
    def finish_bittorrent_session(self, round_num, status='completed'):
        """ç»“æŸBitTorrentäº¤æ¢ä¼šè¯"""
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # ç»Ÿè®¡æ¥æ”¶åˆ°çš„chunksæ•°é‡
            cursor.execute('''
                SELECT COUNT(*) FROM bt_chunks
                WHERE round_num = ? AND holder_client_id = ?
            ''', (round_num, self.client_id))
            
            chunks_received = cursor.fetchone()[0]
            
            # æ›´æ–°ä¼šè¯çŠ¶æ€
            cursor.execute('''
                UPDATE bt_sessions 
                SET end_time = ?, status = ?, total_chunks_received = ?
                WHERE round_num = ?
            ''', (time.time(), status, chunks_received, round_num))
            
            conn.commit()
            conn.close()
            
            logger.info(f"[ChunkManager] Finished BitTorrent session for round {round_num}, status: {status}")
            
        except sqlite3.OperationalError:
            # è¡¨ä¸å­˜åœ¨ï¼Œå¿½ç•¥
            pass
    
    def cleanup_bittorrent_data(self, keep_rounds=5):
        """æ¸…ç†æ—§çš„BitTorrentæ•°æ®"""
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # æ‰¾åˆ°è¦ä¿ç•™çš„æœ€å°è½®æ¬¡
            cursor.execute("SELECT MAX(round_num) FROM bt_sessions")
            max_round = cursor.fetchone()[0]
            
            if max_round is not None:
                min_keep_round = max_round - keep_rounds + 1
                
                # åˆ é™¤æ—§çš„BitTorrentæ•°æ®
                cursor.execute("DELETE FROM bt_chunks WHERE round_num < ?", (min_keep_round,))
                cursor.execute("DELETE FROM bt_exchange_status WHERE round_num < ?", (min_keep_round,))
                cursor.execute("DELETE FROM bt_sessions WHERE round_num < ?", (min_keep_round,))
                
                conn.commit()
                logger.info(f"[ChunkManager] Cleaned BitTorrent data before round {min_keep_round}")
                
            conn.close()
            
        except sqlite3.OperationalError:
            # è¡¨ä¸å­˜åœ¨ï¼Œå¿½ç•¥
            pass
    
    def get_available_clients_for_round(self, round_num: int) -> List[int]:
        """
        è·å–æŒ‡å®šè½®æ¬¡å¯ç”¨çš„æ‰€æœ‰å®¢æˆ·ç«¯ID
        
        Args:
            round_num: ç›®æ ‡è½®æ¬¡
            
        Returns:
            List[int]: å¯ç”¨å®¢æˆ·ç«¯IDåˆ—è¡¨
        """
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # æŸ¥è¯¢æœ¬åœ°chunkæ•°æ®çš„å®¢æˆ·ç«¯ (local chunks don't have source_client_id, they belong to this client)
            cursor.execute('''
                SELECT DISTINCT ? as client_id
                FROM chunk_metadata 
                WHERE round_num = ?
                LIMIT 1
            ''', (self.client_id, round_num))
            
            local_result = cursor.fetchall()
            local_clients = [row[0] for row in local_result] if local_result else []
            
            # æŸ¥è¯¢BitTorrent chunksä¸­çš„å®¢æˆ·ç«¯
            cursor.execute('''
                SELECT DISTINCT source_client_id 
                FROM bt_chunks 
                WHERE round_num = ?
                ORDER BY source_client_id
            ''', (round_num,))
            
            bt_clients = [row[0] for row in cursor.fetchall()]
            
            # åˆå¹¶å¹¶å»é‡
            all_clients = list(set(local_clients + bt_clients))
            all_clients.sort()
            
            conn.close()
            
            logger.debug(f"[ChunkManager] Round {round_num}: Found {len(all_clients)} clients with chunk data: {all_clients}")
            return all_clients
            
        except Exception as e:
            logger.error(f"[ChunkManager] Failed to get available clients for round {round_num}: {e}")
            return []
    
    def reconstruct_model_from_chunks(self, client_id: int, round_num: int) -> Optional[Dict]:
        """
        ä»chunksé‡æ„æŒ‡å®šå®¢æˆ·ç«¯çš„æ¨¡å‹å‚æ•°
        
        Args:
            client_id: ç›®æ ‡å®¢æˆ·ç«¯ID
            round_num: ç›®æ ‡è½®æ¬¡
            
        Returns:
            Dict: é‡æ„çš„æ¨¡å‹å‚æ•°å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # æŸ¥è¯¢è¯¥å®¢æˆ·ç«¯çš„æ‰€æœ‰chunks
            if client_id == self.client_id:
                # Local client - query local chunks
                cursor.execute('''
                    SELECT chunk_id, chunk_hash 
                    FROM chunk_metadata 
                    WHERE round_num = ?
                    ORDER BY chunk_id
                ''', (round_num,))
                local_chunks = cursor.fetchall()
            else:
                # Remote client - query BitTorrent chunks
                cursor.execute('''
                    SELECT chunk_id, chunk_hash 
                    FROM bt_chunks 
                    WHERE source_client_id = ? AND round_num = ?
                    ORDER BY chunk_id
                ''', (client_id, round_num))
                local_chunks = cursor.fetchall()
            
            if not local_chunks:
                logger.warning(f"[ChunkManager] No chunks found for client {client_id}, round {round_num}")
                conn.close()
                return None
            
            # é‡æ„æ¨¡å‹å‚æ•°
            chunk_data_list = []
            missing_chunks = []
            
            for chunk_id, chunk_hash in local_chunks:
                # è·å–chunkæ•°æ®
                cursor.execute('''
                    SELECT data FROM chunk_data WHERE chunk_hash = ?
                ''', (chunk_hash,))
                
                result = cursor.fetchone()
                if result:
                    # ç›´æ¥è·å–åŸå§‹å­—èŠ‚æ•°æ®ï¼Œä¸è¿›è¡Œååºåˆ—åŒ–
                    chunk_data = result[0]
                    chunk_data_list.append((chunk_id, chunk_data))
                else:
                    missing_chunks.append(chunk_id)
            
            if missing_chunks:
                logger.warning(f"[ChunkManager] Missing chunk data for client {client_id}, chunks: {missing_chunks}")
            
            if not chunk_data_list:
                logger.error(f"[ChunkManager] No valid chunk data found for client {client_id}, round {round_num}")
                conn.close()
                return None
            
            # æŒ‰chunk_idæ’åº
            chunk_data_list.sort(key=lambda x: x[0])
            
            # ååºåˆ—åŒ–æ¯ä¸ªchunkå¹¶è¿æ¥
            numpy_chunks = []
            parts_info_list = []
            
            for chunk_id, chunk_data in chunk_data_list:
                # ååºåˆ—åŒ–chunkæ•°æ®
                numpy_chunk = pickle.loads(chunk_data)
                numpy_chunks.append(numpy_chunk)
                
                # è·å–å¯¹åº”çš„parts_info
                cursor.execute('''
                    SELECT parts_info FROM chunk_metadata 
                    WHERE chunk_id = ? AND round_num = ?
                ''', (chunk_id, round_num))
                
                parts_result = cursor.fetchone()
                if parts_result:
                    parts_info = json.loads(parts_result[0])
                    parts_info_list.append((chunk_id, parts_info))
            
            conn.close()
            
            # è¿æ¥æ‰€æœ‰chunkæ•°æ®
            if len(numpy_chunks) == 0:
                logger.error(f"[ChunkManager] No valid chunk data for client {client_id}, round {round_num}")
                return None
                
            combined_numpy = np.concatenate(numpy_chunks) if len(numpy_chunks) > 1 else numpy_chunks[0]
            
            # ä½¿ç”¨parts_infoé‡æ„å›å‚æ•°å­—å…¸
            model_params = self._reconstruct_params_dict(combined_numpy, parts_info_list)
            
            logger.debug(f"[ChunkManager] Successfully reconstructed model for client {client_id}, round {round_num}")
            return model_params
            
        except Exception as e:
            logger.error(f"[ChunkManager] Failed to reconstruct model for client {client_id}, round {round_num}: {e}")
            return None
    
    def _reconstruct_params_dict(self, combined_numpy: np.ndarray, parts_info_list: List[Tuple[int, Dict]]) -> Dict:
        """
        ä½¿ç”¨parts_infoå°†æ‰å¹³åŒ–çš„numpyæ•°ç»„é‡æ„å›å‚æ•°å­—å…¸
        
        Args:
            combined_numpy: è¿æ¥åçš„æ‰å¹³åŒ–numpyæ•°ç»„
            parts_info_list: [(chunk_id, parts_info), ...]æ ¼å¼çš„ç»“æ„ä¿¡æ¯
            
        Returns:
            é‡æ„çš„æ¨¡å‹å‚æ•°å­—å…¸
        """
        import torch
        
        params_dict = {}
        current_pos = 0
        
        # æŒ‰chunk_idæ’åºparts_info
        parts_info_list.sort(key=lambda x: x[0])
        
        for chunk_id, parts_info in parts_info_list:
            for param_name, parts in parts_info.items():
                if param_name not in params_dict:
                    # é¦–æ¬¡é‡åˆ°è¿™ä¸ªå‚æ•°ï¼Œéœ€è¦é¢„ä¼°æ€»å¤§å°
                    total_size = self._estimate_param_size(param_name, parts_info_list)
                    params_dict[param_name] = np.zeros(total_size, dtype=combined_numpy.dtype)
                
                # å¡«å……è¿™ä¸ªå‚æ•°çš„å„ä¸ªéƒ¨åˆ†
                for flat_start, flat_end, shape in parts:
                    chunk_size = flat_end - flat_start
                    chunk_data = combined_numpy[current_pos:current_pos + chunk_size]
                    
                    # å°†æ•°æ®æ”¾å›åŸå§‹å‚æ•°çš„å¯¹åº”ä½ç½®
                    params_dict[param_name][flat_start:flat_end] = chunk_data
                    current_pos += chunk_size
        
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶reshape
        final_params = {}
        for param_name, flat_data in params_dict.items():
            # ä»parts_infoè·å–åŸå§‹å½¢çŠ¶
            original_shape = self._get_original_shape(param_name, parts_info_list)
            if original_shape:
                reshaped_data = flat_data.reshape(original_shape)
                final_params[param_name] = torch.tensor(reshaped_data, dtype=torch.float32)
            else:
                final_params[param_name] = torch.tensor(flat_data, dtype=torch.float32)
        
        return final_params
    
    def _estimate_param_size(self, param_name: str, parts_info_list: List[Tuple[int, Dict]]) -> int:
        """ä¼°ç®—å‚æ•°çš„æ€»å¤§å°"""
        max_end = 0
        for _, parts_info in parts_info_list:
            if param_name in parts_info:
                for flat_start, flat_end, shape in parts_info[param_name]:
                    max_end = max(max_end, flat_end)
        return max_end
    
    def _get_original_shape(self, param_name: str, parts_info_list: List[Tuple[int, Dict]]) -> Optional[tuple]:
        """è·å–å‚æ•°çš„åŸå§‹å½¢çŠ¶"""
        for _, parts_info in parts_info_list:
            if param_name in parts_info:
                # å‡è®¾åŒä¸€å‚æ•°åœ¨æ‰€æœ‰chunksä¸­çš„å½¢çŠ¶ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ª
                for flat_start, flat_end, shape in parts_info[param_name]:
                    return tuple(shape)
        return None
    
    def get_client_sample_size(self, client_id: int, round_num: int) -> Optional[int]:
        """
        è·å–æŒ‡å®šå®¢æˆ·ç«¯åœ¨æŒ‡å®šè½®æ¬¡çš„æ ·æœ¬æ•°é‡
        
        Args:
            client_id: å®¢æˆ·ç«¯ID
            round_num: è½®æ¬¡
            
        Returns:
            int: æ ·æœ¬æ•°é‡ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å›None
        """
        try:
            conn = self._get_optimized_connection()
            cursor = conn.cursor()
            
            # Check if we have chunks from this client for this round
            # Note: sample_size column doesn't exist in schema, using fallback approach
            
            if client_id == self.client_id:
                # Local client - check local chunks
                cursor.execute('''
                    SELECT COUNT(*) FROM chunk_metadata 
                    WHERE round_num = ?
                ''', (round_num,))
            else:
                # Remote client - check BitTorrent chunks
                cursor.execute('''
                    SELECT COUNT(*) FROM bt_chunks 
                    WHERE round_num = ? AND source_client_id = ?
                ''', (round_num, client_id))
            
            result = cursor.fetchone()
            conn.close()
            
            if result and result[0] > 0:
                # Return fixed sample size for BitTorrent FL (toy dataset default)
                return 128
            else:
                logger.debug(f"[ChunkManager] No chunks found for client {client_id}, round {round_num}")
                return None
                
        except Exception as e:
            logger.error(f"[ChunkManager] Failed to get sample size for client {client_id}, round {round_num}: {e}")
            return None