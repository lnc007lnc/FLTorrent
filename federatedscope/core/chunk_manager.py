"""
åŸºäºæ‚¨æä¾›ç®—æ³•çš„æ¨¡å‹åˆ†å—ç®¡ç†ç³»ç»Ÿ
ä½¿ç”¨æ‰å¹³ç´¢å¼•è®°å½•å‚æ•°chunkä¿¡æ¯ï¼ŒæŒ‰èŠ‚ç‚¹åå»ºç«‹æœ¬åœ°æ•°æ®åº“å­˜å‚¨chunk
"""

import os
import json
import hashlib
import pickle
import sqlite3
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
import torch
import torch.nn as nn
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class ChunkManager:
    """
    ç»Ÿä¸€ç®¡ç†æ¨¡å‹åˆ†å—é€»è¾‘ï¼Œä½¿ç”¨æ‰å¹³ç´¢å¼•è®°å½•å‚æ•°chunkä¿¡æ¯ã€‚
    æ¯ä¸ªchunkçš„å®šä¹‰æ ¼å¼ä¸ºï¼š
      {
          'chunk_id': int,
          'parts': { key: [ (flat_start, flat_end, shape), ... ] },
          'flat_size': int
      }
    """
    
    def __init__(self, client_id: int):
        """
        åˆå§‹åŒ–ChunkManagerï¼Œä¸ºæŒ‡å®šå®¢æˆ·ç«¯åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“
        
        Args:
            client_id: å®¢æˆ·ç«¯IDï¼Œç”¨äºåˆ›å»ºèŠ‚ç‚¹ç‰¹å®šçš„æ•°æ®åº“æ–‡ä»¶
        """
        self.client_id = client_id
        
        # æŒ‰èŠ‚ç‚¹ååˆ›å»ºæ•°æ®åº“æ–‡ä»¶è·¯å¾„: /tmp/client_X/client_X_chunks.db
        client_name = f"client_{client_id}"
        db_dir = os.path.join(os.getcwd(), "tmp", client_name)
        os.makedirs(db_dir, exist_ok=True)
        
        self.db_path = os.path.join(db_dir, f"{client_name}_chunks.db")
        self._init_database()
        
        logger.info(f"ğŸ“Š åˆå§‹åŒ–èŠ‚ç‚¹ {client_id} çš„chunkæ•°æ®åº“: {self.db_path}")
        
    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºchunkå…ƒæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunk_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                round_num INTEGER NOT NULL,
                chunk_id INTEGER NOT NULL,
                chunk_hash TEXT NOT NULL,
                parts_info TEXT NOT NULL,
                flat_size INTEGER NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(round_num, chunk_id)
            )
        ''')
        
        # åˆ›å»ºchunkæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunk_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chunk_hash TEXT UNIQUE NOT NULL,
                data BLOB NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_metadata_round 
            ON chunk_metadata(round_num)
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_chunk_metadata_hash 
            ON chunk_metadata(chunk_hash)
        ''')
        
        conn.commit()
        conn.close()
    
    @staticmethod
    def model_to_params(model: nn.Module) -> Dict[str, np.ndarray]:
        """å°†æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°åŠç¼“å†²åŒºè½¬ä¸º numpy æ•°ç»„"""
        params = {name: param.data.cpu().numpy() for name, param in model.named_parameters()}
        for name, buffer in model.named_buffers():
            params[name] = buffer.data.cpu().numpy()
        return params

    @staticmethod
    def params_to_model(params: Dict[str, np.ndarray], model: nn.Module):
        """å°†å‚æ•°å­—å…¸åŠ è½½å›æ¨¡å‹"""
        for name, param in model.named_parameters():
            if name in params:
                param.data = torch.from_numpy(params[name]).to(param.device)
    
    @staticmethod
    def split_model(params: Dict[str, np.ndarray], num_chunks: int) -> List[Dict]:
        """
        å°†æ¨¡å‹å‚æ•°å‡åŒ€åˆ†å‰²ä¸ºæŒ‡å®šæ•°é‡çš„chunkï¼Œè®°å½•æ¯ä¸ªchunkä¸­å„å‚æ•°çš„æ‰å¹³ç´¢å¼•åŒºé—´ã€‚
        è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼ä¸ºï¼š
          {
              'chunk_id': int,
              'parts': { key: [ (flat_start, flat_end, shape), ... ] },
              'flat_size': int
          }
        """
        total_elements = sum(np.prod(v.shape) for v in params.values())
        elements_per_chunk = total_elements // num_chunks

        chunks = []
        current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
        
        # å¯¹æ¯ä¸ªå‚æ•°ï¼ŒæŒ‰ç…§æ‰å¹³é¡ºåºè¿›è¡Œåˆ‡åˆ†
        for key in sorted(params.keys()):
            arr = params[key]
            n = int(np.prod(arr.shape))
            ptr = 0
            while ptr < n:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°çš„chunk
                if current_chunk['flat_size'] >= elements_per_chunk and len(chunks) < num_chunks - 1:
                    chunks.append(current_chunk)
                    current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
                
                # è®¡ç®—å¯ä»¥æ”¾å…¥å½“å‰chunkçš„å…ƒç´ æ•°é‡
                if len(chunks) < num_chunks - 1:
                    remaining = elements_per_chunk - current_chunk['flat_size']
                    take = min(remaining, n - ptr)
                else:
                    # æœ€åä¸€ä¸ªchunkåŒ…å«æ‰€æœ‰å‰©ä½™å…ƒç´ 
                    take = n - ptr
                    
                # ä¸ºå½“å‰chunkä¸­å‚æ•° key æ·»åŠ è¿™ä¸€æ®µä¿¡æ¯
                if key not in current_chunk['parts']:
                    current_chunk['parts'][key] = []
                current_chunk['parts'][key].append((int(ptr), int(ptr + take), arr.shape))
                current_chunk['flat_size'] += take
                ptr += take
                
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk['flat_size'] > 0:
            chunks.append(current_chunk)
            
        return chunks
    
    def extract_chunk_data(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> np.ndarray:
        """
        æ ¹æ®chunkä¿¡æ¯ä»æ¨¡å‹å‚æ•°ä¸­æå–å¯¹åº”çš„æ•°æ®
        
        Args:
            params: æ¨¡å‹å‚æ•°å­—å…¸
            chunk_info: chunkçš„å…ƒæ•°æ®ä¿¡æ¯
            
        Returns:
            æ‰å¹³åŒ–çš„chunkæ•°æ®æ•°ç»„
        """
        chunk_data = []
        
        for key, parts in chunk_info['parts'].items():
            if key not in params:
                logger.warning(f"âš ï¸ å‚æ•° {key} åœ¨æ¨¡å‹ä¸­æœªæ‰¾åˆ°")
                continue
                
            arr_flat = params[key].flatten()
            for flat_start, flat_end, shape in parts:
                chunk_data.append(arr_flat[flat_start:flat_end])
                
        if chunk_data:
            return np.concatenate(chunk_data)
        else:
            return np.array([])
    
    def save_model_chunks(self, model: nn.Module, round_num: int, num_chunks: int = 10) -> List[str]:
        """
        å°†æ¨¡å‹åˆ†å‰²æˆchunkså¹¶ä¿å­˜åˆ°èŠ‚ç‚¹ç‰¹å®šçš„æ•°æ®åº“
        
        Args:
            model: PyTorchæ¨¡å‹
            round_num: è®­ç»ƒè½®æ¬¡
            num_chunks: åˆ†å‰²çš„chunkæ•°é‡
            
        Returns:
            ä¿å­˜çš„chunkå“ˆå¸Œåˆ—è¡¨
        """
        try:
            # å°†æ¨¡å‹è½¬æ¢ä¸ºå‚æ•°å­—å…¸
            params = self.model_to_params(model)
            
            # åˆ†å‰²æ¨¡å‹
            chunks_info = self.split_model(params, num_chunks)
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            saved_hashes = []
            
            for chunk_info in chunks_info:
                # æå–chunkæ•°æ®
                chunk_data = self.extract_chunk_data(params, chunk_info)
                
                # è®¡ç®—chunkå“ˆå¸Œ
                chunk_bytes = pickle.dumps(chunk_data)
                chunk_hash = hashlib.sha256(chunk_bytes).hexdigest()
                
                # ä¿å­˜chunkæ•°æ®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                cursor.execute(
                    "INSERT OR IGNORE INTO chunk_data (chunk_hash, data) VALUES (?, ?)",
                    (chunk_hash, chunk_bytes)
                )
                
                # ä¿å­˜chunkå…ƒæ•°æ®
                parts_json = json.dumps(chunk_info['parts'])
                cursor.execute('''
                    INSERT OR REPLACE INTO chunk_metadata 
                    (round_num, chunk_id, chunk_hash, parts_info, flat_size)
                    VALUES (?, ?, ?, ?, ?)
                ''', (round_num, chunk_info['chunk_id'], chunk_hash, 
                     parts_json, chunk_info['flat_size']))
                
                saved_hashes.append(chunk_hash)
                
            conn.commit()
            conn.close()
            
            logger.info(f"ğŸ’¾ èŠ‚ç‚¹ {self.client_id}: ç¬¬{round_num}è½®ä¿å­˜äº† {len(saved_hashes)} ä¸ªchunks")
            return saved_hashes
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¨¡å‹chunkså¤±è´¥: {e}")
            return []
    
    def load_chunks_by_round(self, round_num: int) -> List[Tuple[Dict, np.ndarray]]:
        """
        åŠ è½½æŒ‡å®šè½®æ¬¡çš„æ‰€æœ‰chunks
        
        Args:
            round_num: è®­ç»ƒè½®æ¬¡
            
        Returns:
            (chunk_info, chunk_data) å…ƒç»„åˆ—è¡¨
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æŸ¥è¯¢chunkå…ƒæ•°æ®
            cursor.execute('''
                SELECT chunk_id, chunk_hash, parts_info, flat_size
                FROM chunk_metadata
                WHERE round_num = ?
                ORDER BY chunk_id
            ''', (round_num,))
            
            metadata_rows = cursor.fetchall()
            
            chunks = []
            for chunk_id, chunk_hash, parts_json, flat_size in metadata_rows:
                # åŠ è½½chunkæ•°æ®
                cursor.execute(
                    "SELECT data FROM chunk_data WHERE chunk_hash = ?",
                    (chunk_hash,)
                )
                data_row = cursor.fetchone()
                
                if data_row:
                    chunk_data = pickle.loads(data_row[0])
                    chunk_info = {
                        'chunk_id': chunk_id,
                        'parts': json.loads(parts_json),
                        'flat_size': flat_size
                    }
                    chunks.append((chunk_info, chunk_data))
                    
            conn.close()
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½chunkså¤±è´¥: {e}")
            return []
    
    def get_chunk_by_id(self, round_num: int, chunk_id: int) -> Optional[Tuple[Dict, np.ndarray]]:
        """
        è·å–æŒ‡å®šè½®æ¬¡å’Œchunk_idçš„chunkæ•°æ®
        
        Args:
            round_num: è®­ç»ƒè½®æ¬¡
            chunk_id: chunk ID
            
        Returns:
            (chunk_info, chunk_data) å…ƒç»„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT chunk_hash, parts_info, flat_size
                FROM chunk_metadata
                WHERE round_num = ? AND chunk_id = ?
            ''', (round_num, chunk_id))
            
            row = cursor.fetchone()
            if row:
                chunk_hash, parts_json, flat_size = row
                
                # åŠ è½½chunkæ•°æ®
                cursor.execute(
                    "SELECT data FROM chunk_data WHERE chunk_hash = ?",
                    (chunk_hash,)
                )
                data_row = cursor.fetchone()
                
                if data_row:
                    chunk_data = pickle.loads(data_row[0])
                    chunk_info = {
                        'chunk_id': chunk_id,
                        'parts': json.loads(parts_json),
                        'flat_size': flat_size
                    }
                    conn.close()
                    return (chunk_info, chunk_data)
                    
            conn.close()
            return None
            
        except Exception as e:
            logger.error(f"âŒ è·å–chunkå¤±è´¥: {e}")
            return None
    
    def get_storage_stats(self) -> Dict:
        """è·å–æ•°æ®åº“å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç»Ÿè®¡chunkå…ƒæ•°æ®
            cursor.execute("SELECT COUNT(*) FROM chunk_metadata")
            total_metadata = cursor.fetchone()[0]
            
            # ç»Ÿè®¡chunkæ•°æ®
            cursor.execute("SELECT COUNT(*) FROM chunk_data")
            total_chunks = cursor.fetchone()[0]
            
            # ç»Ÿè®¡å­˜å‚¨å¤§å°
            cursor.execute("SELECT SUM(LENGTH(data)) FROM chunk_data")
            total_size = cursor.fetchone()[0] or 0
            
            # ç»Ÿè®¡è½®æ¬¡èŒƒå›´
            cursor.execute("SELECT MIN(round_num), MAX(round_num) FROM chunk_metadata")
            min_round, max_round = cursor.fetchone()
            
            conn.close()
            
            return {
                'client_id': self.client_id,
                'db_path': self.db_path,
                'total_metadata_entries': total_metadata,
                'unique_chunks': total_chunks,
                'storage_size_bytes': total_size,
                'storage_size_mb': total_size / (1024 * 1024) if total_size else 0,
                'round_range': (min_round, max_round) if min_round is not None else (None, None)
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å­˜å‚¨ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def cleanup_old_rounds(self, keep_rounds: int = 5):
        """
        æ¸…ç†æ—§è½®æ¬¡çš„chunksï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ è½®
        
        Args:
            keep_rounds: ä¿ç•™æœ€è¿‘å‡ è½®çš„æ•°æ®
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ‰¾åˆ°è¦ä¿ç•™çš„æœ€å°è½®æ¬¡
            cursor.execute("SELECT MAX(round_num) FROM chunk_metadata")
            max_round = cursor.fetchone()[0]
            
            if max_round is not None:
                min_keep_round = max_round - keep_rounds + 1
                
                # åˆ é™¤æ—§çš„å…ƒæ•°æ®
                cursor.execute(
                    "DELETE FROM chunk_metadata WHERE round_num < ?",
                    (min_keep_round,)
                )
                
                # åˆ é™¤ä¸å†è¢«å¼•ç”¨çš„chunkæ•°æ®
                cursor.execute('''
                    DELETE FROM chunk_data 
                    WHERE chunk_hash NOT IN (
                        SELECT DISTINCT chunk_hash FROM chunk_metadata
                    )
                ''')
                
                conn.commit()
                logger.info(f"ğŸ§¹ èŠ‚ç‚¹ {self.client_id}: æ¸…ç†äº†ç¬¬{min_keep_round}è½®ä¹‹å‰çš„chunks")
                
            conn.close()
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§chunkså¤±è´¥: {e}")
    
    def reconstruct_model_from_chunks(self, round_num: int, target_model: nn.Module) -> bool:
        """
        ä»chunksé‡æ„æ¨¡å‹å‚æ•°
        
        Args:
            round_num: è¦é‡æ„çš„è½®æ¬¡
            target_model: ç›®æ ‡æ¨¡å‹ï¼Œå‚æ•°å°†è¢«é‡æ„çš„å€¼æ›¿æ¢
            
        Returns:
            æ˜¯å¦é‡æ„æˆåŠŸ
        """
        try:
            chunks = self.load_chunks_by_round(round_num)
            if not chunks:
                logger.warning(f"âš ï¸ ç¬¬{round_num}è½®æ²¡æœ‰æ‰¾åˆ°chunks")
                return False
            
            # è·å–ç›®æ ‡æ¨¡å‹çš„å‚æ•°å½¢çŠ¶ä¿¡æ¯
            model_params = self.model_to_params(target_model)
            
            # åˆå§‹åŒ–é‡æ„å‚æ•°å­—å…¸
            reconstructed_params = {}
            for param_name, param_array in model_params.items():
                reconstructed_params[param_name] = np.zeros_like(param_array)
            
            # ä»chunksé‡æ„å‚æ•°
            for chunk_info, chunk_data in chunks:
                data_ptr = 0
                
                for param_name, parts in chunk_info['parts'].items():
                    if param_name not in reconstructed_params:
                        logger.warning(f"âš ï¸ å‚æ•° {param_name} åœ¨ç›®æ ‡æ¨¡å‹ä¸­ä¸å­˜åœ¨")
                        continue
                        
                    for flat_start, flat_end, shape in parts:
                        chunk_size = flat_end - flat_start
                        part_data = chunk_data[data_ptr:data_ptr + chunk_size]
                        
                        # ç›´æ¥å¯¹æ‰å¹³åŒ–çš„å‚æ•°æ•°ç»„è¿›è¡Œèµ‹å€¼
                        param_flat = reconstructed_params[param_name].reshape(-1)
                        param_flat[flat_start:flat_end] = part_data
                        
                        data_ptr += chunk_size
            
            # å°†é‡æ„çš„å‚æ•°åŠ è½½åˆ°æ¨¡å‹
            self.params_to_model(reconstructed_params, target_model)
            
            logger.info(f"ğŸ“¦ èŠ‚ç‚¹ {self.client_id}: æˆåŠŸä»chunksé‡æ„ç¬¬{round_num}è½®çš„æ¨¡å‹")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‡æ„æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())
            return False