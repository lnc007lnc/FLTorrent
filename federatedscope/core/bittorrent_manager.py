"""
BitTorrentåè®®ç®¡ç†å™¨
å®ç°ç»å…¸BitTorrentåè®®ç”¨äºFederatedScopeä¸­çš„chunkäº¤æ¢
"""

import time
import hashlib
import random
import logging
from typing import Dict, Set, List, Tuple, Optional, Any

logger = logging.getLogger(__name__)


class BitTorrentManager:
    """ç®¡ç†BitTorrentåè®®çš„æ ¸å¿ƒé€»è¾‘ï¼ˆåŒ…å«å…³é”®Bugä¿®å¤ï¼‰"""
    
    def __init__(self, client_id: int, round_num: int, chunk_manager, comm_manager, neighbors: List[int]):
        self.client_id = client_id
        self.round_num = round_num  # ğŸ”´ å…³é”®ï¼šå½“å‰è½®æ¬¡
        self.chunk_manager = chunk_manager
        self.comm_manager = comm_manager
        self.neighbors = neighbors  # ğŸ”§ ä¿®å¤ï¼šç›´æ¥ä¼ å…¥é‚»å±…åˆ—è¡¨
        
        # BitTorrentçŠ¶æ€
        self.peer_bitfields: Dict[int, Dict] = {}  # {peer_id: bitfield}
        self.interested_in: Set[int] = set()  # æ„Ÿå…´è¶£çš„peers
        self.interested_by: Set[int] = set()  # å¯¹æˆ‘æ„Ÿå…´è¶£çš„peers
        self.choked_peers: Set[int] = set()  # è¢«chokeçš„peers
        self.unchoked_peers: Set[int] = set()  # unchokedçš„peersï¼ˆå¯ä»¥ä¸‹è½½ï¼‰
        
        # æ€§èƒ½ç®¡ç†
        self.download_rate: Dict[int, float] = {}  # {peer_id: bytes/sec}
        self.upload_rate: Dict[int, float] = {}  # {peer_id: bytes/sec}
        self.last_unchoke_time = 0
        self.optimistic_unchoke_peer = None
        
        # ğŸ”§ ä¿®å¤ï¼šç®€åŒ–çŠ¶æ€ç®¡ç†ï¼Œé¿å…å¤æ‚çš„é”æœºåˆ¶
        # FederatedScopeæ˜¯å•çº¿ç¨‹æ¶ˆæ¯é©±åŠ¨ï¼Œä¸éœ€è¦é”
        
        # ğŸ”§ Bugä¿®å¤2: é˜²æ­»é”æœºåˆ¶
        self.ever_unchoked: Set[int] = set()  # è®°å½•æ›¾ç»unchokeè¿‡çš„peers
        self.last_activity: Dict[int, float] = {}  # {peer_id: timestamp} æœ€åæ´»åŠ¨æ—¶é—´
        self.stalled_threshold = 30.0  # 30ç§’æ— æ´»åŠ¨è§†ä¸ºstalled
        
        # ğŸ”§ Bugä¿®å¤3: æ¶ˆæ¯é‡ä¼ æœºåˆ¶
        self.pending_requests: Dict[Tuple, Tuple[int, float]] = {}  # {(source_id, chunk_id): (peer_id, timestamp)}
        self.request_timeout = 5.0  # 5ç§’è¯·æ±‚è¶…æ—¶
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_count: Dict[Tuple, int] = {}  # {(source_id, chunk_id): count}
        
        # ğŸ†• åŒæ± è¯·æ±‚ç®¡ç†ç³»ç»Ÿ - è§£å†³ä¼˜å…ˆçº§åè½¬å’Œé‡å¤é€‰æ‹©é—®é¢˜
        self.MAX_ACTIVE_REQUESTS = 2  # æ´»è·ƒè¯·æ±‚æ± å¤§å°ï¼šå®é™…å‘é€çš„å¹¶å‘è¯·æ±‚æ•°é‡
        self.MAX_PENDING_QUEUE = 2    # å¾…å‘é€é˜Ÿåˆ—æ± å¤§å°ï¼šé¢„é€‰æ‹©çš„chunké˜Ÿåˆ—å¤§å°
        self.pending_queue: List[Tuple] = []  # å¾…å‘é€é˜Ÿåˆ—ï¼šæŒ‰é‡è¦æ€§æ’åºçš„chunkåˆ—è¡¨
        
        # ğŸ”§ Bugä¿®å¤4: ç¡®ä¿æœ€å°unchokeæ•°é‡
        self.MIN_UNCHOKE_SLOTS = 1  # è‡³å°‘ä¿æŒ1ä¸ªunchokeï¼Œé˜²æ­¢å®Œå…¨æ­»é”
        self.MAX_UPLOAD_SLOTS = 4
        
        # ğŸ”§ ä¿®å¤ï¼šä¸ä½¿ç”¨åå°çº¿ç¨‹ï¼Œé€šè¿‡æ¶ˆæ¯å›è°ƒæ£€æŸ¥è¶…æ—¶
        self.last_timeout_check = time.time()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_downloaded = 0
        self.total_uploaded = 0
        self.chunks_per_client = 10  # é»˜è®¤å€¼ï¼Œå¯é…ç½®
        
        # ğŸ”§ CRITICAL FIX: Exchange state management
        self.is_stopped = False  # Stop flag for exchange termination
        
        logger.info(f"[BT] BitTorrentManager initialized for client {client_id}, round {round_num}")
        
    def start_exchange(self):
        """å¯åŠ¨BitTorrent chunkäº¤æ¢æµç¨‹ï¼ˆæ— éœ€Trackerï¼‰"""
        logger.info(f"[BT] Client {self.client_id}: Starting BitTorrent exchange")
        logger.info(f"[BT] Client {self.client_id}: Neighbors: {self.neighbors}")
        
        # 1. ç›´æ¥å‘æ‰€æœ‰æ‹“æ‰‘é‚»å±…å‘é€bitfield
        for neighbor_id in self.neighbors:
            logger.info(f"[BT] Client {self.client_id}: Sending bitfield to neighbor {neighbor_id}")
            self._send_bitfield(neighbor_id)
        
        # 2. å¯åŠ¨å®šæœŸunchokeç®—æ³•ï¼ˆæ¯10ç§’ï¼‰
        self._schedule_regular_unchoke()
        
        # 3. å¯åŠ¨optimistic unchokeï¼ˆæ¯30ç§’ï¼‰
        self._schedule_optimistic_unchoke()
        
    def handle_bitfield(self, sender_id: int, bitfield_content: Dict):
        """å¤„ç†æ¥æ”¶åˆ°çš„bitfieldæ¶ˆæ¯ï¼ˆåŒ…å«é‡è¦æ€§åˆ†æ•°ï¼‰"""
        # ğŸ”§ CRITICAL FIX: Check if exchange is stopped
        if self.is_stopped:
            logger.debug(f"[BT] Client {self.client_id}: Ignoring bitfield from peer {sender_id} - exchange stopped")
            return
        
        # ğŸ†• å¤„ç†æ–°æ ¼å¼çš„bitfieldï¼ˆåŒ…å«é‡è¦æ€§åˆ†æ•°ï¼‰
        if isinstance(bitfield_content, dict) and 'bitfield' in bitfield_content:
            # æ–°æ ¼å¼ï¼š{round_num: x, bitfield: [{round, source, chunk, importance_score}, ...]}
            bitfield_list = bitfield_content.get('bitfield', [])
            
            # è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼å¹¶å­˜å‚¨é‡è¦æ€§åˆ†æ•°
            bitfield = {}
            if not hasattr(self, 'peer_importance_scores'):
                self.peer_importance_scores = {}  # {peer_id: {chunk_key: importance_score}}
            
            if sender_id not in self.peer_importance_scores:
                self.peer_importance_scores[sender_id] = {}
            
            for chunk_entry in bitfield_list:
                chunk_key = (chunk_entry['round'], chunk_entry['source'], chunk_entry['chunk'])
                bitfield[chunk_key] = True
                
                # ğŸ†• å­˜å‚¨é‡è¦æ€§åˆ†æ•°
                importance_score = chunk_entry.get('importance_score', 0.0)
                self.peer_importance_scores[sender_id][chunk_key] = importance_score
                
                logger.debug(f"[BT] Client {self.client_id}: Peer {sender_id} has chunk {chunk_key} with importance {importance_score:.4f}")
        else:
            # å…¼å®¹æ—§æ ¼å¼
            bitfield = bitfield_content
        
        self.peer_bitfields[sender_id] = bitfield
        logger.debug(f"[BT] Client {self.client_id}: Received bitfield from peer {sender_id} with {len(bitfield)} chunks")
        
        # ğŸ”§ è°ƒè¯•ï¼šè¾“å‡ºè¯¦ç»†çš„bitfieldåˆ†æ
        logger.debug(f"[BT] Client {self.client_id}: BitTorrent Manager received bitfield from peer {sender_id}:")
        if bitfield:
            for chunk_key, has_chunk in bitfield.items():
                round_num, source_id, chunk_id = chunk_key
                importance_score = self.peer_importance_scores.get(sender_id, {}).get(chunk_key, 0.0)
                logger.debug(f"[BT] Client {self.client_id}: - Round {round_num}, Source {source_id}, Chunk {chunk_id}: {has_chunk} (importance: {importance_score:.4f})")
        else:
            logger.warning(f"[BT] Client {self.client_id}: âš ï¸ BitTorrent Manager got EMPTY bitfield from peer {sender_id}!")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æˆ‘éœ€è¦çš„chunks
        if self._has_interesting_chunks(sender_id):
            logger.debug(f"[BT] Client {self.client_id}: Peer {sender_id} has interesting chunks, sending interested")
            self._send_interested(sender_id)
        else:
            logger.info(f"[BT] Client {self.client_id}: Peer {sender_id} has no interesting chunks")
            
    def handle_interested(self, sender_id: int):
        """å¤„ç†interestedæ¶ˆæ¯"""
        self.interested_by.add(sender_id)
        logger.debug(f"[BT] Client {self.client_id}: Peer {sender_id} is interested")
        # æ ¹æ®å½“å‰upload slotså†³å®šæ˜¯å¦unchoke
        self._evaluate_unchoke(sender_id)
        
    def handle_request(self, sender_id: int, round_num: int, source_client_id: int, chunk_id: int):
        """å¤„ç†chunkè¯·æ±‚"""
        # ğŸ”§ CRITICAL FIX: Check if exchange is stopped
        if self.is_stopped:
            logger.debug(f"[BT] Client {self.client_id}: Ignoring request from peer {sender_id} - exchange stopped")
            return
        logger.debug(f"[BT-HANDLE] Client {self.client_id}: Handling request from {sender_id} for chunk {source_client_id}:{chunk_id}")
        
        # ğŸ”´ éªŒè¯è½®æ¬¡åŒ¹é…
        if round_num != self.round_num:
            logger.warning(f"[BT-HANDLE] Client {self.client_id}: Round mismatch - Request round {round_num} vs BitTorrent round {self.round_num}")
            logger.warning(f"[BT-HANDLE] Client {self.client_id}: Skipping request due to round mismatch")
            return
            
        if sender_id not in self.choked_peers:
            logger.debug(f"[BT-HANDLE] Client {self.client_id}: Peer {sender_id} is not choked, processing request")
            # å‘é€chunkæ•°æ®
            # ğŸ”´ æ·»åŠ round_numå‚æ•°åˆ°get_chunk_data
            logger.debug(f"[BT-HANDLE] Client {self.client_id}: Querying chunk_data with params (round={round_num}, source_client={source_client_id}, chunk_id={chunk_id})")
            chunk_data = self.chunk_manager.get_chunk_data(round_num, source_client_id, chunk_id)
            if chunk_data is not None:
                # å‘é€chunkæ•°æ®ï¼Œå³ä½¿æ˜¯ç©ºçš„chunkä¹Ÿè¦å‘é€
                chunk_size = len(chunk_data) if hasattr(chunk_data, '__len__') else 0
                if chunk_size > 0:
                    logger.debug(f"[BT-HANDLE] Client {self.client_id}: Found non-empty chunk data (size={chunk_size}), sending piece to {sender_id}")
                else:
                    logger.debug(f"[BT-HANDLE] Client {self.client_id}: Found empty chunk data (size={chunk_size}), sending empty piece to {sender_id}")
                
                self._send_piece(sender_id, round_num, source_client_id, chunk_id, chunk_data)
                logger.debug(f"[BT-HANDLE] Client {self.client_id}: Successfully sent chunk {source_client_id}:{chunk_id} to peer {sender_id}")
            else:
                logger.warning(f"[BT-HANDLE] Client {self.client_id}: Chunk {source_client_id}:{chunk_id} not found in database (round={round_num})")
                logger.warning(f"[BT-HANDLE] Client {self.client_id}: Database query returned: {chunk_data}")
        else:
            logger.info(f"[BT-HANDLE] Client {self.client_id}: Peer {sender_id} is choked, ignoring request")
            
    def handle_piece(self, sender_id: int, round_num: int, source_client_id: int, chunk_id: int, chunk_data: bytes, checksum: str):
        """
        å¤„ç†æ¥æ”¶åˆ°çš„chunkæ•°æ®ï¼ˆåŒ…å«å®Œæ•´æ€§æ ¡éªŒï¼‰
        ğŸ”´ å…³é”®ä¿®æ”¹ï¼šéªŒè¯round_numåŒ¹é…
        """
        # ğŸ”§ CRITICAL FIX: Check if exchange is stopped
        if self.is_stopped:
            logger.debug(f"[BT] Client {self.client_id}: Ignoring piece from peer {sender_id} - exchange stopped")
            return
        logger.debug(f"[BT-PIECE] Client {self.client_id}: Received piece from {sender_id} for chunk {source_client_id}:{chunk_id} (piece_round={round_num}, bt_round={self.round_num}, timestamp={time.time():.3f})")
        
        # ğŸ”´ éªŒè¯è½®æ¬¡æ˜¯å¦åŒ¹é…
        if round_num != self.round_num:
            logger.warning(f"[BT-PIECE] Client {self.client_id}: Round mismatch - Piece round {round_num} vs BitTorrent round {self.round_num}")
            logger.warning(f"[BT-PIECE] Client {self.client_id}: Rejecting piece due to round mismatch")
            return False
            
        # ğŸ”§ ä¿®å¤ï¼šchunk_dataç°åœ¨æ˜¯base64ç¼–ç çš„å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£ç åæ ¡éªŒ
        logger.debug(f"[BT-PIECE] Client {self.client_id}: Received encoded chunk data, type={type(chunk_data)}, size={len(chunk_data)}")
        
        # è§£ç base64æ•°æ®
        try:
            import base64
            import pickle
            decoded_data = base64.b64decode(chunk_data.encode('utf-8'))
            logger.debug(f"[BT-PIECE] Client {self.client_id}: Decoded base64 data, size={len(decoded_data)}")
        except Exception as e:
            logger.error(f"[BT-PIECE] Client {self.client_id}: Failed to decode base64 data: {e}")
            return False
        
        # å¯¹è§£ç åçš„åºåˆ—åŒ–æ•°æ®è®¡ç®—å“ˆå¸Œ
        calculated_checksum = hashlib.sha256(decoded_data).hexdigest()
        logger.debug(f"[BT-PIECE] Client {self.client_id}: Checksum verification - calculated={calculated_checksum[:8]}..., received={checksum[:8]}..., size={len(decoded_data)}")
        
        if calculated_checksum != checksum:
            logger.error(f"[BT-PIECE] Client {self.client_id}: Chunk integrity check failed for {source_client_id}:{chunk_id}")
            logger.error(f"[BT-PIECE] Client {self.client_id}: Expected={checksum}, Got={calculated_checksum}")
            # é‡æ–°è¯·æ±‚è¿™ä¸ªchunk
            chunk_key = (round_num, source_client_id, chunk_id)
            self.retry_count[chunk_key] = self.retry_count.get(chunk_key, 0) + 1
            logger.warning(f"[BT-PIECE] Client {self.client_id}: Retry count for chunk {chunk_key}: {self.retry_count[chunk_key]}")
            return False
        
        # ğŸ”§ ååºåˆ—åŒ–å¾—åˆ°åŸå§‹chunkæ•°æ®
        try:
            deserialized_data = pickle.loads(decoded_data)
            logger.debug(f"[BT-PIECE] Client {self.client_id}: Successfully deserialized chunk data, type={type(deserialized_data)}")
        except Exception as e:
            logger.error(f"[BT-PIECE] Client {self.client_id}: Failed to deserialize chunk data: {e}")
            return False
        
        # ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“
        # ğŸ”´ ä¼ é€’round_numåˆ°saveæ–¹æ³•ï¼Œä½¿ç”¨ååºåˆ—åŒ–åçš„æ•°æ®
        self.chunk_manager.save_remote_chunk(round_num, source_client_id, chunk_id, deserialized_data)
        
        # æ¸…é™¤pendingè¯·æ±‚
        chunk_key = (round_num, source_client_id, chunk_id)
        if chunk_key in self.pending_requests:
            logger.debug(f"[BT-PIECE] Client {self.client_id}: Clearing pending request for chunk {chunk_key}")
            del self.pending_requests[chunk_key]
            logger.debug(f"[BT-PIECE] Client {self.client_id}: Active pool: {len(self.pending_requests)}/{self.MAX_ACTIVE_REQUESTS}, Queue: {len(self.pending_queue)}/{self.MAX_PENDING_QUEUE}")
            
            # ğŸ†• åŒæ± ç³»ç»Ÿï¼šä»é˜Ÿåˆ—æ± è½¬ç§»è¯·æ±‚åˆ°æ´»è·ƒæ± 
            self._transfer_from_queue_to_active()
        else:
            logger.debug(f"[BT-PIECE] Client {self.client_id}: No pending request found for chunk {chunk_key}")
        
        # å‘æ‰€æœ‰é‚»å±…å‘é€haveæ¶ˆæ¯
        # ğŸ”´ ä¼ é€’round_numä¿¡æ¯
        logger.debug(f"[BT-PIECE] Client {self.client_id}: Broadcasting have message for chunk {source_client_id}:{chunk_id}")
        self._broadcast_have(round_num, source_client_id, chunk_id)
        
        # æ›´æ–°ä¸‹è½½é€Ÿç‡å’Œæ´»åŠ¨æ—¶é—´
        self._update_download_rate(sender_id, len(decoded_data))
        self.last_activity[sender_id] = time.time()
        self.total_downloaded += len(decoded_data)
        
        logger.debug(f"[BT] Client {self.client_id}: Received chunk {source_client_id}:{chunk_id} from peer {sender_id}")
        return True
        
    def handle_have(self, sender_id: int, round_num: int, source_client_id: int, chunk_id: int, importance_score: float = 0.0):
        """å¤„ç†haveæ¶ˆæ¯ï¼ˆåŒ…å«é‡è¦æ€§åˆ†æ•°ï¼‰"""
        if round_num != self.round_num:
            return
            
        chunk_key = (round_num, source_client_id, chunk_id)
        if sender_id not in self.peer_bitfields:
            self.peer_bitfields[sender_id] = {}
        self.peer_bitfields[sender_id][chunk_key] = True
        
        # ğŸ†• å­˜å‚¨é‡è¦æ€§åˆ†æ•°
        if not hasattr(self, 'peer_importance_scores'):
            self.peer_importance_scores = {}
        if sender_id not in self.peer_importance_scores:
            self.peer_importance_scores[sender_id] = {}
        
        self.peer_importance_scores[sender_id][chunk_key] = importance_score
        
        logger.debug(f"[BT] Client {self.client_id}: Peer {sender_id} has chunk {source_client_id}:{chunk_id} with importance {importance_score:.4f}")
        
    def handle_choke(self, sender_id: int):
        """å¤„ç†chokeæ¶ˆæ¯"""
        self.choked_peers.add(sender_id)
        logger.debug(f"[BT] Client {self.client_id}: Choked by peer {sender_id}")
        
    def handle_unchoke(self, sender_id: int):
        """å¤„ç†unchokeæ¶ˆæ¯"""
        self.choked_peers.discard(sender_id)
        logger.debug(f"[BT] Client {self.client_id}: Unchoked by peer {sender_id}")
        
    def _importance_guided_selection(self) -> Optional[Tuple]:
        """é‡è¦æ€§æŒ‡å¯¼çš„chunké€‰æ‹©ç®—æ³•ï¼ˆimportance-first + rarest-firstæ··åˆç­–ç•¥ï¼‰"""
        # ğŸ†• é‡è¦æ€§åˆ†æ•°å·®å¼‚é˜ˆå€¼ï¼ˆå½“ä¸¤ä¸ªchunké‡è¦æ€§å·®å¼‚å°äºè¯¥å€¼æ—¶è®¤ä¸ºç›¸ä¼¼ï¼‰
        IMPORTANCE_SIMILARITY_THRESHOLD = 0.01  # 0.01 means chunks with importance difference < 1% are considered similar
        
        # ç»Ÿè®¡æ¯ä¸ªchunkçš„ç¨€æœ‰åº¦
        chunk_availability = {}
        for peer_id, bitfield in self.peer_bitfields.items():
            for chunk_key, has_chunk in bitfield.items():
                # ğŸ”´ åªè€ƒè™‘å½“å‰è½®æ¬¡çš„chunks
                if has_chunk and chunk_key[0] == self.round_num:
                    chunk_availability[chunk_key] = chunk_availability.get(chunk_key, 0) + 1
        
        # é€‰æ‹©å¯è·å¾—çš„chunks
        my_bitfield = self.chunk_manager.get_global_bitfield(self.round_num)
        
        # ğŸ”§ æ’é™¤å·²æ‹¥æœ‰å’Œæ­£åœ¨è¯·æ±‚çš„chunks
        needed_chunks = []
        for chunk_key, availability_count in chunk_availability.items():
            if chunk_key not in my_bitfield and chunk_key not in self.pending_requests:
                # ğŸ†• è·å–chunké‡è¦æ€§åˆ†æ•°
                importance_score = self._get_chunk_importance_score(chunk_key)
                needed_chunks.append({
                    'chunk_key': chunk_key,
                    'availability': availability_count,
                    'importance': importance_score
                })
        
        # ğŸ”§ è°ƒè¯•ä¿¡æ¯
        pending_chunks = [k for k, v in chunk_availability.items() if k in self.pending_requests]
        already_have = [k for k, v in chunk_availability.items() if k in my_bitfield]
        
        if not chunk_availability:
            if not hasattr(self, '_logged_no_chunks'):
                logger.info(f"[BT] Client {self.client_id}: No chunks available from peers. Peer count: {len(self.peer_bitfields)}")
                for peer_id, bitfield in self.peer_bitfields.items():
                    logger.info(f"[BT] Client {self.client_id}: Peer {peer_id} bitfield size: {len(bitfield)}")
                self._logged_no_chunks = True
        elif not needed_chunks:
            total_available = len(chunk_availability)
            pending_count = len(pending_chunks)
            have_count = len(already_have)
            logger.debug(f"[BT] Client {self.client_id}: No needed chunks - Total: {total_available}, Already have: {have_count}, Pending: {pending_count}")
            
            if not hasattr(self, '_logged_all_chunks'):
                logger.info(f"[BT] Client {self.client_id}: All chunks handled - My chunks: {len(my_bitfield)}, Pending requests: {len(self.pending_requests)}")
                self._logged_all_chunks = True
        
        if needed_chunks:
            # ğŸ†• é‡è¦æ€§æŒ‡å¯¼çš„é€‰æ‹©ç­–ç•¥
            logger.debug(f"[BT] Client {self.client_id}: Evaluating {len(needed_chunks)} candidate chunks for selection")
            
            # 1. æŒ‰é‡è¦æ€§åˆ†æ•°é™åºæ’åº
            needed_chunks.sort(key=lambda x: x['importance'], reverse=True)
            
            if len(needed_chunks) == 1:
                selected = needed_chunks[0]
                logger.debug(f"[BT] Client {self.client_id}: Selected only candidate chunk {selected['chunk_key']} (importance: {selected['importance']:.4f}, rarity: {selected['availability']})")
                return selected['chunk_key']
            
            # 2. æ‰¾åˆ°é‡è¦æ€§æœ€é«˜çš„chunk
            highest_importance = needed_chunks[0]['importance']
            
            # 3. æ‰¾åˆ°æ‰€æœ‰ä¸æœ€é«˜é‡è¦æ€§ç›¸è¿‘çš„chunks
            similar_importance_chunks = []
            for chunk in needed_chunks:
                importance_diff = abs(chunk['importance'] - highest_importance)
                if importance_diff <= IMPORTANCE_SIMILARITY_THRESHOLD:
                    similar_importance_chunks.append(chunk)
                else:
                    break  # ç”±äºå·²ç»æ’åºï¼Œåç»­chunksé‡è¦æ€§æ›´ä½
            
            logger.debug(f"[BT] Client {self.client_id}: Found {len(similar_importance_chunks)} chunks with similar high importance (threshold: {IMPORTANCE_SIMILARITY_THRESHOLD})")
            
            # 4. åœ¨ç›¸ä¼¼é‡è¦æ€§çš„chunksä¸­æŒ‰ç¨€æœ‰åº¦é€‰æ‹©
            if len(similar_importance_chunks) == 1:
                selected = similar_importance_chunks[0]
                logger.info(f"[BT] Client {self.client_id}: Selected chunk {selected['chunk_key']} by importance priority (importance: {selected['importance']:.4f}, rarity: {selected['availability']})")
                return selected['chunk_key']
            else:
                # æŒ‰ç¨€æœ‰åº¦æ’åºï¼ˆè¶Šå°‘peeræ‹¥æœ‰è¶Šç¨€æœ‰ï¼‰
                similar_importance_chunks.sort(key=lambda x: (x['availability'], random.random()))
                selected = similar_importance_chunks[0]
                logger.info(f"[BT] Client {self.client_id}: Selected chunk {selected['chunk_key']} by rarity among high-importance chunks (importance: {selected['importance']:.4f}, rarity: {selected['availability']})")
                return selected['chunk_key']
        
        return None
    
    def _transfer_from_queue_to_active(self):
        """ä»å¾…å‘é€é˜Ÿåˆ—è½¬ç§»è¯·æ±‚åˆ°æ´»è·ƒæ± """
        while (len(self.pending_requests) < self.MAX_ACTIVE_REQUESTS and 
               len(self.pending_queue) > 0):
            
            # ä»é˜Ÿåˆ—å¤´éƒ¨å–å‡ºchunkï¼ˆå·²æŒ‰é‡è¦æ€§æ’åºï¼‰
            chunk_key = self.pending_queue.pop(0)
            
            # æ£€æŸ¥chunkæ˜¯å¦ä»ç„¶éœ€è¦
            my_bitfield = self.chunk_manager.get_global_bitfield(self.round_num)
            if chunk_key in my_bitfield or chunk_key in self.pending_requests:
                continue  # è·³è¿‡å·²æ‹¥æœ‰æˆ–æ­£åœ¨è¯·æ±‚çš„chunk
            
            # æ‰¾åˆ°æ‹¥æœ‰è¯¥chunkçš„peer
            peer_id = self._find_peer_with_chunk(chunk_key)
            if peer_id and peer_id not in self.choked_peers:
                round_num, source_id, chunk_id = chunk_key
                success = self._send_request(peer_id, source_id, chunk_id)
                if success:
                    logger.debug(f"[BT-POOL] Client {self.client_id}: Transferred chunk {chunk_key} from queue to active pool")
                    break  # æˆåŠŸè½¬ç§»ä¸€ä¸ªè¯·æ±‚
                else:
                    logger.debug(f"[BT-POOL] Client {self.client_id}: Failed to transfer chunk {chunk_key} to active pool")
            else:
                logger.debug(f"[BT-POOL] Client {self.client_id}: No available peer for chunk {chunk_key}")
    
    def _fill_pending_queue(self):
        """å¡«å……å¾…å‘é€é˜Ÿåˆ—ï¼ˆåªåœ¨é˜Ÿåˆ—ä¸ºç©ºæ—¶è°ƒç”¨ï¼‰"""
        if len(self.pending_queue) > 0:
            return  # é˜Ÿåˆ—ä¸ä¸ºç©ºï¼Œä¸éœ€è¦å¡«å……
        
        logger.debug(f"[BT-POOL] Client {self.client_id}: Filling pending queue...")
        
        # è·å–æ‰€æœ‰éœ€è¦çš„chunkså¹¶æŒ‰é‡è¦æ€§æ’åº
        needed_chunks = []
        
        # ç»Ÿè®¡æ¯ä¸ªchunkçš„ç¨€æœ‰åº¦
        chunk_availability = {}
        for peer_id, bitfield in self.peer_bitfields.items():
            for chunk_key, has_chunk in bitfield.items():
                if has_chunk and chunk_key[0] == self.round_num:
                    chunk_availability[chunk_key] = chunk_availability.get(chunk_key, 0) + 1
        
        my_bitfield = self.chunk_manager.get_global_bitfield(self.round_num)
        
        # é€‰æ‹©éœ€è¦çš„chunks
        for chunk_key, availability_count in chunk_availability.items():
            if (chunk_key not in my_bitfield and 
                chunk_key not in self.pending_requests and
                chunk_key not in self.pending_queue):
                
                importance_score = self._get_chunk_importance_score(chunk_key)
                needed_chunks.append({
                    'chunk_key': chunk_key,
                    'availability': availability_count,
                    'importance': importance_score
                })
        
        if needed_chunks:
            # æŒ‰é‡è¦æ€§æ’åºï¼Œé‡è¦æ€§é«˜çš„åœ¨å‰
            needed_chunks.sort(key=lambda x: x['importance'], reverse=True)
            
            # å¡«å……é˜Ÿåˆ—ï¼Œæœ€å¤šå¡«å……åˆ°MAX_PENDING_QUEUEå¤§å°
            for i, chunk in enumerate(needed_chunks[:self.MAX_PENDING_QUEUE]):
                self.pending_queue.append(chunk['chunk_key'])
            
            logger.info(f"[BT-POOL] Client {self.client_id}: Filled pending queue with {len(self.pending_queue)} chunks (from {len(needed_chunks)} candidates)")
            
            # è¾“å‡ºå‰å‡ ä¸ªé«˜é‡è¦æ€§chunksçš„è¯¦ç»†ä¿¡æ¯
            for i, chunk in enumerate(needed_chunks[:3]):
                logger.debug(f"[BT-POOL] Client {self.client_id}: Queue #{i+1}: {chunk['chunk_key']} (importance: {chunk['importance']:.4f}, rarity: {chunk['availability']})")
        else:
            logger.debug(f"[BT-POOL] Client {self.client_id}: No chunks available to fill queue")
    
    def _get_chunk_importance_score(self, chunk_key: Tuple[int, int, int]) -> float:
        """è·å–chunkçš„é‡è¦æ€§åˆ†æ•°"""
        round_num, source_client_id, chunk_id = chunk_key
        
        # ğŸ†• ä»æ‰€æœ‰å·²çŸ¥é‡è¦æ€§åˆ†æ•°ä¸­è·å–
        # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå·±çš„chunk
        if source_client_id == self.client_id:
            chunk_importance_scores = self.chunk_manager.get_chunk_importance_scores(round_num)
            if chunk_id in chunk_importance_scores:
                chunk_data = chunk_importance_scores[chunk_id]
                return chunk_data.get('importance_score', 0.0)
        
        # 2. ä»peerçš„bitfieldä¸­è·å–é‡è¦æ€§åˆ†æ•°
        if hasattr(self, 'peer_importance_scores'):
            for peer_id, peer_scores in self.peer_importance_scores.items():
                if chunk_key in peer_scores:
                    return peer_scores[chunk_key]
        
        # 3. é»˜è®¤è¿”å›0.0
        return 0.0
    
    def _rarest_first_selection(self) -> Optional[Tuple]:
        """Rarest First chunké€‰æ‹©ç®—æ³•ï¼ˆå…¼å®¹æ€§åˆ«åï¼‰"""
        return self._importance_guided_selection()
        
    def _regular_unchoke_algorithm(self):
        """ç»å…¸çš„Reciprocal Unchokeç®—æ³•ï¼ˆåŒ…å«é˜²æ­»é”æ”¹è¿›ï¼‰"""
        # ğŸ”§ Bugä¿®å¤6: åŠ¨æ€è°ƒæ•´upload slots
        # Staræ‹“æ‰‘ä¸­å¿ƒèŠ‚ç‚¹éœ€è¦æ›´å¤šslots
        if self._is_central_node():
            self.MAX_UPLOAD_SLOTS = 8
        
        # æ ¹æ®ä¸‹è½½é€Ÿç‡æ’åºinterested peers
        interested_peers = list(self.interested_by)
        interested_peers.sort(key=lambda p: self.download_rate.get(p, 0), reverse=True)
        
        # é€‰æ‹©å‰Nä¸ªpeersè¿›è¡Œregular unchokeï¼ˆé¢„ç•™1ä¸ªç»™optimisticï¼‰
        regular_slots = self.MAX_UPLOAD_SLOTS - 1
        new_unchoked = set(interested_peers[:regular_slots])
        
        # ğŸ”§ Bugä¿®å¤7: å…¬å¹³æ€§ä¿è¯ - ç¡®ä¿æ¯ä¸ªpeerè‡³å°‘è¢«unchokeè¿‡ä¸€æ¬¡
        for peer_id in self.interested_by:
            if peer_id not in self.ever_unchoked and len(new_unchoked) < self.MAX_UPLOAD_SLOTS:
                new_unchoked.add(peer_id)
                self.ever_unchoked.add(peer_id)
                logger.debug(f"[BT] Fairness unchoke for peer {peer_id}")
        
        # ğŸ”§ Bugä¿®å¤8: ç¡®ä¿è‡³å°‘æœ‰MIN_UNCHOKE_SLOTSä¸ªunchoke
        if len(new_unchoked) == 0 and len(self.interested_by) > 0:
            # éšæœºé€‰æ‹©ä¸€ä¸ªpeerè¿›è¡Œunchokeï¼Œé˜²æ­¢å®Œå…¨æ­»é”
            emergency_peer = random.choice(list(self.interested_by))
            new_unchoked.add(emergency_peer)
            logger.warning(f"[BT] Emergency unchoke for peer {emergency_peer}")
        
        # æ›´æ–°choke/unchokeçŠ¶æ€
        for peer_id in self.unchoked_peers - new_unchoked:
            self._send_choke(peer_id)
        for peer_id in new_unchoked - self.unchoked_peers:
            self._send_unchoke(peer_id)
            
        self.unchoked_peers = new_unchoked
        
    def _optimistic_unchoke(self):
        """Optimistic unchokeæœºåˆ¶ï¼ˆé˜²æ­»é”çš„å…³é”®ï¼‰"""
        # ä»è¢«chokeçš„interested peersä¸­éšæœºé€‰æ‹©ä¸€ä¸ª
        choked_interested = self.interested_by - self.unchoked_peers
        if choked_interested:
            # ğŸ”§ Bugä¿®å¤9: ä¼˜å…ˆé€‰æ‹©ä»æœªunchokeè¿‡çš„peer
            never_unchoked = choked_interested - self.ever_unchoked
            if never_unchoked:
                self.optimistic_unchoke_peer = random.choice(list(never_unchoked))
            else:
                self.optimistic_unchoke_peer = random.choice(list(choked_interested))
            
            self._send_unchoke(self.optimistic_unchoke_peer)
            self.unchoked_peers.add(self.optimistic_unchoke_peer)
            self.ever_unchoked.add(self.optimistic_unchoke_peer)
            logger.info(f"[BT] Optimistic unchoke for peer {self.optimistic_unchoke_peer}")
            
    def _is_central_node(self) -> bool:
        """ğŸ› Bugä¿®å¤27: åˆ¤æ–­æ˜¯å¦ä¸ºstaræ‹“æ‰‘çš„ä¸­å¿ƒèŠ‚ç‚¹"""
        # ç®€å•åˆ¤æ–­ï¼šå¦‚æœè¿æ¥çš„é‚»å±…æ•°é‡è¶…è¿‡æ€»èŠ‚ç‚¹æ•°çš„ä¸€åŠï¼Œå¯èƒ½æ˜¯ä¸­å¿ƒèŠ‚ç‚¹
        if len(self.neighbors) > 2:  # å‡è®¾3ä¸ªä»¥ä¸Šè¿æ¥ä¸ºä¸­å¿ƒèŠ‚ç‚¹
            return True
        return False
        
    def _find_alternative_peers(self, chunk_key: Tuple, exclude: int = None) -> List[int]:
        """ğŸ› Bugä¿®å¤28: æŸ¥æ‰¾æ‹¥æœ‰æŒ‡å®šchunkçš„æ›¿ä»£peers"""
        alternatives = []
        for peer_id, bitfield in self.peer_bitfields.items():
            if peer_id != exclude and chunk_key in bitfield and bitfield[chunk_key]:
                alternatives.append(peer_id)
        return alternatives
        
    def _find_peer_with_chunk(self, chunk_key: Tuple) -> Optional[int]:
        """æŸ¥æ‰¾æ‹¥æœ‰æŒ‡å®šchunkçš„peer"""
        for peer_id, bitfield in self.peer_bitfields.items():
            if chunk_key in bitfield and bitfield[chunk_key]:
                return peer_id
        return None
        
    def _send_bitfield(self, peer_id: int):
        """å‘æŒ‡å®špeerå‘é€bitfieldï¼ˆåŒ…å«é‡è¦æ€§åˆ†æ•°ï¼‰"""
        from federatedscope.core.message import Message
        
        # ğŸ”§ ä¿®å¤ï¼šå°†bitfieldè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        my_bitfield = self.chunk_manager.get_global_bitfield(self.round_num)
        logger.info(f"[BT] Client {self.client_id}: My bitfield for round {self.round_num}: {len(my_bitfield)} chunks")
        
        # ğŸ†• è·å–æœ¬è½®æ¬¡çš„chunké‡è¦æ€§åˆ†æ•°
        chunk_importance_scores = self.chunk_manager.get_chunk_importance_scores(self.round_num)
        logger.debug(f"[BT] Client {self.client_id}: Got {len(chunk_importance_scores)} importance scores for round {self.round_num}")
        
        # ğŸ”§ è°ƒè¯•ï¼šè¯¦ç»†è¾“å‡ºæˆ‘æ‹¥æœ‰çš„chunks
        if my_bitfield:
            logger.info(f"[BT] Client {self.client_id}: My chunks for round {self.round_num}:")
            for chunk_key, has_chunk in my_bitfield.items():
                if has_chunk:
                    round_num, source_id, chunk_id = chunk_key
                    # é™é»˜è®°å½•æ‹¥æœ‰çš„chunks
                    pass
        else:
            logger.warning(f"[BT] Client {self.client_id}: âš ï¸ I have NO chunks for round {self.round_num}!")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ŒåŒ…å«é‡è¦æ€§åˆ†æ•°
        bitfield_list = []
        for (round_num, source_id, chunk_id), has_chunk in my_bitfield.items():
            if has_chunk:
                # ğŸ†• è·å–chunké‡è¦æ€§åˆ†æ•°
                importance_score = 0.0
                if source_id == self.client_id and chunk_id in chunk_importance_scores:
                    # è‡ªå·±çš„chunkï¼Œä½¿ç”¨æœ¬åœ°ä¿å­˜çš„é‡è¦æ€§åˆ†æ•°
                    chunk_data = chunk_importance_scores[chunk_id]
                    importance_score = chunk_data.get('importance_score', 0.0)
                    logger.debug(f"[BT] Client {self.client_id}: Using local importance {importance_score:.4f} for own chunk {chunk_id}")
                
                bitfield_list.append({
                    'round': round_num,
                    'source': source_id,
                    'chunk': chunk_id,
                    'importance_score': importance_score  # ğŸ†• æ·»åŠ é‡è¦æ€§åˆ†æ•°
                })
        
        logger.info(f"[BT] Client {self.client_id}: Sending {len(bitfield_list)} chunks in bitfield to peer {peer_id}")
        
        self.comm_manager.send(
            Message(msg_type='bitfield',
                   sender=self.client_id,
                   receiver=[peer_id],
                   state=self.round_num,
                   content={
                       'round_num': self.round_num,
                       'bitfield': bitfield_list
                   })
        )
        
    def _send_interested(self, peer_id: int):
        """å‘é€interestedæ¶ˆæ¯"""
        self.interested_in.add(peer_id)
        from federatedscope.core.message import Message
        self.comm_manager.send(
            Message(msg_type='interested',
                   sender=self.client_id,
                   receiver=[peer_id],
                   state=self.round_num,
                   content={})
        )
        
    def _send_unchoke(self, peer_id: int):
        """å‘é€unchokeæ¶ˆæ¯"""
        from federatedscope.core.message import Message
        self.comm_manager.send(
            Message(msg_type='unchoke',
                   sender=self.client_id,
                   receiver=[peer_id],
                   state=self.round_num,
                   content={})
        )
        
    def _send_choke(self, peer_id: int):
        """å‘é€chokeæ¶ˆæ¯"""
        from federatedscope.core.message import Message
        self.comm_manager.send(
            Message(msg_type='choke',
                   sender=self.client_id,
                   receiver=[peer_id],
                   state=self.round_num,
                   content={})
        )
    
    def _broadcast_have(self, round_num: int, source_client_id: int, chunk_id: int):
        """å‘æ‰€æœ‰é‚»å±…å‘é€haveæ¶ˆæ¯ï¼ˆåŒ…å«é‡è¦æ€§åˆ†æ•°ï¼‰"""
        # ğŸ”´ haveæ¶ˆæ¯åŒ…å«è½®æ¬¡ä¿¡æ¯
        from federatedscope.core.message import Message
        
        # ğŸ†• è·å–chunké‡è¦æ€§åˆ†æ•°
        importance_score = 0.0
        if source_client_id == self.client_id:
            # è‡ªå·±çš„chunkï¼Œä»æ•°æ®åº“è·å–é‡è¦æ€§åˆ†æ•°
            chunk_importance_scores = self.chunk_manager.get_chunk_importance_scores(round_num)
            if chunk_id in chunk_importance_scores:
                chunk_data = chunk_importance_scores[chunk_id]
                importance_score = chunk_data.get('importance_score', 0.0)
                logger.debug(f"[BT] Client {self.client_id}: Broadcasting have with importance {importance_score:.4f} for own chunk {chunk_id}")
        
        for neighbor_id in self.neighbors:
            self.comm_manager.send(
                Message(msg_type='have',
                       sender=self.client_id,
                       receiver=[neighbor_id],
                       state=round_num,
                       content={
                           'round_num': round_num,
                           'source_client_id': source_client_id,
                           'chunk_id': chunk_id,
                           'importance_score': importance_score  # ğŸ†• æ·»åŠ é‡è¦æ€§åˆ†æ•°
                       })
            )
                
    def check_timeouts(self):
        """ğŸ”§ ä¿®å¤ï¼šéé˜»å¡è¶…æ—¶æ£€æŸ¥ï¼Œåœ¨æ¶ˆæ¯å¤„ç†ä¸­è°ƒç”¨"""
        current_time = time.time()
        
        # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
        if current_time - self.last_timeout_check < 1.0:
            return
        
        self.last_timeout_check = current_time
        timeout_requests = []
        
        # æŸ¥æ‰¾è¶…æ—¶çš„è¯·æ±‚
        for chunk_key, (peer_id, timestamp) in self.pending_requests.items():
            if current_time - timestamp > self.request_timeout:
                timeout_requests.append((chunk_key, peer_id))
        
        # å¤„ç†è¶…æ—¶è¯·æ±‚
        for chunk_key, peer_id in timeout_requests:
            # ğŸ”´ chunk_keyç°åœ¨åŒ…å«è½®æ¬¡ä¿¡æ¯
            round_num, source_id, chunk_id = chunk_key
            retry_count = self.retry_count.get(chunk_key, 0)
            
            if retry_count < self.max_retries:
                # é‡æ–°è¯·æ±‚
                logger.warning(f"[BT] Request timeout for chunk {chunk_key}, retrying ({retry_count+1}/{self.max_retries})")
                
                # ä»å…¶ä»–peerè¯·æ±‚
                alternative_peers = self._find_alternative_peers(chunk_key, exclude=peer_id)
                if alternative_peers:
                    new_peer = alternative_peers[0]
                    # ğŸ”´ ä¼ é€’æ­£ç¡®çš„å‚æ•°ç»™_send_request
                    self._send_request(new_peer, source_id, chunk_id)
                    self.pending_requests[chunk_key] = (new_peer, current_time)
                    self.retry_count[chunk_key] = retry_count + 1
                else:
                    logger.error(f"[BT] No alternative peers for chunk {chunk_key}")
                    del self.pending_requests[chunk_key]
            else:
                # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                logger.error(f"[BT] Max retries reached for chunk {chunk_key}")
                del self.pending_requests[chunk_key]
                if chunk_key in self.retry_count:
                    del self.retry_count[chunk_key]
                        
    def _send_request(self, peer_id: int, source_id: int, chunk_id: int):
        """å‘é€chunkè¯·æ±‚ï¼ˆåŒæ± ç®¡ç†ç³»ç»Ÿï¼‰"""
        # ğŸ”´ chunk_keyåŒ…å«è½®æ¬¡ä¿¡æ¯
        chunk_key = (self.round_num, source_id, chunk_id)
        
        # ğŸ”§ CRITICAL FIX: Check for duplicate requests to prevent network flooding
        if chunk_key in self.pending_requests:
            existing_peer, existing_time = self.pending_requests[chunk_key]
            logger.debug(f"[BT-REQ] Client {self.client_id}: DUPLICATE REQUEST PREVENTED for chunk {source_id}:{chunk_id} - already pending from peer {existing_peer} for {time.time() - existing_time:.1f}s")
            return False
        
        # ğŸ†• æ£€æŸ¥æ´»è·ƒæ± æ˜¯å¦å·²æ»¡
        if len(self.pending_requests) >= self.MAX_ACTIVE_REQUESTS:
            logger.debug(f"[BT-REQ] Client {self.client_id}: ACTIVE POOL FULL ({len(self.pending_requests)}/{self.MAX_ACTIVE_REQUESTS}), skipping request for chunk {source_id}:{chunk_id}")
            return False
        
        self.pending_requests[chunk_key] = (peer_id, time.time())
        
        logger.debug(f"[BT-REQ] Client {self.client_id}: Sending request to peer {peer_id} for chunk {source_id}:{chunk_id}")
        logger.debug(f"[BT-REQ] Client {self.client_id}: Active pool: {len(self.pending_requests)}/{self.MAX_ACTIVE_REQUESTS}, Queue: {len(self.pending_queue)}/{self.MAX_PENDING_QUEUE}")
        
        from federatedscope.core.message import Message
        self.comm_manager.send(
            Message(msg_type='request',
                   sender=self.client_id,
                   receiver=[peer_id],
                   state=self.round_num,
                   content={
                       'round_num': self.round_num,  # ğŸ”´ è¯·æ±‚çš„è½®æ¬¡
                       'source_client_id': source_id,
                       'chunk_id': chunk_id
                   })
        )
        return True
    
    def _send_piece(self, peer_id: int, round_num: int, source_client_id: int, chunk_id: int, chunk_data):
        """å‘é€chunkæ•°æ®"""
        # ğŸ”§ ä¿®å¤ï¼šé¢„åºåˆ—åŒ–chunk_dataå¹¶base64ç¼–ç é¿å…ç½‘ç»œä¼ è¾“ä¸­çš„æ•°æ®ç±»å‹å˜åŒ–
        import pickle
        import base64
        serialized_data = pickle.dumps(chunk_data)
        encoded_data = base64.b64encode(serialized_data).decode('utf-8')
        checksum = hashlib.sha256(serialized_data).hexdigest()
        
        logger.debug(f"[BT-SEND] Client {self.client_id}: Serializing chunk {source_client_id}:{chunk_id}, original_type={type(chunk_data)}, serialized_size={len(serialized_data)}, encoded_size={len(encoded_data)}")
        
        # ğŸ”´ æ¶ˆæ¯åŒ…å«è½®æ¬¡ä¿¡æ¯
        from federatedscope.core.message import Message
        self.comm_manager.send(
            Message(msg_type='piece',
                   sender=self.client_id,
                   receiver=[peer_id],
                   state=round_num,
                   content={
                       'round_num': round_num,  # ğŸ”´ chunkæ‰€å±è½®æ¬¡
                       'source_client_id': source_client_id,
                       'chunk_id': chunk_id,
                       'data': encoded_data,  # ğŸ”§ å‘é€base64ç¼–ç çš„å­—ç¬¦ä¸²
                       'checksum': checksum
                   })
        )
        
        # æ›´æ–°ä¸Šä¼ ç»Ÿè®¡
        self.total_uploaded += len(serialized_data)
        
    def _has_interesting_chunks(self, peer_id: int) -> bool:
        """æ£€æŸ¥peeræ˜¯å¦æœ‰æˆ‘éœ€è¦çš„chunks"""
        if peer_id not in self.peer_bitfields:
            return False
            
        my_bitfield = self.chunk_manager.get_global_bitfield(self.round_num)
        peer_bitfield = self.peer_bitfields[peer_id]
        
        # æ£€æŸ¥peeræ˜¯å¦æœ‰æˆ‘æ²¡æœ‰çš„chunks
        for chunk_key, has_chunk in peer_bitfield.items():
            if has_chunk and chunk_key not in my_bitfield and chunk_key[0] == self.round_num:
                return True
        return False
        
    def _evaluate_unchoke(self, peer_id: int):
        """è¯„ä¼°æ˜¯å¦unchokeæŒ‡å®špeer"""
        if len(self.unchoked_peers) < self.MAX_UPLOAD_SLOTS:
            self._send_unchoke(peer_id)
            self.unchoked_peers.add(peer_id)
            self.ever_unchoked.add(peer_id)
            
    def _schedule_regular_unchoke(self):
        """å®‰æ’å®šæœŸunchoke"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™åº”è¯¥é€šè¿‡å®šæ—¶å™¨æˆ–æ¶ˆæ¯å¾ªç¯è°ƒç”¨
        self.last_unchoke_time = time.time()
        
    def _schedule_optimistic_unchoke(self):
        """å®‰æ’optimistic unchoke"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™åº”è¯¥é€šè¿‡å®šæ—¶å™¨æˆ–æ¶ˆæ¯å¾ªç¯è°ƒç”¨
        pass
        
    def _update_download_rate(self, peer_id: int, bytes_received: int):
        """æ›´æ–°ä¸‹è½½é€Ÿç‡ç»Ÿè®¡"""
        current_time = time.time()
        if peer_id not in self.last_activity:
            self.last_activity[peer_id] = current_time
            self.download_rate[peer_id] = 0
            
        time_diff = current_time - self.last_activity[peer_id]
        if time_diff > 0:
            # ç®€å•çš„é€Ÿç‡è®¡ç®—
            rate = bytes_received / time_diff
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            if peer_id in self.download_rate:
                self.download_rate[peer_id] = 0.8 * self.download_rate[peer_id] + 0.2 * rate
            else:
                self.download_rate[peer_id] = rate
                
    def get_progress(self) -> Dict[str, Any]:
        """è·å–äº¤æ¢è¿›åº¦ä¿¡æ¯"""
        my_bitfield = self.chunk_manager.get_global_bitfield(self.round_num)
        total_expected = len(self.neighbors) * self.chunks_per_client + self.chunks_per_client  # åŒ…æ‹¬è‡ªå·±çš„chunks
        
        return {
            'chunks_collected': len(my_bitfield),
            'total_expected': total_expected,
            'progress_ratio': len(my_bitfield) / total_expected if total_expected > 0 else 0,
            'active_peers': len(self.peer_bitfields),
            'pending_requests': len(self.pending_requests),
            'bytes_downloaded': self.total_downloaded,
            'bytes_uploaded': self.total_uploaded
        }
    
    def stop_exchange(self):
        """
        ğŸ”§ CRITICAL FIX: Stop BitTorrent exchange immediately
        This method is called when server timeout occurs or new round begins
        to prevent interference with next round BitTorrent operations
        """
        logger.info(f"[BT] Client {self.client_id}: Stopping BitTorrent exchange for round {self.round_num}")
        
        # Set stop flag
        self.is_stopped = True
        
        # Clear all pending operations
        self.pending_requests.clear()
        self.retry_count.clear()
        
        # Clear peer state
        self.peer_bitfields.clear()
        self.interested_in.clear()
        self.interested_by.clear()
        self.choked_peers.clear()
        self.unchoked_peers.clear()
        self.ever_unchoked.clear()
        self.last_activity.clear()
        
        # Clear rate tracking
        self.download_rate.clear()
        self.upload_rate.clear()
        
        logger.info(f"[BT] Client {self.client_id}: BitTorrent exchange stopped successfully")
        logger.info(f"[BT] Client {self.client_id}: Final stats - Downloaded: {self.total_downloaded} bytes, Uploaded: {self.total_uploaded} bytes")