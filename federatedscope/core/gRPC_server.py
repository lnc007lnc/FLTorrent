import logging
import time
import threading
import hashlib
import pickle
import json
from collections import deque

from federatedscope.core.proto import gRPC_comm_manager_pb2, \
    gRPC_comm_manager_pb2_grpc

# ðŸš€ FIX: Import at module level to avoid import lock contention in multi-threaded gRPC server
# Dynamic imports inside functions cause thread blocking due to Python's import lock
# This is CRITICAL for gRPC server thread pool performance
from federatedscope.core.message import Message, ChunkData

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class gRPCComServeFunc(gRPC_comm_manager_pb2_grpc.gRPCComServeFuncServicer):
    def __init__(self, chunk_manager=None):
        self.msg_queue = deque()
        # ðŸš€ Streaming queues for chunk transfer
        self.chunk_stream_queues = {}  # {client_id: deque()}
        # ðŸš€ CRITICAL FIX: Add chunk_manager reference for data access (will be set by client)
        self.chunk_manager = chunk_manager

        # ðŸ”§ FIX: Singleton thread management - standby mode normally, start when needed
        self._background_processor = None
        self._processor_lock = threading.Lock()
        self._active_uploads = 0  # active upload connection counter

        # ðŸš€ FIX: Separate semaphores for CONTROL vs DATA streams
        #
        # CRITICAL ARCHITECTURE FIX:
        # Previously, control messages (streamChunks: REQUEST/HAVE/BITFIELD) and
        # data messages (uploadChunks/downloadChunks: PIECE) shared ONE semaphore.
        # This caused DEADLOCK: when persistent data connections filled the semaphore,
        # control messages couldn't get through, and downloads stalled.
        #
        # NEW DESIGN:
        # 1. Control stream (streamChunks): NO semaphore - lightweight, must always work
        # 2. Data streams (uploadChunks/downloadChunks): Separate data semaphore
        #
        # Calculation for data semaphore:
        # - 50 clients Ã— 8 neighbors Ã— 2 (Upload+Download) = 800 persistent connections
        # - Set to 1000 with buffer for safety
        self._data_semaphore = threading.Semaphore(1000)

        # ðŸ”§ DEPRECATED: Keep for backward compatibility, but control streams no longer use it
        self._streaming_semaphore = self._data_semaphore  # Alias for any old references
    
    def _ensure_background_processor(self):
        """ðŸ”§ Lazy loading: only start background processing thread when needed"""
        with self._processor_lock:
            if self._background_processor is None or not self._background_processor.is_alive():
                self._background_processor = threading.Thread(
                    target=self._background_chunk_processor,
                    daemon=True,
                    name="SharedBackgroundProcessor"
                )
                self._background_processor.start()
                logger.info("[ðŸŽ¯ gRPCServer] ðŸ”§ Background processor started (lazy-loaded)")
    
    def _background_chunk_processor(self):
        """ðŸ”§ Shared background processing thread - standby mode, processes chunk data for all clients"""
        logger.info("[ðŸŽ¯ gRPCServer] ðŸš‡ Background processor thread started, entering standby mode")
        
        while True:
            try:
                # Standby mode: sleep when no active connections
                if self._active_uploads == 0:
                    time.sleep(0.1)  # 100ms standby check interval
                    continue
                
                # Processing mode: Process message queue when there are active connections
                # Actual chunk processing logic can be added here
                time.sleep(0.01)  # 10ms processing interval
                
            except Exception as e:
                logger.error(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Background processor error: {e}")
                time.sleep(1.0)  # Error recovery interval
        
    def sendMessage(self, request, context):
        self.msg_queue.append(request)
        return gRPC_comm_manager_pb2.MessageResponse(msg='ACK')
        
    def streamChunks(self, request_iterator, context):
        """ðŸš€ Bidirectional streaming RPC for chunk control messages (REQUEST/HAVE/BITFIELD)

        ðŸ”§ ARCHITECTURE: Control streams do NOT use semaphore!
        - Control messages are lightweight (no large data transfer)
        - Control messages MUST always get through to prevent deadlock
        - Data streams (uploadChunks/downloadChunks) use separate _data_semaphore
        """
        logger.debug("[gRPCServer] streamChunks method called")
        logger.debug(f"[ðŸ” gRPCServer] streamChunks called from peer: {context.peer()}")

        # ðŸš€ NO SEMAPHORE for control streams - they must always work!

        # ðŸ”§ FIX: Simple yield points - less aggressive, minimal overhead
        message_count = 0
        YIELD_INTERVAL = 50  # Yield every 50 messages - balance between fairness and throughput

        try:
            for request in request_iterator:
                message_count += 1

                # ðŸ”§ Periodic yield: Give sendMessage RPCs a chance (only every 50 messages)
                if message_count % YIELD_INTERVAL == 0:
                    time.sleep(0)  # Just yield to scheduler, no actual sleep
                logger.debug(f"[ðŸ” gRPCServer] Received request: sender_id={request.sender_id}, receiver_id={request.receiver_id}, chunk_type={request.chunk_type}, from peer: {context.peer()}")
                # Process control messages (HAVE, BITFIELD, REQUEST, CANCEL)
                if request.chunk_type in [gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,
                                         gRPC_comm_manager_pb2.ChunkType.CHUNK_BITFIELD,
                                         gRPC_comm_manager_pb2.ChunkType.CHUNK_REQUEST,
                                         gRPC_comm_manager_pb2.ChunkType.CHUNK_CANCEL]:
                    # ðŸ”§ CRITICAL FIX: Only send REQUEST messages to the target receiver, not back to sender
                    if request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_REQUEST:
                        # REQUEST messages should only go to the receiver (target peer), not be broadcast
                        converted_msg = self._convert_chunk_to_message(request)
                        # Only queue the message if it's meant for a specific receiver
                        self.msg_queue.append(converted_msg)
                        logger.debug(f"[ðŸŽ¯ gRPCServer] Routing REQUEST from {request.sender_id} to {request.receiver_id}")
                    else:
                        # Other messages (HAVE, BITFIELD, CANCEL) can be processed normally
                        self.msg_queue.append(self._convert_chunk_to_message(request))
                    
                    # Send acknowledgment  
                    yield gRPC_comm_manager_pb2.ChunkStreamResponse(
                        sender_id=request.sender_id,
                        receiver_id=request.receiver_id,
                        success=True,
                        response_type=gRPC_comm_manager_pb2.ChunkResponseType.CHUNK_ACK,
                        round_num=request.round_num,
                        chunk_id=request.chunk_id
                    )
        except Exception as e:
            logger.error(f"[gRPCServer] streamChunks error: {e}")
            yield gRPC_comm_manager_pb2.ChunkStreamResponse(
                success=False,
                error_message=str(e)
            )
        finally:
            # ðŸš€ Control streams don't use semaphore, nothing to release
            logger.debug(f"[ðŸŽ¯ gRPCServer] streamChunks ended from {context.peer()}")

    def uploadChunks(self, request_iterator, context):
        """ðŸš€ Optimization 2: Enhanced underground pipeline mode - Never-stopping high-performance chunk upload processing

        ðŸ”§ ARCHITECTURE: Data streams use _data_semaphore (separate from control)
        - Data transfers are heavy (large PIECE messages)
        - Limit concurrent data streams to prevent resource exhaustion
        - Control streams (streamChunks) are NOT limited, ensuring REQUEST/HAVE always work
        """
        logger.debug("[ðŸŽ¯ gRPCServer] ðŸ“¤ Enhanced upload pipeline started - UNDERGROUND MODE")

        # ðŸš€ CRITICAL: Acquire DATA semaphore (separate from control streams)
        self._data_semaphore.acquire()
        logger.debug(f"[ðŸŽ¯ gRPCServer] uploadChunks acquired DATA semaphore from {context.peer()}")

        # ðŸš€ Underground pipeline performance parameters
        successful_chunks = 0
        failed_chunks = 0
        error_messages = []
        processing_start_time = time.time()
        client_id = 0  # Will be obtained from the first request

        # ðŸš€ Performance monitoring
        chunk_sizes = []
        processing_times = []
        last_performance_report = time.time()

        # ðŸ”§ FIX: Fixed yield interval - less aggressive
        YIELD_INTERVAL = 50  # Yield every 50 chunks

        def enhanced_underground_processor():
            """ðŸš€ Enhanced underground pipeline processor - High performance, never-stopping, intelligent error handling"""
            nonlocal successful_chunks, failed_chunks, error_messages, chunk_sizes, processing_times, client_id, last_performance_report

            logger.debug("[ðŸŽ¯ gRPCServer] ðŸš‡ Enhanced underground pipeline started - PERFORMANCE MODE")

            # ðŸš€ Underground pipeline batch processing optimization
            chunk_batch = []
            batch_size = 10  # Batch processing size
            batch_timeout = 0.1  # 100ms batch processing timeout
            last_batch_time = time.time()

            # ðŸ”§ FIX: Simple yield points - less aggressive
            request_count = 0

            try:
                for request in request_iterator:
                    request_count += 1

                    # ðŸ”§ Periodic yield: Give sendMessage RPCs a chance (only every 50 chunks)
                    if request_count % YIELD_INTERVAL == 0:
                        time.sleep(0)  # Just yield to scheduler
                    request_start_time = time.time()
                    
                    # ðŸš€ Get client_id (from first request)
                    if client_id == 0 and request.sender_id:
                        client_id = request.sender_id
                        logger.info(f"[ðŸŽ¯ gRPCServer] ðŸ“¤ Client {client_id} connected to upload pipeline")
                    
                    # ðŸš€ Fast request classification and processing
                    if request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_PIECE:
                        chunk_size = len(request.chunk_data) if request.chunk_data else 0
                        logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Processing CHUNK_PIECE - size={chunk_size}B, chunk={request.source_client_id}:{request.chunk_id}")
                        
                        try:
                            # ðŸš€ Batch conversion optimization - Collect to batch processing queue
                            chunk_batch.append(request)
                            
                            # ðŸš€ Intelligent batch processing trigger conditions
                            current_time = time.time()
                            should_process_batch = (
                                len(chunk_batch) >= batch_size or  # Batch size reached
                                (current_time - last_batch_time) > batch_timeout or  # Timeout
                                chunk_size > 1024 * 1024  # Large chunk processed immediately
                            )
                            
                            if should_process_batch:
                                # ðŸš€ Batch process chunks
                                batch_start_time = time.time()
                                for chunk_req in chunk_batch:
                                    converted_msg = self._convert_chunk_to_message(chunk_req)
                                    self.msg_queue.append(converted_msg)
                                    successful_chunks += 1
                                    
                                    # Performance statistics
                                    if chunk_req.chunk_data:
                                        chunk_sizes.append(len(chunk_req.chunk_data))
                                
                                batch_time = time.time() - batch_start_time
                                processing_times.append(batch_time)
                                
                                logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Batch processed: {len(chunk_batch)} chunks in {batch_time:.3f}s")
                                
                                chunk_batch.clear()
                                last_batch_time = current_time
                                
                        except Exception as e:
                            failed_chunks += 1
                            error_msg = f"Chunk processing error: {e}"
                            error_messages.append(error_msg)
                            logger.error(f"[ðŸŽ¯ gRPCServer] ðŸš‡ {error_msg}")
                            
                    elif request.chunk_type in [
                        gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,
                        gRPC_comm_manager_pb2.ChunkType.CHUNK_BITFIELD,
                        gRPC_comm_manager_pb2.ChunkType.CHUNK_CANCEL,
                        # ðŸ”§ Add new BitTorrent control message types
                        gRPC_comm_manager_pb2.ChunkType.CHUNK_INTERESTED_REQ,
                        gRPC_comm_manager_pb2.ChunkType.CHUNK_UNCHOKE_REQ,
                        gRPC_comm_manager_pb2.ChunkType.CHUNK_CHOKE_REQ
                    ]:
                        # ðŸš€ Control message fast processing channel
                        logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Processing control message - type={request.chunk_type}")
                        converted_msg = self._convert_chunk_to_message(request)
                        self.msg_queue.append(converted_msg)
                        successful_chunks += 1
                        
                    else:
                        # ðŸš€ Heartbeat message processing
                        if request.chunk_data == b'heartbeat':
                            logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸš‡ðŸ’“ Heartbeat received from client {request.sender_id}")
                        else:
                            failed_chunks += 1
                            error_msg = f"Unknown chunk type: {request.chunk_type}"
                            error_messages.append(error_msg)
                            logger.warning(f"[ðŸŽ¯ gRPCServer] ðŸš‡ {error_msg}")
                    
                    # ðŸš€ Processing time monitoring
                    processing_time = time.time() - request_start_time
                    if processing_time > 0.05:  # Slow processing over 50ms
                        logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸŒ Slow chunk processing: {processing_time:.3f}s")
                    
                    # ðŸš€ Regular performance reporting
                    current_time = time.time()
                    if current_time - last_performance_report > 120.0:  # Report every 2 minutes
                        if chunk_sizes:
                            avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes)
                            avg_processing_time = sum(processing_times) / len(processing_times)
                            logger.info(f"[ðŸŽ¯ gRPCServer] ðŸ“Š Underground pipeline performance:")
                            logger.info(f"[ðŸŽ¯ gRPCServer] ðŸ“Š   {successful_chunks} chunks processed, {failed_chunks} failed")
                            logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸ“Š   Avg chunk size: {avg_chunk_size:.0f}B, processing time: {avg_processing_time:.3f}s")
                        last_performance_report = current_time
                
                # ðŸš€ Process remaining batch data
                if chunk_batch:
                    logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Processing final batch: {len(chunk_batch)} chunks")
                    for chunk_req in chunk_batch:
                        converted_msg = self._convert_chunk_to_message(chunk_req)
                        self.msg_queue.append(converted_msg)
                        successful_chunks += 1
                
                total_processing_time = time.time() - processing_start_time
                logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Underground pipeline completed - total time: {total_processing_time:.3f}s")
                        
            except Exception as e:
                logger.error(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Underground pipeline fatal error: {e}")
                failed_chunks += 1
                error_messages.append(str(e))
            finally:
                # ðŸš€ Final performance statistics
                total_time = time.time() - processing_start_time
                logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸš‡ Underground pipeline finished:")
                logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸ“Š   Success: {successful_chunks}, Failed: {failed_chunks}")
                logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸ“Š   Total time: {total_time:.3f}s")
                if successful_chunks > 0:
                    logger.debug(f"[ðŸŽ¯ gRPCServer] ðŸ“Š   Throughput: {successful_chunks/total_time:.1f} chunks/sec")
        
        # ðŸ”§ Ensure background processing thread exists (lazy loading)
        self._ensure_background_processor()

        # ðŸš€ Process directly in current thread
        logger.debug(f"[ðŸŽ¯ gRPCServer] Processing upload request for client {client_id}")
        try:
            enhanced_underground_processor()
        finally:
            # ðŸš€ CRITICAL: Always release DATA semaphore when streaming ends
            self._data_semaphore.release()
            logger.debug(f"[ðŸŽ¯ gRPCServer] uploadChunks released DATA semaphore from {context.peer()}")

        # ðŸš€ Return processing result
        logger.debug("[ðŸŽ¯ gRPCServer] Upload processing completed")
        return gRPC_comm_manager_pb2.ChunkBatchResponse(
            client_id=client_id,
            successful_chunks=successful_chunks,
            failed_chunks=failed_chunks,
            error_messages=error_messages
        )

    def downloadChunks(self, request, context):
        """ðŸš€ Server streaming RPC for batch chunk download

        ðŸ”§ ARCHITECTURE: Data streams use _data_semaphore (separate from control)
        """
        logger.debug(f"[gRPCServer] downloadChunks method called for client {request.client_id}")

        # ðŸš€ CRITICAL: Acquire DATA semaphore (separate from control streams)
        self._data_semaphore.acquire()
        logger.debug(f"[ðŸŽ¯ gRPCServer] downloadChunks acquired DATA semaphore from {context.peer()}")

        # ðŸ”§ FIX: Simple yield points - less aggressive
        chunk_count = 0
        YIELD_INTERVAL = 50  # Yield every 50 chunks

        try:
            # Convert batch request to individual message requests
            for chunk_req in request.chunk_requests:
                chunk_count += 1

                # ðŸ”§ Periodic yield: Give sendMessage RPCs a chance (only every 50 chunks)
                if chunk_count % YIELD_INTERVAL == 0:
                    time.sleep(0)  # Just yield to scheduler
                # Create chunk request message for compatibility
                chunk_request = gRPC_comm_manager_pb2.ChunkStreamRequest(
                    sender_id=request.sender_id,   
                    receiver_id=request.client_id,          
                    round_num=request.round_num,
                    source_client_id=chunk_req.source_client_id,
                    chunk_id=chunk_req.chunk_id,
                    chunk_type=gRPC_comm_manager_pb2.ChunkType.CHUNK_REQUEST,
                    importance_score=chunk_req.importance_score
                )
                
                # Add to message queue for traditional BitTorrent compatibility
                self.msg_queue.append(self._convert_chunk_to_message(chunk_request))
                
                # ðŸš€ Enhanced error handling: Check chunk_manager and get chunk data
                chunk_data = None
                error_message = None
                
                if not hasattr(self, 'chunk_manager') or self.chunk_manager is None:
                    error_message = "Chunk manager not initialized on server"
                    logger.error(f"[gRPCServer] ðŸš« {error_message} for chunk {chunk_req.source_client_id}:{chunk_req.chunk_id}")
                else:
                    try:
                        chunk_data = self.chunk_manager.get_chunk_data(request.round_num, chunk_req.source_client_id, chunk_req.chunk_id)
                        logger.debug(f"[gRPCServer] ðŸ“¥ Chunk lookup: round={request.round_num}, source={chunk_req.source_client_id}, chunk={chunk_req.chunk_id}, found={chunk_data is not None}")
                        
                        if chunk_data is None:
                            error_message = f"Chunk {chunk_req.source_client_id}:{chunk_req.chunk_id} not found in storage"
                            
                    except Exception as e:
                        error_message = f"Error accessing chunk data: {str(e)}"
                        logger.error(f"[gRPCServer] ðŸš« Exception during chunk lookup: {e}")
                
                # ðŸš€ Unified error response handling
                if error_message:
                    logger.warning(f"[gRPCServer] ðŸ“¤ Sending NACK for chunk {chunk_req.source_client_id}:{chunk_req.chunk_id}: {error_message} from {request.sender_id}")
                    yield gRPC_comm_manager_pb2.ChunkStreamResponse(
                        sender_id=request.client_id,
                        receiver_id=request.sender_id,
                        success=False,
                        response_type=gRPC_comm_manager_pb2.ChunkResponseType.CHUNK_NACK,
                        round_num=request.round_num,
                        chunk_id=chunk_req.chunk_id,
                        error_message=error_message
                    )
                    continue
                
                # ðŸš€ Success response: Return actual chunk data
                logger.debug(f"[gRPCServer] ðŸ“¤ Sending chunk data for {chunk_req.source_client_id}:{chunk_req.chunk_id} to client {request.client_id}")
                
                # chunk_data is already bytes from optimized cache/write_queue, no need to re-serialize
                if isinstance(chunk_data, bytes):
                    serialized_data = chunk_data  # Use bytes directly
                else:
                    serialized_data = pickle.dumps(chunk_data)  # Fallback for legacy data
                checksum = hashlib.sha256(serialized_data).hexdigest()
                data_size = len(serialized_data)
                
                logger.debug(f"[gRPCServer] ðŸ“¤ Chunk data prepared: size={data_size}B, checksum={checksum[:8]}...")
                
                yield gRPC_comm_manager_pb2.ChunkStreamResponse(
                    sender_id=request.client_id,
                    receiver_id=request.sender_id,
                    success=True,
                    response_type=gRPC_comm_manager_pb2.ChunkResponseType.CHUNK_ACK,
                    round_num=request.round_num,
                    chunk_id=chunk_req.chunk_id,
                    response_data=serialized_data
                )
                
        except Exception as e:
            logger.error(f"[gRPCServer] downloadChunks error: {e}")
            yield gRPC_comm_manager_pb2.ChunkStreamResponse(
                success=False,
                error_message=str(e)
            )
        finally:
            # ðŸš€ CRITICAL: Always release DATA semaphore when streaming ends
            self._data_semaphore.release()
            logger.debug(f"[ðŸŽ¯ gRPCServer] downloadChunks released DATA semaphore from {context.peer()}")

    def _convert_chunk_to_message(self, chunk_request):
        """ðŸ”§ Convert chunk stream request to traditional message format for compatibility"""
        # This is a compatibility bridge - convert streaming chunk requests 
        # back to the traditional message format that the existing system expects
        
        if chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_PIECE:
            # ðŸš€ CRITICAL FIX: Correctly create piece message with all necessary chunk data
            # Note: Message and ChunkData are imported at module level to avoid import lock contention

            # Create ChunkData wrapper
            chunk_wrapper = ChunkData(chunk_request.chunk_data, chunk_request.checksum)

            # Create complete protobuf message with all piece data
            message_request = gRPC_comm_manager_pb2.MessageRequest()
            
            # Create traditional format Message object
            traditional_msg = Message(
                msg_type='piece',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'source_client_id': chunk_request.source_client_id,
                    'chunk_id': chunk_request.chunk_id,
                    'data': chunk_wrapper,  # ChunkData object containing raw bytes
                    'checksum': chunk_request.checksum
                }
            )
            
            logger.debug(f"[ðŸ”§ gRPCServer] Created traditional message: type={traditional_msg.msg_type}, sender={traditional_msg.sender}")
            logger.debug(f"[ðŸ”§ gRPCServer] Message content keys: {list(traditional_msg.content.keys())}")
            
            # Convert Message object to protobuf format
            message_request = traditional_msg.transform(to_list=True)
            
            logger.debug(f"[ðŸ”§ gRPCServer] Successfully converted CHUNK_PIECE to MessageRequest")
            return message_request
            
        elif chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_REQUEST:
            # Process request message
            traditional_msg = Message(
                msg_type='request',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'source_client_id': chunk_request.source_client_id,
                    'chunk_id': chunk_request.chunk_id
                }
            )
            
            return traditional_msg.transform(to_list=True)
            
        elif chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE:
            # ðŸš€ å¤„ç†haveæ¶ˆæ¯
            traditional_msg = Message(
                msg_type='have',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'source_client_id': chunk_request.source_client_id,
                    'chunk_id': chunk_request.chunk_id,
                    'importance_score': chunk_request.importance_score
                }
            )
            
            logger.debug(f"[ðŸ”§ gRPCServer] Converting CHUNK_HAVE to traditional message format")
            return traditional_msg.transform(to_list=True)
            
        elif chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_BITFIELD:
            # ðŸš€ Process bitfield message
            # Decode bitfield data
            bitfield_data = []
            if chunk_request.chunk_data:
                try:
                    bitfield_data = json.loads(chunk_request.chunk_data.decode('utf-8'))
                except Exception as e:
                    logger.error(f"[ðŸ”§ gRPCServer] Failed to decode bitfield data: {e}")
            
            traditional_msg = Message(
                msg_type='bitfield',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'bitfield': bitfield_data
                }
            )
            
            logger.debug(f"[ðŸ”§ gRPCServer] Converting CHUNK_BITFIELD to traditional message format, chunks: {len(bitfield_data)}")
            return traditional_msg.transform(to_list=True)
            
        elif chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_CANCEL:
            # ðŸš€ Process cancel message
            traditional_msg = Message(
                msg_type='cancel',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'source_client_id': chunk_request.source_client_id,
                    'chunk_id': chunk_request.chunk_id
                }
            )
            
            logger.debug(f"[ðŸ”§ gRPCServer] Converting CHUNK_CANCEL to traditional message format")
            return traditional_msg.transform(to_list=True)
            
        elif chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_INTERESTED_REQ:
            # ðŸ”§ Process interested message
            traditional_msg = Message(
                msg_type='interested',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'peer_id': chunk_request.sender_id
                }
            )
            
            logger.debug(f"[ðŸ”§ gRPCServer] Converting CHUNK_INTERESTED_REQ to traditional message format")
            return traditional_msg.transform(to_list=True)
            
        elif chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_UNCHOKE_REQ:
            # ðŸ”§ Process unchoke message
            traditional_msg = Message(
                msg_type='unchoke',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'peer_id': chunk_request.sender_id
                }
            )
            
            logger.debug(f"[ðŸ”§ gRPCServer] Converting CHUNK_UNCHOKE_REQ to traditional message format")
            return traditional_msg.transform(to_list=True)
            
        elif chunk_request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_CHOKE_REQ:
            # ðŸ”§ Process choke message
            traditional_msg = Message(
                msg_type='choke',
                sender=chunk_request.sender_id,
                receiver=[chunk_request.receiver_id],
                state=chunk_request.round_num,
                content={
                    'round_num': chunk_request.round_num,
                    'peer_id': chunk_request.sender_id
                }
            )
            
            logger.debug(f"[ðŸ”§ gRPCServer] Converting CHUNK_CHOKE_REQ to traditional message format")
            return traditional_msg.transform(to_list=True)
            
        else:
            logger.warning(f"[ðŸ”§ gRPCServer] Unknown chunk_type: {chunk_request.chunk_type}")
            # Return empty MessageRequest as fallback
            return gRPC_comm_manager_pb2.MessageRequest()

    def receive(self):
        # ðŸ”§ FIX: Replace busy-wait with sleep to avoid CPU spin
        # With 50+ clients, busy-wait caused 100% CPU usage and resource exhaustion
        wait_start = time.time()
        last_heartbeat = time.time()
        while len(self.msg_queue) == 0:
            time.sleep(0.001)  # 1ms sleep prevents CPU burning
            # ðŸ” DEBUG: Heartbeat every 10 seconds while waiting
            if time.time() - last_heartbeat >= 10.0:
                logger.warning(f"[ðŸ’“ RECV-HEARTBEAT] gRPC receive() waiting for messages, queue_size=0, wait_time={time.time()-wait_start:.1f}s")
                last_heartbeat = time.time()
        msg = self.msg_queue.popleft()
        return msg
