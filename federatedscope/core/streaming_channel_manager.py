"""
ğŸš€ gRPC Streaming Channel Manager for BitTorrent
åœ¨æ‹“æ‰‘æ„å»ºæ—¶åˆ›å»ºä¸“ç”¨streamingé€šé“ï¼Œæä¾›é«˜æ•ˆçš„chunkä¼ è¾“
"""

import grpc
import threading
import time
import logging
from typing import Dict, Optional, List, Tuple
from collections import defaultdict
import queue
import json

from federatedscope.core.proto import gRPC_comm_manager_pb2_grpc
from federatedscope.core.proto import gRPC_comm_manager_pb2

logger = logging.getLogger(__name__)


class StreamingChannel:
    """å•ä¸ªstreamingé€šé“å°è£…"""
    
    def __init__(self, peer_id, channel, stub, stream_type='bidirectional', client_id=-1, client_instance=None):
        self.peer_id = peer_id
        self.channel = channel
        self.stub = stub
        self.stream_type = stream_type
        self.client_id = client_id  # æ·»åŠ client_idæ”¯æŒ
        self.client_instance = client_instance  # æ·»åŠ å®¢æˆ·ç«¯å®ä¾‹å¼•ç”¨ï¼Œç”¨äºè°ƒç”¨å›è°ƒå‡½æ•°
        
        # Streamingå¯¹è±¡
        self.request_stream = None
        self.response_stream = None
        self.request_queue = queue.Queue()
        
        # çŠ¶æ€ç®¡ç†
        self.is_active = False
        self.last_activity = time.time()
        self.bytes_sent = 0
        self.bytes_received = 0
        self.chunks_sent = 0
        self.chunks_received = 0
        
        # çº¿ç¨‹ç®¡ç†
        self.sender_thread = None
        self.receiver_thread = None
        self.upload_sender_thread = None
        
    def start_streaming(self):
        """å¯åŠ¨streamingï¼ˆè‡ªåŠ¨å›é€€å…¼å®¹ï¼‰"""
        if self.is_active:
            return
            
        try:
            # ğŸš€ å°è¯•å¯åŠ¨å¤šæµstreaming
            self._start_multi_streaming()
            
        except Exception as e:
            # ğŸ”§ Streamingå¤±è´¥ï¼Œæ ‡è®°ä¸ºä¸å¯ç”¨ï¼Œä½†ä¸æŠ›å‡ºé”™è¯¯
            logger.warning(f"[StreamChannel] Multi-streaming not supported for peer {self.peer_id}, will use traditional messaging: {e}")
            self.is_active = False  # æ ‡è®°streamingä¸å¯ç”¨
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚ä»£ç ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
            
    def _start_multi_streaming(self):
        """å†…éƒ¨æ–¹æ³•ï¼šå¯åŠ¨å¤šæµå¹¶å‘streaming - ğŸš€ å¤šé‡ä¼˜åŒ–è®¾è®¡"""
        # ğŸš€ å¤šæµå¹¶å‘ä¼˜åŒ–ï¼šåˆ›å»ºä¸“ç”¨æµç®¡é“
        
        # ğŸ”§ CRITICAL FIX: å…ˆè®¾ç½®is_active=Trueï¼Œå†åˆ›å»ºgenerators
        self.is_active = True
        logger.info(f"[StreamChannel] ğŸš€ Activating multi-pipeline streaming to peer {self.peer_id}")
        
        # ğŸš€ ä¼˜åŒ–1ï¼šæµç®¡é“å¤ç”¨ - åˆ›å»ºä¸“ç”¨é˜Ÿåˆ—å’Œç¼“å†²åŒº
        self.control_request_queue = queue.Queue(maxsize=500)  # æ§åˆ¶æ¶ˆæ¯é˜Ÿåˆ—
        self.upload_request_queue = queue.PriorityQueue(maxsize=500)   # ğŸ¯ ä¸Šä¼ ä¼˜å…ˆé˜Ÿåˆ— (importance+rareæ’åº)
        self.download_request_queue = queue.PriorityQueue(maxsize=500) # ğŸ¯ ä¸‹è½½ä¼˜å…ˆé˜Ÿåˆ— (importance+rareæ’åº)
        
        # ğŸš€ ä¼˜åŒ–2ï¼šæ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
        self.control_msg_count = 0
        self.upload_chunk_count = 0
        self.download_request_count = 0
        self.last_activity_time = time.time()
        
        # ğŸš€ ä¼˜åŒ–3ï¼šæ™ºèƒ½æµåˆ†é…
        # 1. æ§åˆ¶æµï¼šä¸“é—¨å¤„ç†è½»é‡çº§æ§åˆ¶æ¶ˆæ¯ (HAVE, BITFIELD, INTERESTEDç­‰)
        self.control_stream = self.stub.streamChunks(self._enhanced_control_generator())
        logger.info(f"[StreamChannel] âœ… Control pipeline established for peer {self.peer_id}")
        
        # 2. ä¸Šä¼ æµï¼šä¸“é—¨ä¼ è¾“å¤§å—chunkæ•°æ® (åœ¨ä¸“ç”¨çº¿ç¨‹ä¸­ç®¡ç†)
        # 3. ä¸‹è½½æµï¼šæ‰¹é‡å¤„ç†chunkè¯·æ±‚ (åœ¨ä¸“ç”¨çº¿ç¨‹ä¸­ç®¡ç†)
        
        # ğŸš€ ä¼˜åŒ–4ï¼šä¸“ç”¨çº¿ç¨‹æ± ï¼Œæé«˜å¹¶å‘æ€§èƒ½
        self.control_receiver_thread = threading.Thread(
            target=self._enhanced_control_response_handler,
            daemon=True,
            name=f"ControlPipeline-{self.peer_id}"
        )
        
        self.download_sender_thread = threading.Thread(
            target=self._enhanced_download_batch_processor,
            daemon=True,
            name=f"DownloadPipeline-{self.peer_id}"
        )
        
        self.upload_sender_thread = threading.Thread(
            target=self._enhanced_upload_stream_processor,
            daemon=True,
            name=f"UploadPipeline-{self.peer_id}"
        )
        
        # å¯åŠ¨ä¼˜åŒ–çš„æµç®¡é“å¤„ç†å™¨
        self.control_receiver_thread.start()
        self.download_sender_thread.start()
        self.upload_sender_thread.start()
        
        logger.info(f"[StreamChannel] ğŸš€ Multi-pipeline streaming ACTIVE for peer {self.peer_id}")
        logger.info(f"[StreamChannel] ğŸ“Š Performance monitoring enabled - Control/Upload/Download pipelines ready")
            
    def _enhanced_control_generator(self):
        """ğŸš€ ä¼˜åŒ–1ï¼šå¢å¼ºçš„æ§åˆ¶æ¶ˆæ¯æµç”Ÿæˆå™¨ - ä¸“é—¨å¤„ç†è½»é‡çº§æ§åˆ¶æ¶ˆæ¯"""
        logger.info(f"[StreamChannel] ğŸ›ï¸ Control pipeline generator started for peer {self.peer_id}")
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–å‚æ•°
        heartbeat_interval = 30.0  # å¿ƒè·³é—´éš”
        last_heartbeat = time.time()
        processed_count = 0
        
        while self.is_active:
            try:
                # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨ä¸“ç”¨æ§åˆ¶é˜Ÿåˆ—ï¼Œé¿å…æ¶ˆæ¯æ··ä¹±
                try:
                    request = self.control_request_queue.get(timeout=0.1)  # æ›´å¿«çš„å“åº”æ—¶é—´
                except queue.Empty:
                    # ğŸš€ ä¼˜åŒ–ï¼šå‘é€å¿ƒè·³ä¿æŒè¿æ¥æ´»è·ƒ
                    current_time = time.time()
                    if current_time - last_heartbeat > heartbeat_interval:
                        # å‘é€è½»é‡çº§å¿ƒè·³æ¶ˆæ¯ç»´æŒè¿æ¥
                        heartbeat_request = self._create_heartbeat_message()
                        if heartbeat_request:
                            yield heartbeat_request
                            last_heartbeat = current_time
                            logger.debug(f"[StreamChannel] ğŸ’“ Heartbeat sent to peer {self.peer_id}")
                    continue
                
                if request is None:  # Sentinel for shutdown
                    logger.info(f"[StreamChannel] ğŸ›ï¸ Control pipeline shutdown for peer {self.peer_id}")
                    break
                
                # ğŸš€ ä¼˜åŒ–ï¼šæ™ºèƒ½æ¶ˆæ¯åˆ†æµ - åªå¤„ç†æ§åˆ¶æ¶ˆæ¯
                if self._is_control_message(request):
                    yield request
                    processed_count += 1
                    self.control_msg_count += 1
                    self.last_activity_time = time.time()
                    
                    # ğŸš€ æ€§èƒ½ç›‘æ§
                    if processed_count % 50 == 0:
                        logger.debug(f"[StreamChannel] ğŸ›ï¸ Control pipeline processed {processed_count} messages for peer {self.peer_id}")
                else:
                    # éæ§åˆ¶æ¶ˆæ¯é‡æ–°è·¯ç”±åˆ°æ­£ç¡®çš„é˜Ÿåˆ—
                    self._route_message_to_correct_pipeline(request)
                    
                self.control_request_queue.task_done()
                
            except Exception as e:
                logger.error(f"[StreamChannel] ğŸ›ï¸ Control generator error for peer {self.peer_id}: {e}")
                time.sleep(0.1)  # é¿å…é”™è¯¯å¾ªç¯
                
        logger.info(f"[StreamChannel] ğŸ›ï¸ Control pipeline generator ended for peer {self.peer_id}, processed {processed_count} messages")
        
    def _is_control_message(self, request) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ§åˆ¶æ¶ˆæ¯"""
        control_types = [
            gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,
            gRPC_comm_manager_pb2.ChunkType.CHUNK_BITFIELD,
            gRPC_comm_manager_pb2.ChunkType.CHUNK_CANCEL,
            # æ·»åŠ æ›´å¤šæ§åˆ¶æ¶ˆæ¯ç±»å‹
        ]
        return request.chunk_type in control_types
        
    def _create_heartbeat_message(self):
        """åˆ›å»ºå¿ƒè·³æ¶ˆæ¯ç»´æŒè¿æ¥"""
        try:
            return gRPC_comm_manager_pb2.ChunkStreamRequest(
                sender_id=self.client_id,
                receiver_id=self.peer_id,
                round_num=0,  # å¿ƒè·³æ¶ˆæ¯ä¸éœ€è¦å…·ä½“è½®æ¬¡
                chunk_type=gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,
                timestamp=int(time.time() * 1000)
            )
        except Exception:
            return None
            
    def _route_message_to_correct_pipeline(self, request):
        """å°†æ¶ˆæ¯è·¯ç”±åˆ°æ­£ç¡®çš„æµç®¡é“"""
        try:
            if request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_PIECE:
                # chunkæ•°æ® -> ä¸Šä¼ æµ (CHUNK_PIECEæŒ‰importance+rareæ’åº)
                importance_score = getattr(request, 'importance_score', 0.0)
                rarity_score = 0  # è·¯ç”±æ—¶æš‚æ—¶ä½¿ç”¨é»˜è®¤å€¼
                priority = self._calculate_request_priority(importance_score, rarity_score)
                self.upload_request_queue.put((priority, request), timeout=0.1)
                logger.debug(f"[StreamChannel] ğŸ”„ Routed CHUNK_PIECE to upload pipeline for peer {self.peer_id}")
            elif request.chunk_type == gRPC_comm_manager_pb2.ChunkType.CHUNK_REQUEST:
                # chunkè¯·æ±‚ -> ä¸‹è½½æµ (CHUNK_REQUESTæŒ‰importance+rareæ’åº)
                importance_score = getattr(request, 'importance_score', 0.0)
                rarity_score = 0  # è·¯ç”±æ—¶æš‚æ—¶ä½¿ç”¨é»˜è®¤å€¼
                priority = self._calculate_request_priority(importance_score, rarity_score)
                self.download_request_queue.put((priority, request), timeout=0.1)
                logger.debug(f"[StreamChannel] ğŸ”„ Routed CHUNK_REQUEST to download pipeline for peer {self.peer_id}")
            else:
                logger.warning(f"[StreamChannel] ğŸ”„ Unknown message type for routing: {request.chunk_type}")
        except queue.Full:
            logger.warning(f"[StreamChannel] ğŸ”„ Target pipeline queue full, dropping message for peer {self.peer_id}")
        except Exception as e:
            logger.error(f"[StreamChannel] ğŸ”„ Message routing error for peer {self.peer_id}: {e}")
                
    def _upload_request_generator(self):
        """ğŸš€ ä¸“ç”¨ä¸Šä¼ æµï¼šåªå¤„ç†chunkæ•°æ®ä¼ è¾“ - ä¿®å¤Generatorè¿‡æ—©ç»ˆæ­¢é—®é¢˜"""
        logger.info(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Upload stream generator started for peer {self.peer_id}")
        
        # ğŸ”§ FIX: æ·»åŠ ç­‰å¾…æœºåˆ¶å‚æ•°
        empty_queue_wait = 0.01  # 10msç­‰å¾…æ—¶é—´
        max_idle_time = 1.0     # æœ€å¤§ç©ºé—²æ—¶é—´1ç§’
        last_activity = time.time()
        
        while self.is_active:
            try:
                # ğŸ”§ CRITICAL FIX: ä½¿ç”¨å¸¦è¶…æ—¶çš„getæ›¿ä»£get_nowaitï¼Œå¹¶ç»§ç»­å¾ªç¯è€Œä¸æ˜¯return
                try:
                    priority_item = self.upload_request_queue.get(timeout=empty_queue_wait)
                    last_activity = time.time()  # æ›´æ–°æ´»åŠ¨æ—¶é—´
                    # ğŸ¯ ä»ä¼˜å…ˆé˜Ÿåˆ—ä¸­è§£åŒ…ï¼š(priority, request)
                    priority, request = priority_item
                    
                except queue.Empty:
                    # ğŸš€ FIX: é˜Ÿåˆ—ä¸ºç©ºæ—¶ç»§ç»­ç­‰å¾…ï¼Œè€Œä¸æ˜¯ç»ˆæ­¢generator
                    current_time = time.time()
                    idle_time = current_time - last_activity
                    
                    # å®šæœŸè¾“å‡ºè°ƒè¯•ä¿¡æ¯ï¼Œä½†ä¸è¦å¤ªé¢‘ç¹
                    if idle_time > max_idle_time and int(current_time) % 5 == 0:  # æ¯5ç§’è¾“å‡ºä¸€æ¬¡
                        logger.debug(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Waiting for data, idle for {idle_time:.1f}s")
                        last_activity = current_time  # é‡ç½®è®¡æ—¶å™¨é¿å…é‡å¤æ—¥å¿—
                    
                    continue  # ğŸš€ å…³é”®ä¿®å¤ï¼šç»§ç»­å¾ªç¯è€Œä¸æ˜¯returnç»ˆæ­¢generator
                    
                if request is None:  # Sentinel for shutdown
                    logger.info(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Received shutdown sentinel for peer {self.peer_id}")
                    break
                
                # ğŸš€ å¤„ç†æ­£å¸¸çš„chunkè¯·æ±‚
                logger.info(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Yielding chunk request for peer {self.peer_id}")
                logger.info(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Request details - round={request.round_num}, source={request.source_client_id}, chunk={request.chunk_id}, size={len(request.chunk_data)}")
                
                yield request
                self.upload_request_queue.task_done()
                
                logger.info(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Successfully yielded chunk request for peer {self.peer_id}")
                
            except Exception as e:
                logger.error(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Unexpected error: {e}")
                if not self.is_active:
                    break
                # ğŸ”§ FIX: é‡åˆ°å¼‚å¸¸æ—¶çŸ­æš‚ç­‰å¾…åç»§ç»­ï¼Œé¿å…ç´§å¯†å¾ªç¯
                time.sleep(0.1)
                
        logger.info(f"[ğŸš€ UploadGenerator] Client {self.client_id}: Upload stream generator ended for peer {self.peer_id}")
    
    def _enhanced_upload_stream_processor(self):
        """ğŸš€ ä¼˜åŒ–2ï¼šå¢å¼ºçš„ä¸Šä¼ æµå¤„ç†å™¨ - åœ°ä¸‹ç®¡é“æ¨¡å¼ä¸æ‰¹é‡ä¼ è¾“"""
        logger.info(f"[StreamChannel] ğŸ“¤ Upload pipeline processor started for peer {self.peer_id}")
        
        # ğŸš€ åœ°ä¸‹ç®¡é“æ€§èƒ½å‚æ•°
        total_chunks_uploaded = 0
        total_upload_time = 0.0
        last_performance_check = time.time()
        connection_retries = 0
        max_retries = 3
        
        # ğŸ”§ CRITICAL FIX: æŒç»­è¿è¡Œçš„upload pipelineï¼Œä¸è¦è¿‡æ—©é€€å‡º
        while self.is_active:
            try:
                logger.debug(f"[StreamChannel] ğŸ“¤ Starting persistent upload pipeline to peer {self.peer_id} (attempt {connection_retries + 1})")
                
                # ğŸš€ åœ°ä¸‹ç®¡é“æ¨¡å¼ï¼šæŒç»­è¿è¡Œçš„ä¸Šä¼ æµ
                upload_start_time = time.time()
                upload_response = self.stub.uploadChunks(self._enhanced_upload_generator())
                
                # ğŸš€ å…³é”®ï¼šæ¶ˆè´¹æœ€ç»ˆå“åº”ä»¥å®Œæˆå®¢æˆ·ç«¯æµ
                if upload_response:
                    upload_duration = time.time() - upload_start_time
                    logger.debug(f"[StreamChannel] ğŸ“¤ Upload pipeline batch completed for peer {self.peer_id}")
                    logger.debug(f"[StreamChannel] ğŸ“¤ Response: success={upload_response.successful_chunks}, failed={upload_response.failed_chunks}")
                    logger.debug(f"[StreamChannel] ğŸ“¤ Batch duration: {upload_duration:.3f}s")
                    
                    # ğŸš€ æ€§èƒ½ç»Ÿè®¡æ›´æ–°
                    total_chunks_uploaded += upload_response.successful_chunks
                    total_upload_time += upload_duration
                    
                    # ğŸ”§ FIX: é‡ç½®è¿æ¥è®¡æ•°å™¨ï¼Œç»§ç»­è¿è¡Œè€Œä¸æ˜¯é€€å‡º
                    connection_retries = 0
                    logger.debug(f"[StreamChannel] ğŸ“¤ Upload pipeline continuing for peer {self.peer_id}...")
                    continue  # ç»§ç»­è¿è¡Œï¼Œè€Œä¸æ˜¯break
                
            except Exception as e:
                connection_retries += 1
                error_msg = str(e)
                
                if "UNAVAILABLE" in error_msg or "DEADLINE_EXCEEDED" in error_msg:
                    # ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œå¯ä»¥é‡è¯•
                    logger.warning(f"[StreamChannel] ğŸ“¤ Upload pipeline connection error for peer {self.peer_id} (attempt {connection_retries}): {e}")
                    if connection_retries < max_retries:
                        retry_delay = min(2.0 ** connection_retries, 10.0)  # æŒ‡æ•°é€€é¿
                        logger.info(f"[StreamChannel] ğŸ“¤ Retrying upload pipeline in {retry_delay}s...")
                        time.sleep(retry_delay)
                        continue
                else:
                    # å…¶ä»–ä¸¥é‡é”™è¯¯ï¼ŒçŸ­æš‚ç­‰å¾…åé‡è¯•
                    logger.error(f"[StreamChannel] ğŸ“¤ Upload pipeline error for peer {self.peer_id}: {e}")
                    if connection_retries >= max_retries:
                        logger.warning(f"[StreamChannel] ğŸ“¤ Too many errors, waiting 60s before restart for peer {self.peer_id}")
                        time.sleep(60.0)
                        connection_retries = 0  # é‡ç½®è®¡æ•°å™¨
                    else:
                        time.sleep(5.0)  # çŸ­æš‚ç­‰å¾…åç»§ç»­
                    continue
        
        # ğŸ”§ è¿™ä¸ªåˆ†æ”¯æ°¸è¿œä¸ä¼šåˆ°è¾¾ï¼Œå› ä¸ºwhileå¾ªç¯åªåœ¨is_active=Falseæ—¶é€€å‡º
        logger.info(f"[StreamChannel] ğŸ“¤ Upload pipeline processor ended for peer {self.peer_id} (is_active={self.is_active})")
        
        # ğŸš€ æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š
        if total_chunks_uploaded > 0:
            avg_upload_time = total_upload_time / max(total_chunks_uploaded, 1)
            logger.info(f"[StreamChannel] ğŸ“Š Upload pipeline final stats for peer {self.peer_id}:")
            logger.info(f"[StreamChannel] ğŸ“Š   {total_chunks_uploaded} chunks uploaded in {total_upload_time:.3f}s")
            logger.info(f"[StreamChannel] ğŸ“Š   Average: {avg_upload_time:.3f}s per chunk")
        
        logger.info(f"[StreamChannel] ğŸ“¤ Upload pipeline processor ended for peer {self.peer_id}")
        
    def _enhanced_upload_generator(self):
        """ğŸš€ ä¼˜åŒ–2ï¼šå¢å¼ºçš„ä¸Šä¼ æµç”Ÿæˆå™¨ - æ°¸ä¸åœæ­¢çš„åœ°ä¸‹ç®¡é“"""
        logger.debug(f"[StreamChannel] ğŸ“¤ Upload pipeline generator started for peer {self.peer_id}")
        
        # ğŸš€ åœ°ä¸‹ç®¡é“ä¼˜åŒ–å‚æ•°
        empty_queue_wait = 0.01        # 10msç­‰å¾…æ—¶é—´
        heartbeat_interval = 60.0      # 60ç§’å¿ƒè·³é—´éš”
        last_heartbeat = time.time()
        processed_chunks = 0
        consecutive_empty_checks = 0
        max_empty_checks = 100         # æœ€å¤§ç©ºæ£€æŸ¥æ¬¡æ•° (1ç§’)
        
        while self.is_active:
            try:
                # ğŸš€ é«˜æ•ˆé˜Ÿåˆ—å¤„ç†
                try:
                    priority_item = self.upload_request_queue.get(timeout=empty_queue_wait)
                    consecutive_empty_checks = 0  # é‡ç½®è®¡æ•°å™¨
                    # ğŸ¯ ä»ä¼˜å…ˆé˜Ÿåˆ—ä¸­è§£åŒ…ï¼š(priority, request)
                    priority, request = priority_item
                    
                except queue.Empty:
                    consecutive_empty_checks += 1
                    
                    # ğŸš€ ä¼˜åŒ–ï¼šè‡ªé€‚åº”ç­‰å¾…ç­–ç•¥
                    if consecutive_empty_checks >= max_empty_checks:
                        # é˜Ÿåˆ—é•¿æ—¶é—´ä¸ºç©ºï¼Œå‘é€å¿ƒè·³ç»´æŒè¿æ¥
                        current_time = time.time()
                        if current_time - last_heartbeat > heartbeat_interval:
                            heartbeat_msg = self._create_upload_heartbeat()
                            if heartbeat_msg:
                                yield heartbeat_msg
                                last_heartbeat = current_time
                                logger.debug(f"[StreamChannel] ğŸ“¤ğŸ’“ Upload heartbeat sent to peer {self.peer_id}")
                        consecutive_empty_checks = 0  # é‡ç½®è®¡æ•°å™¨
                    continue
                
                if request is None:  # Sentinel for shutdown
                    logger.info(f"[StreamChannel] ğŸ“¤ Upload generator shutdown for peer {self.peer_id}")
                    break
                
                # ğŸš€ å¤„ç†chunkæ•°æ®ä¼ è¾“
                logger.debug(f"[StreamChannel] ğŸ“¤ Yielding chunk {request.source_client_id}:{request.chunk_id} to peer {self.peer_id}")
                
                yield request
                processed_chunks += 1
                self.upload_chunk_count += 1
                self.last_activity_time = time.time()
                
                self.upload_request_queue.task_done()
                
                # ğŸš€ æ€§èƒ½ç›‘æ§
                if processed_chunks % 100 == 0:
                    logger.debug(f"[StreamChannel] ğŸ“¤ Upload generator processed {processed_chunks} chunks for peer {self.peer_id}")
                
            except Exception as e:
                logger.error(f"[StreamChannel] ğŸ“¤ Upload generator error for peer {self.peer_id}: {e}")
                if not self.is_active:
                    break
                time.sleep(0.1)  # é¿å…é”™è¯¯å¾ªç¯
                
        logger.info(f"[StreamChannel] ğŸ“¤ Upload generator ended for peer {self.peer_id}, processed {processed_chunks} chunks")
        
    def _create_upload_heartbeat(self):
        """åˆ›å»ºä¸Šä¼ å¿ƒè·³æ¶ˆæ¯"""
        try:
            return gRPC_comm_manager_pb2.ChunkStreamRequest(
                sender_id=self.client_id,
                receiver_id=self.peer_id,
                round_num=0,
                chunk_type=gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,  # è½»é‡çº§å¿ƒè·³
                timestamp=int(time.time() * 1000),
                chunk_data=b'heartbeat'  # æœ€å°åŒ–æ•°æ®
            )
        except Exception:
            return None
                
    def _enhanced_control_response_handler(self):
        """ğŸš€ ä¼˜åŒ–1ï¼šå¢å¼ºçš„æ§åˆ¶æ¶ˆæ¯å“åº”å¤„ç†å™¨ - é«˜æ€§èƒ½å“åº”å¤„ç†"""
        logger.info(f"[StreamChannel] ğŸ›ï¸ Control pipeline response handler started for peer {self.peer_id}")
        
        processed_responses = 0
        error_count = 0
        last_stats_report = time.time()
        
        try:
            for response in self.control_stream:
                if not self.is_active:
                    logger.info(f"[StreamChannel] ğŸ›ï¸ Control response handler stopping for peer {self.peer_id}")
                    break
                
                # ğŸš€ ä¼˜åŒ–ï¼šå¿«é€Ÿå“åº”å¤„ç†
                start_time = time.time()
                try:
                    self._handle_control_response(response)
                    processed_responses += 1
                    self.last_activity_time = time.time()
                    
                    # ğŸš€ æ€§èƒ½ç›‘æ§
                    processing_time = time.time() - start_time
                    if processing_time > 0.1:  # è¶…è¿‡100msçš„æ…¢å“åº”
                        logger.warning(f"[StreamChannel] ğŸŒ Slow control response processing: {processing_time:.3f}s for peer {self.peer_id}")
                    
                except Exception as e:
                    error_count += 1
                    logger.error(f"[StreamChannel] ğŸ›ï¸ Control response processing error for peer {self.peer_id}: {e}")
                    if error_count > 10:  # é”™è¯¯è¿‡å¤šæ—¶æ–­å¼€è¿æ¥
                        logger.error(f"[StreamChannel] ğŸ›ï¸ Too many errors ({error_count}), stopping control pipeline for peer {self.peer_id}")
                        break
                
                # ğŸš€ å®šæœŸæ€§èƒ½æŠ¥å‘Š
                current_time = time.time()
                if current_time - last_stats_report > 60.0:  # æ¯åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡
                    logger.info(f"[StreamChannel] ğŸ“Š Control pipeline stats for peer {self.peer_id}: {processed_responses} responses, {error_count} errors")
                    last_stats_report = current_time
                    
        except Exception as e:
            logger.error(f"[StreamChannel] ğŸ›ï¸ Control response handler fatal error for peer {self.peer_id}: {e}")
        finally:
            logger.info(f"[StreamChannel] ğŸ›ï¸ Control response handler ended for peer {self.peer_id}: {processed_responses} responses processed, {error_count} errors")
            
    def _enhanced_download_batch_processor(self):
        """ğŸš€ ä¼˜åŒ–3ï¼šå¢å¼ºçš„ä¸‹è½½æ‰¹å¤„ç†å™¨ - æ™ºèƒ½æ‰¹é‡å¤„ç†chunkè¯·æ±‚"""
        logger.info(f"[StreamChannel] ğŸ“¥ Download pipeline processor started for peer {self.peer_id}")
        
        # ğŸš€ æ™ºèƒ½æ‰¹å¤„ç†å‚æ•°
        batch_requests = []
        adaptive_batch_size = 5   # è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
        max_batch_size = 20       # æœ€å¤§æ‰¹å¤„ç†å¤§å°
        min_batch_timeout = 0.05  # æœ€å°æ‰¹å¤„ç†è¶…æ—¶ (50ms)
        max_batch_timeout = 0.2   # æœ€å¤§æ‰¹å¤„ç†è¶…æ—¶ (200ms)
        current_batch_timeout = min_batch_timeout
        
        # ğŸš€ æ€§èƒ½ç›‘æ§
        processed_batches = 0
        total_chunks_downloaded = 0
        total_download_time = 0.0
        last_performance_check = time.time()
        
        while self.is_active:
            batch_start_time = time.time()
            
            try:
                # ğŸš€ ä¼˜åŒ–ï¼šè‡ªé€‚åº”æ‰¹é‡æ”¶é›†
                end_time = time.time() + current_batch_timeout
                while time.time() < end_time and len(batch_requests) < adaptive_batch_size:
                    try:
                        priority_item = self.download_request_queue.get(timeout=0.01)
                        if priority_item is None:  # å…¼å®¹æ—§æ ¼å¼çš„å…³é—­ä¿¡å·
                            logger.info(f"[StreamChannel] ğŸ“¥ Download pipeline shutdown signal for peer {self.peer_id}")
                            break
                        # ğŸ¯ ä»ä¼˜å…ˆé˜Ÿåˆ—ä¸­è§£åŒ…ï¼š(priority, request)
                        priority, request = priority_item
                        if request is None:  # æ–°æ ¼å¼çš„å…³é—­ä¿¡å·
                            logger.info(f"[StreamChannel] ğŸ“¥ Download pipeline shutdown signal for peer {self.peer_id}")
                            break
                        batch_requests.append(request)
                    except queue.Empty:
                        break
                
                # ğŸš€ ä¼˜åŒ–ï¼šæ™ºèƒ½æ‰¹é‡å‘é€
                if batch_requests:
                    batch_size = len(batch_requests)
                    logger.debug(f"[StreamChannel] ğŸ“¥ Sending batch of {batch_size} download requests to peer {self.peer_id}")
                    
                    batch_request = gRPC_comm_manager_pb2.ChunkBatchRequest(
                        client_id=self.peer_id,
                        sender_id=self.client_id,
                        round_num=batch_requests[0].round_num,
                        chunk_requests=[
                            gRPC_comm_manager_pb2.ChunkRequest(
                                source_client_id=req.source_client_id,
                                chunk_id=req.chunk_id,
                                importance_score=req.importance_score
                            ) for req in batch_requests
                        ]
                    )
                    
                    # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¹¶è¡Œå¤„ç†ä¸‹è½½å“åº”
                    download_start = time.time()
                    try:
                        download_stream = self.stub.downloadChunks(batch_request)
                        chunks_received = 0
                        
                        for chunk_response in download_stream:
                            self._handle_download_response(chunk_response)
                            chunks_received += 1
                            self.download_request_count += 1
                        
                        download_time = time.time() - download_start
                        total_download_time += download_time
                        total_chunks_downloaded += chunks_received
                        processed_batches += 1
                        
                        # ğŸš€ è‡ªé€‚åº”ä¼˜åŒ–ï¼šæ ¹æ®æ€§èƒ½è°ƒæ•´æ‰¹å¤„ç†å‚æ•°
                        avg_time_per_chunk = download_time / max(chunks_received, 1)
                        if avg_time_per_chunk < 0.01:  # å¾ˆå¿«ï¼Œå¢åŠ æ‰¹å¤§å°
                            adaptive_batch_size = min(adaptive_batch_size + 2, max_batch_size)
                            current_batch_timeout = min(current_batch_timeout * 1.1, max_batch_timeout)
                        elif avg_time_per_chunk > 0.05:  # è¾ƒæ…¢ï¼Œå‡å°æ‰¹å¤§å°
                            adaptive_batch_size = max(adaptive_batch_size - 1, 3)
                            current_batch_timeout = max(current_batch_timeout * 0.9, min_batch_timeout)
                        
                        logger.debug(f"[StreamChannel] ğŸ“¥ Batch completed: {chunks_received} chunks in {download_time:.3f}s, adaptive_size={adaptive_batch_size}")
                        
                    except Exception as e:
                        logger.error(f"[StreamChannel] ğŸ“¥ Download stream error for peer {self.peer_id}: {e}")
                        # é”™è¯¯æ—¶é‡ç½®æ‰¹å¤„ç†å‚æ•°
                        adaptive_batch_size = 5
                        current_batch_timeout = min_batch_timeout
                    
                    batch_requests.clear()
                    
                # ğŸš€ å®šæœŸæ€§èƒ½æŠ¥å‘Šå’Œä¼˜åŒ–
                current_time = time.time()
                if current_time - last_performance_check > 30.0:  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                    if processed_batches > 0:
                        avg_batch_time = total_download_time / processed_batches
                        avg_chunks_per_batch = total_chunks_downloaded / processed_batches
                        logger.info(f"[StreamChannel] ğŸ“Š Download pipeline performance for peer {self.peer_id}:")
                        logger.info(f"[StreamChannel] ğŸ“Š   {processed_batches} batches, {total_chunks_downloaded} chunks")
                        logger.info(f"[StreamChannel] ğŸ“Š   Avg: {avg_chunks_per_batch:.1f} chunks/batch, {avg_batch_time:.3f}s/batch")
                        logger.info(f"[StreamChannel] ğŸ“Š   Current adaptive_size: {adaptive_batch_size}, timeout: {current_batch_timeout:.3f}s")
                    last_performance_check = current_time
                    
            except Exception as e:
                logger.error(f"[StreamChannel] ğŸ“¥ Download batch processor error for peer {self.peer_id}: {e}")
                batch_requests.clear()
                time.sleep(0.1)  # é¿å…é”™è¯¯å¾ªç¯
            
        logger.info(f"[StreamChannel] ğŸ“¥ Download pipeline processor ended for peer {self.peer_id}")
        logger.info(f"[StreamChannel] ğŸ“Š Final stats: {processed_batches} batches, {total_chunks_downloaded} total chunks downloaded")
            
    def _handle_control_response(self, response):
        """ğŸ”§ ä¿®å¤ï¼šå¤„ç†æ§åˆ¶æ¶ˆæ¯å“åº” - æ”¯æŒæ‰€æœ‰å“åº”ç±»å‹"""
        logger.info(f"[StreamChannel] Received control response from peer {self.peer_id}: type={response.response_type}")
        
        if not self.client_instance:
            logger.warning(f"[StreamChannel] No client instance for peer {self.peer_id}, cannot process control response")
            return
            
        try:
            from federatedscope.core.message import Message
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æ‰€æœ‰ChunkResponseTypeæšä¸¾å€¼
            if response.response_type == gRPC_comm_manager_pb2.ChunkResponseType.CHUNK_ACK:
                # ACKå“åº”ï¼šç¡®è®¤æ”¶åˆ°æ§åˆ¶æ¶ˆæ¯
                logger.debug(f"[StreamChannel] Received ACK from peer {self.peer_id} for chunk {response.chunk_id}")
                # ACKæ¶ˆæ¯é€šå¸¸ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œåªè®°å½•æ—¥å¿—
                return
                
            elif response.response_type == gRPC_comm_manager_pb2.ChunkResponseType.CHUNK_NACK:
                # NACKå“åº”ï¼šæ‹’ç»/é”™è¯¯å“åº”
                logger.warning(f"[StreamChannel] Received NACK from peer {self.peer_id} for chunk {response.chunk_id}")
                # TODO: å¯ä»¥åœ¨è¿™é‡Œå®ç°é‡ä¼ é€»è¾‘
                return
                
            elif response.response_type == gRPC_comm_manager_pb2.ChunkResponseType.CHUNK_HAVE_RESP:
                # HAVEå“åº”ï¼špeeré€šçŸ¥æ‹¥æœ‰æŸä¸ªchunk
                content = {
                    'round_num': response.round_num,
                    'source_client_id': getattr(response, 'source_client_id', response.sender_id),
                    'chunk_id': response.chunk_id
                }
                msg_type = 'have'
                
            elif response.response_type == gRPC_comm_manager_pb2.ChunkResponseType.CHUNK_INTERESTED:
                # INTERESTEDå“åº”ï¼špeerè¡¨ç¤ºæ„Ÿå…´è¶£
                content = {
                    'round_num': response.round_num,
                    'peer_id': response.sender_id
                }
                msg_type = 'interested'
                
            else:
                # ğŸ”§ ä½¿ç”¨æ•°å€¼è¿›è¡Œå…¼å®¹æ€§æ£€æŸ¥
                response_type_value = int(response.response_type)
                logger.warning(f"[StreamChannel] Unknown control response type: {response_type_value} from peer {self.peer_id}")
                
                # ğŸ”§ å°è¯•æ¨æ–­å“åº”ç±»å‹
                if response_type_value == 0:  # CHUNK_ACK
                    logger.debug(f"[StreamChannel] Treating response type 0 as ACK from peer {self.peer_id}")
                    return
                elif response_type_value == 1:  # CHUNK_NACK  
                    logger.warning(f"[StreamChannel] Treating response type 1 as NACK from peer {self.peer_id}")
                    return
                elif response_type_value == 2:  # CHUNK_HAVE_RESP
                    content = {
                        'round_num': response.round_num,
                        'source_client_id': getattr(response, 'source_client_id', response.sender_id),
                        'chunk_id': response.chunk_id
                    }
                    msg_type = 'have'
                elif response_type_value == 3:  # CHUNK_INTERESTED
                    content = {
                        'round_num': response.round_num,
                        'peer_id': response.sender_id
                    }
                    msg_type = 'interested'
                else:
                    logger.error(f"[StreamChannel] Unsupported response type: {response_type_value} from peer {self.peer_id}")
                    return
                
            # åˆ›å»ºMessageå¯¹è±¡å¹¶è°ƒç”¨callback
            if 'msg_type' in locals() and 'content' in locals():
                message = Message(
                    msg_type=msg_type,
                    sender=self.peer_id,
                    receiver=[self.client_id],
                    content=content
                )
                
                # ğŸ”§ è°ƒç”¨æ­£ç¡®çš„callbackå‡½æ•°
                if msg_type == 'have':
                    logger.debug(f"[StreamChannel] âœ… Calling callback_funcs_for_have for peer {self.peer_id}")
                    self.client_instance.callback_funcs_for_have(message)
                elif msg_type == 'interested':
                    logger.debug(f"[StreamChannel] âœ… Calling callback_funcs_for_interested for peer {self.peer_id}")
                    if hasattr(self.client_instance, 'callback_funcs_for_interested'):
                        self.client_instance.callback_funcs_for_interested(message)
                    else:
                        logger.debug(f"[StreamChannel] No callback for interested messages")
                elif msg_type == 'bitfield':
                    logger.debug(f"[StreamChannel] âœ… Calling callback_funcs_for_bitfield for peer {self.peer_id}")
                    if hasattr(self.client_instance, 'callback_funcs_for_bitfield'):
                        self.client_instance.callback_funcs_for_bitfield(message)
                    else:
                        logger.debug(f"[StreamChannel] No callback for bitfield messages")
                
        except Exception as e:
            logger.error(f"[StreamChannel] Error handling control response from peer {self.peer_id}: {e}")
            import traceback
            logger.error(f"[StreamChannel] Traceback: {traceback.format_exc()}")
        
    def _handle_download_response(self, response):
        """ğŸ”§ ä¼˜åŒ–ï¼šå¤„ç†chunkä¸‹è½½å“åº” - å»é‡ç»Ÿè®¡ï¼Œé˜²é‡å¤å¤„ç†"""        
        if not self.client_instance:
            logger.warning(f"[StreamChannel] No client instance for peer {self.peer_id}, cannot process chunk download")
            return
            
        try:
            # éªŒè¯å“åº”æ•°æ®
            if not response.response_data:
                logger.error(f"[StreamChannel] ğŸš« Empty chunk data from peer {self.peer_id}, chunk_id={response.chunk_id}")
                return
            
            # ğŸ”§ ç»Ÿä¸€ç»Ÿè®¡ç®¡ç† (åªç»Ÿè®¡ä¸€æ¬¡)
            chunk_size = len(response.response_data)
            self.chunks_received += 1
            self.bytes_received += chunk_size
            
            # ğŸ”§ æ›´æ–°å»é‡çŠ¶æ€
            source_client_id = getattr(response, 'source_client_id', response.sender_id)
            self.mark_chunk_completed(response.round_num, source_client_id, response.chunk_id)
            
            # åˆ›å»ºChunkDataå¯¹è±¡
            from federatedscope.core.message import Message, ChunkData
            import hashlib
                
            # è®¡ç®—æ ¡éªŒå’Œ
            checksum = hashlib.sha256(response.response_data).hexdigest()
            
            # åˆ›å»ºChunkDataå¯¹è±¡åŒ…è£…åŸå§‹æ•°æ®
            chunk_data = ChunkData(response.response_data, checksum)
            
            # æ„é€ æ¶ˆæ¯å†…å®¹ (ä¸ä¼ ç»ŸBitTorrent PIECEæ¶ˆæ¯æ ¼å¼å…¼å®¹)
            content = {
                'round_num': response.round_num,
                'source_client_id': source_client_id,  # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µ
                'chunk_id': response.chunk_id,
                'data': chunk_data,
                'checksum': checksum
            }
            
            # åˆ›å»ºMessageå¯¹è±¡
            message = Message(
                msg_type='piece',
                sender=self.peer_id,
                receiver=[self.client_id],
                content=content
            )
            
            # è°ƒç”¨callbackå‡½æ•°å¤„ç†chunk piece
            logger.debug(f"[StreamChannel] Calling callback_funcs_for_piece for peer {self.peer_id}, chunk {response.sender_id}:{response.chunk_id}")
            self.client_instance.callback_funcs_for_piece(message)
            
        except Exception as e:
            logger.error(f"[StreamChannel] Error handling chunk download from peer {self.peer_id}: {e}")
        
    def _handle_chunk_response(self, response):
        """å…¼å®¹æ€§æ–¹æ³•ï¼šå¤„ç†æ”¶åˆ°çš„chunkå“åº”"""
        self.chunks_received += 1
        self.bytes_received += len(response.response_data) if response.response_data else 0
        logger.info(f"[StreamChannel] Received chunk response from peer {self.peer_id}")
        
    def _calculate_request_priority(self, importance_score: float, rarity_score: int) -> tuple:
        """ğŸ¯ è®¡ç®—è¯·æ±‚ä¼˜å…ˆçº§ï¼šimportanceä¼˜å…ˆï¼Œç›¸è¿‘importanceæ—¶æŒ‰rareæ’åº"""
        import random
        importance_jitter = random.uniform(-0.01, 0.01)
        rarity_jitter = random.uniform(-0.1, 0.1)
        # ä¸»ä¼˜å…ˆçº§ï¼šimportance (å–è´Ÿå€¼å› ä¸ºPriorityQueueæ˜¯æœ€å°å †ï¼Œæˆ‘ä»¬è¦é«˜importanceä¼˜å…ˆ)
        primary_priority = -importance_score + importance_jitter * rarity_score
        # primary_priority = -random.random()
        # æ¬¡ä¼˜å…ˆçº§ï¼šrarity (æ•°å€¼è¶Šå°è¶Šç¨€æœ‰ï¼Œä¼˜å…ˆçº§è¶Šé«˜)
        secondary_priority = rarity_score + rarity_jitter
        
        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šæ—¶é—´æˆ³ (ç¡®ä¿åŒä¼˜å…ˆçº§ä¸‹çš„FIFO)
        tertiary_priority = random.random()
        
        return (primary_priority, secondary_priority, tertiary_priority)
        
    def send_chunk_request(self, round_num: int, source_client_id: int, chunk_id: int, 
                          importance_score: float = 0.0, rarity_score: int = 0):
        """ğŸ”§ å¢å¼ºå»é‡ï¼šå‘é€chunkè¯·æ±‚ - ä¸¥æ ¼é˜²æ­¢é‡å¤è¯·æ±‚"""
        if not self.is_active:
            logger.warning(f"[StreamChannel] Cannot send to inactive channel for peer {self.peer_id}")
            return False
        
        # ğŸ”§ å…³é”®å»é‡æœºåˆ¶ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»è¯·æ±‚è¿‡è¿™ä¸ªchunk
        chunk_key = (round_num, source_client_id, chunk_id)
        
        # åˆå§‹åŒ–å»é‡é›†åˆ
        if not hasattr(self, 'requested_chunks'):
            self.requested_chunks = set()
        if not hasattr(self, 'completed_chunks'):
            self.completed_chunks = set()
        
        # ğŸ”§ ä¸¥æ ¼å»é‡æ£€æŸ¥
        if chunk_key in self.requested_chunks:
            logger.debug(f"[StreamChannel] ğŸš« DUPLICATE REQUEST blocked: chunk {source_client_id}:{chunk_id} already requested from peer {self.peer_id}")
            return False
        
        if chunk_key in self.completed_chunks:
            logger.debug(f"[StreamChannel] ğŸš« REDUNDANT REQUEST blocked: chunk {source_client_id}:{chunk_id} already completed from peer {self.peer_id}")
            return False
        
        # ğŸ”§ é˜Ÿåˆ—å¤§å°é™åˆ¶ (é¿å…å†…å­˜æ³„æ¼)
        if self.download_request_queue.qsize() >= self.download_request_queue.maxsize * 0.8:
            logger.warning(f"[StreamChannel] ğŸš« REQUEST QUEUE near full ({self.download_request_queue.qsize()}/{self.download_request_queue.maxsize}), rejecting new request")
            return False
            
        request = gRPC_comm_manager_pb2.ChunkStreamRequest(
            sender_id=self.client_id,
            receiver_id=self.peer_id,
            round_num=round_num,
            source_client_id=source_client_id,
            chunk_id=chunk_id,
            chunk_type=gRPC_comm_manager_pb2.ChunkType.CHUNK_REQUEST,
            importance_score=importance_score,
            timestamp=int(time.time() * 1000)
        )
        
        try:
            # ğŸ¯ è®¡ç®—ä¼˜å…ˆçº§ï¼šCHUNK_REQUESTæŒ‰importance+rareæ’åº
            priority = self._calculate_request_priority(importance_score, rarity_score)
            
            # ğŸš€ ä¸¥æ ¼çš„è¯·æ±‚é˜Ÿåˆ—ç®¡ç† - ä½¿ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—
            self.download_request_queue.put((priority, request), timeout=0.5)
            
            # ğŸ”§ è®°å½•è¯·æ±‚ï¼Œé˜²æ­¢é‡å¤
            self.requested_chunks.add(chunk_key)
            
            logger.debug(f"[StreamChannel] âœ… Priority request queued: chunk {source_client_id}:{chunk_id} (importance:{importance_score:.3f}, rarity:{rarity_score}) to peer {self.peer_id}")
            logger.debug(f"[StreamChannel] ğŸ“Š Request stats: {len(self.requested_chunks)} requested, {len(self.completed_chunks)} completed")
            
            return True
        except queue.Full:
            logger.error(f"[StreamChannel] ğŸš« Download request queue full for peer {self.peer_id}")
            return False
        except Exception as e:
            logger.error(f"[StreamChannel] ğŸš« Failed to queue request: {e}")
            return False
            
    def mark_chunk_completed(self, round_num: int, source_client_id: int, chunk_id: int):
        """ğŸ”§ æ ‡è®°chunkå·²å®Œæˆï¼Œæ¸…ç†å»é‡çŠ¶æ€"""
        chunk_key = (round_num, source_client_id, chunk_id)
        
        if hasattr(self, 'requested_chunks'):
            self.requested_chunks.discard(chunk_key)  # ä»è¯·æ±‚é›†åˆä¸­ç§»é™¤
        if hasattr(self, 'completed_chunks'):
            self.completed_chunks.add(chunk_key)      # åŠ å…¥å®Œæˆé›†åˆ
            
        logger.debug(f"[StreamChannel] âœ… Marked chunk {source_client_id}:{chunk_id} as completed for peer {self.peer_id}")
            
    def send_chunk_data(self, round_num: int, source_client_id: int, chunk_id: int,
                       chunk_data: bytes, checksum: str, importance_score: float = 0.0, rarity_score: int = 0):
        """ğŸš€ å‘é€chunkæ•°æ® - ä½¿ç”¨ä¸“ç”¨ä¸Šä¼ æµ"""
        if not self.is_active:
            logger.warning(f"[StreamChannel] Cannot send to inactive channel for peer {self.peer_id}")
            return False
            
        # ğŸ”§ CRITICAL FIX: æ­£ç¡®è®¾ç½®sender_idå’Œreceiver_id
        # sender_idåº”è¯¥æ˜¯å‘é€æ–¹çš„client_idï¼Œreceiver_idåº”è¯¥æ˜¯æ¥æ”¶æ–¹çš„peer_id
        request = gRPC_comm_manager_pb2.ChunkStreamRequest(
            sender_id=self.client_id,  # å‘é€æ–¹client_id
            receiver_id=self.peer_id,   # æ¥æ”¶æ–¹peer_id
            round_num=round_num,
            source_client_id=source_client_id,
            chunk_id=chunk_id,
            chunk_data=chunk_data,
            checksum=checksum,
            chunk_type=gRPC_comm_manager_pb2.ChunkType.CHUNK_PIECE,
            importance_score=importance_score,
            timestamp=int(time.time() * 1000)
        )
        
        try:
            # ğŸ¯ è®¡ç®—ä¸Šä¼ ä¼˜å…ˆçº§ï¼šCHUNK_PIECEæŒ‰importance+rareæ’åº
            priority = self._calculate_request_priority(importance_score, rarity_score)
            
            # ğŸš€ ä½¿ç”¨ä¸“ç”¨ä¸Šä¼ é˜Ÿåˆ—ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
            logger.info(f"[ğŸš€ StreamChannel] Client {self.client_id}: Queueing priority chunk data {source_client_id}:{chunk_id} (importance:{importance_score:.3f}, rarity:{rarity_score}) to peer {self.peer_id}")
            logger.info(f"[ğŸš€ StreamChannel] Client {self.client_id}: Chunk data size: {len(chunk_data)}, queue size before: {self.upload_request_queue.qsize()}")
            
            self.upload_request_queue.put((priority, request), timeout=1.0)
            self.chunks_sent += 1
            self.bytes_sent += len(chunk_data)
            
            logger.info(f"[ğŸš€ StreamChannel] Client {self.client_id}: Successfully queued chunk data {source_client_id}:{chunk_id} to peer {self.peer_id}")
            logger.info(f"[ğŸš€ StreamChannel] Client {self.client_id}: Queue size after: {self.upload_request_queue.qsize()}")
            return True
        except queue.Full:
            logger.error(f"[StreamChannel] Upload queue full for peer {self.peer_id}")
            return False
    
    def send_control_message(self, msg_type: str, round_num: int, **kwargs) -> bool:
        """ğŸš€ ä¼˜åŒ–3ï¼šå‘é€BitTorrentæ§åˆ¶æ¶ˆæ¯ - ä½¿ç”¨ä¸“ç”¨æ§åˆ¶æµé˜Ÿåˆ—"""
        if not self.is_active:
            logger.warning(f"[StreamChannel] Cannot send to inactive channel for peer {self.peer_id}")
            return False
        
        # ğŸš€ æ™ºèƒ½æ¶ˆæ¯ç±»å‹æ˜ å°„
        chunk_type_map = {
            'bitfield': gRPC_comm_manager_pb2.ChunkType.CHUNK_BITFIELD,
            'have': gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,
            'cancel': gRPC_comm_manager_pb2.ChunkType.CHUNK_CANCEL,
            'request': gRPC_comm_manager_pb2.ChunkType.CHUNK_REQUEST,
            'piece': gRPC_comm_manager_pb2.ChunkType.CHUNK_PIECE,
            # BitTorrentæ§åˆ¶æ¶ˆæ¯æ‰©å±•
            'interested': gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,
            'unchoke': gRPC_comm_manager_pb2.ChunkType.CHUNK_HAVE,
            'choke': gRPC_comm_manager_pb2.ChunkType.CHUNK_CANCEL
        }
        
        if msg_type not in chunk_type_map:
            logger.error(f"[StreamChannel] ğŸš« Unsupported control message type: {msg_type}")
            return False
        
        # ğŸš€ é«˜æ•ˆæ¶ˆæ¯æ„å»º
        request = gRPC_comm_manager_pb2.ChunkStreamRequest(
            sender_id=self.client_id,
            receiver_id=self.peer_id,
            round_num=round_num,
            chunk_type=chunk_type_map[msg_type],
            timestamp=int(time.time() * 1000)
        )
        
        # ğŸš€ æ ¹æ®æ¶ˆæ¯ç±»å‹è®¾ç½®ç‰¹å®šå­—æ®µ
        if msg_type == 'bitfield':
            # bitfieldæ¶ˆæ¯ï¼šä½¿ç”¨chunk_dataä¼ è¾“bitfieldæ•°æ®
            bitfield_data = kwargs.get('bitfield', [])
            request.chunk_data = json.dumps(bitfield_data).encode('utf-8')
            logger.debug(f"[StreamChannel] ğŸ›ï¸ Client {self.client_id}: Sending BITFIELD to peer {self.peer_id}, chunks: {len(bitfield_data)}")
            
        elif msg_type == 'have':
            # haveæ¶ˆæ¯ï¼šæ ‡è¯†æ‹¥æœ‰çš„chunk
            request.source_client_id = kwargs.get('source_client_id', 0)
            request.chunk_id = kwargs.get('chunk_id', 0)
            request.importance_score = kwargs.get('importance_score', 0.0)
            logger.debug(f"[StreamChannel] ğŸ›ï¸ Client {self.client_id}: Sending HAVE to peer {self.peer_id}, chunk: {request.source_client_id}:{request.chunk_id}")
            
        elif msg_type == 'cancel':
            # cancelæ¶ˆæ¯ï¼šå–æ¶ˆchunkè¯·æ±‚
            request.source_client_id = kwargs.get('source_client_id', 0)
            request.chunk_id = kwargs.get('chunk_id', 0)
            logger.debug(f"[StreamChannel] ğŸ›ï¸ Client {self.client_id}: Sending CANCEL to peer {self.peer_id}, chunk: {request.source_client_id}:{request.chunk_id}")
            
        elif msg_type == 'interested':
            # interestedæ¶ˆæ¯ï¼šè¡¨ç¤ºå¯¹peeræ„Ÿå…´è¶£
            request.source_client_id = self.client_id
            request.chunk_id = 0
            logger.debug(f"[StreamChannel] ğŸ›ï¸ Client {self.client_id}: Sending INTERESTED to peer {self.peer_id}")
            
        elif msg_type == 'unchoke':
            # unchokeæ¶ˆæ¯ï¼šå…è®¸peerå‘é€è¯·æ±‚
            request.source_client_id = self.client_id
            request.chunk_id = 0
            logger.debug(f"[StreamChannel] ğŸ›ï¸ Client {self.client_id}: Sending UNCHOKE to peer {self.peer_id}")
            
        elif msg_type == 'choke':
            # chokeæ¶ˆæ¯ï¼šç¦æ­¢peerå‘é€è¯·æ±‚
            request.source_client_id = self.client_id  
            request.chunk_id = 0
            logger.debug(f"[StreamChannel] ğŸ›ï¸ Client {self.client_id}: Sending CHOKE to peer {self.peer_id}")
        
        try:
            # ğŸš€ ä½¿ç”¨ä¸“ç”¨æ§åˆ¶æ¶ˆæ¯é˜Ÿåˆ—ï¼Œæé«˜åˆ†å‘æ•ˆç‡
            self.control_request_queue.put(request, timeout=0.5)  # æ›´çŸ­è¶…æ—¶æ—¶é—´
            self.control_msg_count += 1
            logger.debug(f"[StreamChannel] âœ… Client {self.client_id}: Queued {msg_type.upper()} message to control pipeline for peer {self.peer_id}")
            return True
            
        except queue.Full:
            logger.warning(f"[StreamChannel] ğŸš« Control pipeline queue full for peer {self.peer_id}, dropping {msg_type} message")
            return False
        except Exception as e:
            logger.error(f"[StreamChannel] ğŸš« Failed to queue {msg_type} message for peer {self.peer_id}: {e}")
            return False
            
    def close(self):
        """å…³é—­streamingé€šé“"""
        if not self.is_active:
            return
            
        self.is_active = False
        
        # å‘é€å…³é—­ä¿¡å·åˆ°å„ä¸ªé˜Ÿåˆ—
        try:
            self.request_queue.put(None)
            # ğŸ¯ ä¼˜å…ˆé˜Ÿåˆ—éœ€è¦ç‰¹æ®Šå¤„ç†å…³é—­ä¿¡å·ï¼šä½¿ç”¨æœ€é«˜ä¼˜å…ˆçº§
            shutdown_priority = (-999999, 0, 0)  # æœ€é«˜ä¼˜å…ˆçº§çš„å…³é—­ä¿¡å·
            self.upload_request_queue.put((shutdown_priority, None))
            self.download_request_queue.put((shutdown_priority, None))
        except:
            pass
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        threads = [
            self.control_receiver_thread,
            self.download_sender_thread,
            self.upload_sender_thread,
        ]
        
        for thread in threads:
            if thread and thread.is_alive():
                thread.join(timeout=1.0)
            
        # å…³é—­gRPCé€šé“
        try:
            self.channel.close()
            logger.info(f"[StreamChannel] Closed streaming to peer {self.peer_id}")
        except:
            pass
            
    def get_stats(self):
        """è·å–é€šé“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'peer_id': self.peer_id,
            'is_active': self.is_active,
            'bytes_sent': self.bytes_sent,
            'bytes_received': self.bytes_received,
            'chunks_sent': self.chunks_sent,
            'chunks_received': self.chunks_received,
            'last_activity': self.last_activity
        }


class StreamingChannelManager:
    """ğŸš€ gRPC Streamingé€šé“ç®¡ç†å™¨ - åœ¨æ‹“æ‰‘æ„å»ºæ—¶åˆ›å»ºä¸“ç”¨é€šé“"""
    
    def __init__(self, client_id, client_instance=None):
        self.client_id = client_id
        self.client_instance = client_instance
        self.channels: Dict[int, StreamingChannel] = {}  # peer_id -> StreamingChannel
        self.server_stubs: Dict[str, gRPC_comm_manager_pb2_grpc.gRPCComServeFuncStub] = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_bytes_sent = 0
        self.total_bytes_received = 0
        self.total_chunks_sent = 0
        self.total_chunks_received = 0
        
        logger.info(f"[StreamingManager] Initialized for client {client_id}")
        
    def create_channels_for_topology(self, neighbor_addresses: Dict[int, Tuple[str, int]]):
        """
        ğŸš€ åœ¨æ‹“æ‰‘æ„å»ºæ—¶ä¸ºæ‰€æœ‰é‚»å±…åˆ›å»ºä¸“ç”¨streamingé€šé“ - ä¿®å¤æ¶ˆæ¯å¤§å°é™åˆ¶
        Args:
            neighbor_addresses: {peer_id: (host, port)}
        """
        logger.info(f"[StreamingManager] Client {self.client_id}: Creating enhanced streaming channels for topology")
        logger.info(f"[StreamingManager] Neighbor addresses: {neighbor_addresses}")
        
        # ğŸš€ ä¿®å¤: gRPCé€šé“é…ç½® - è®¾ç½®å¤§æ¶ˆæ¯çª—å£
        max_message_size = 512 * 1024 * 1024  # 512MBæ¶ˆæ¯é™åˆ¶
        max_receive_size = 512 * 1024 * 1024  # 512MBæ¥æ”¶é™åˆ¶
        
        grpc_options = [
            ('grpc.max_send_message_length', max_message_size),
            ('grpc.max_receive_message_length', max_receive_size),
            ('grpc.keepalive_time_ms', 30000),          # 30ç§’keepalive
            ('grpc.keepalive_timeout_ms', 10000),       # 10ç§’keepaliveè¶…æ—¶
            ('grpc.keepalive_permit_without_calls', True),
            ('grpc.http2.max_pings_without_data', 0),
            ('grpc.http2.min_time_between_pings_ms', 10000),
            ('grpc.http2.min_ping_interval_without_data_ms', 300000),
            ('grpc.so_reuseaddr', 1),
        ]
        
        logger.info(f"[StreamingManager] ğŸš€ Enhanced gRPC configuration:")
        logger.info(f"[StreamingManager] ğŸ“¦ Max message size: {max_message_size / (1024*1024):.0f}MB")
        logger.info(f"[StreamingManager] ğŸ“¥ Max receive size: {max_receive_size / (1024*1024):.0f}MB")
        
        for peer_id, (host, port) in neighbor_addresses.items():
            if peer_id == self.client_id:
                continue  # è·³è¿‡è‡ªå·±
                
            try:
                # ğŸš€ åˆ›å»ºå¢å¼ºçš„gRPCé€šé“ - æ”¯æŒå¤§æ¶ˆæ¯ä¼ è¾“
                address = f"{host}:{port}"
                channel = grpc.insecure_channel(address, options=grpc_options)
                
                # åˆ›å»ºstub
                stub = gRPC_comm_manager_pb2_grpc.gRPCComServeFuncStub(channel)
                
                # åˆ›å»ºstreamingé€šé“å°è£…
                stream_channel = StreamingChannel(peer_id, channel, stub, client_id=self.client_id, client_instance=self.client_instance)
                
                # å¯åŠ¨streaming (å¦‚æœå¤±è´¥ä¼šè‡ªåŠ¨å›é€€)
                stream_channel.start_streaming()
                
                # æ€»æ˜¯ä¿å­˜é€šé“ï¼Œå³ä½¿streamingå¤±è´¥ä¹Ÿå¯ä»¥ç”¨äºä¼ ç»Ÿæ¶ˆæ¯
                self.channels[peer_id] = stream_channel
                self.server_stubs[address] = stub
                
                if stream_channel.is_active:
                    logger.info(f"[StreamingManager] âœ… Enhanced streaming channel: Client {self.client_id} -> Peer {peer_id} ({address})")
                    logger.info(f"[StreamingManager] ğŸš€ Support for chunks up to {max_message_size / (1024*1024):.0f}MB")
                else:
                    logger.info(f"[StreamingManager] ğŸ”§ Traditional fallback channel: Client {self.client_id} -> Peer {peer_id} ({address})")
                
            except Exception as e:
                logger.error(f"[StreamingManager] Failed to create enhanced channel to peer {peer_id} at {host}:{port}: {e}")
                
        logger.info(f"[StreamingManager] Client {self.client_id}: Created {len(self.channels)} enhanced streaming channels")
        logger.info(f"[StreamingManager] ğŸš€ STREAMING OPTIMIZATION: BitTorrent messages now support high-performance streaming:")
        logger.info(f"[StreamingManager]   ğŸ“¦ CHUNK_PIECE: Large chunk data via dedicated upload streams") 
        logger.info(f"[StreamingManager]   ğŸ“¤ CHUNK_REQUEST: Chunk requests via control streams")
        logger.info(f"[StreamingManager]   ğŸ¯ CHUNK_HAVE: Have notifications via control streams") 
        logger.info(f"[StreamingManager]   ğŸ“‹ CHUNK_BITFIELD: Bitfield updates via control streams")
        logger.info(f"[StreamingManager]   âŒ CHUNK_CANCEL: Cancel requests via control streams")
        logger.info(f"[StreamingManager]   ğŸ”„ Auto fallback to traditional messaging if streaming fails")
        
    def send_chunk_request(self, peer_id: int, round_num: int, source_client_id: int, 
                          chunk_id: int, importance_score: float = 0.0, rarity_score: int = 0) -> bool:
        """é€šè¿‡streamingé€šé“å‘é€chunkè¯·æ±‚"""
        if peer_id not in self.channels:
            logger.warning(f"[StreamingManager] No streaming channel to peer {peer_id} for chunk request")
            return False
            
        return self.channels[peer_id].send_chunk_request(
            round_num, source_client_id, chunk_id, importance_score, rarity_score)
            
    def send_bittorrent_message(self, peer_id: int, msg_type: str, **kwargs) -> bool:
        """ğŸš€ ç»Ÿä¸€çš„BitTorrentæ¶ˆæ¯å‘é€æ¥å£ - æ”¯æŒæ‰€æœ‰æ¶ˆæ¯ç±»å‹"""
        if peer_id not in self.channels:
            logger.info(f"[StreamingManager] ğŸ” Attempting to send {msg_type.upper()} message to peer {peer_id}")
            logger.info(f"[StreamingManager] ğŸ” Client {self.client_id} has channels to peers: {list(self.channels.keys())}")
            logger.warning(f"[StreamingManager] No streaming channel to peer {peer_id}")
            return False
        
        # æ ¹æ®æ¶ˆæ¯ç±»å‹æ„å»ºä¸åŒçš„è¯·æ±‚
        if msg_type == 'piece':
            return self.channels[peer_id].send_chunk_data(**kwargs)
        elif msg_type == 'request':
            return self.channels[peer_id].send_chunk_request(**kwargs)
        elif msg_type in ['bitfield', 'have', 'cancel', 'interested', 'unchoke', 'choke']:
            return self.channels[peer_id].send_control_message(msg_type, **kwargs)
        else:
            logger.error(f"[StreamingManager] Unknown BitTorrent message type: {msg_type}")
            return False
    
    def send_chunk_data(self, peer_id: int, round_num: int, source_client_id: int,
                       chunk_id: int, chunk_data: bytes, checksum: str, 
                       importance_score: float = 0.0, rarity_score: int = 0) -> bool:
        """é€šè¿‡streamingé€šé“å‘é€chunkæ•°æ®"""
        if peer_id not in self.channels:
            logger.warning(f"[StreamingManager] No streaming channel to peer {peer_id} for chunk data")
            return False
            
        success = self.channels[peer_id].send_chunk_data(
            round_num, source_client_id, chunk_id, chunk_data, checksum, importance_score, rarity_score)
            
        if success:
            self.total_chunks_sent += 1
            self.total_bytes_sent += len(chunk_data)
            
        return success
        
    def get_active_peers(self) -> List[int]:
        """è·å–æ‰€æœ‰æ´»è·ƒçš„peeråˆ—è¡¨"""
        return [peer_id for peer_id, channel in self.channels.items() if channel.is_active]
        
    def close_all_channels(self):
        """å…³é—­æ‰€æœ‰streamingé€šé“"""
        logger.info(f"[StreamingManager] Client {self.client_id}: Closing all streaming channels")
        
        for peer_id, channel in self.channels.items():
            channel.close()
            
        self.channels.clear()
        self.server_stubs.clear()
        
        logger.info(f"[StreamingManager] Client {self.client_id}: All streaming channels closed")
        
    def get_channel_stats(self):
        """è·å–æ‰€æœ‰é€šé“çš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'client_id': self.client_id,
            'total_channels': len(self.channels),
            'active_channels': len([c for c in self.channels.values() if c.is_active]),
            'total_bytes_sent': self.total_bytes_sent,
            'total_bytes_received': self.total_bytes_received,
            'total_chunks_sent': self.total_chunks_sent,
            'total_chunks_received': self.total_chunks_received,
            'channels': {peer_id: channel.get_stats() for peer_id, channel in self.channels.items()}
        }
        return stats