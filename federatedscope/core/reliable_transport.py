"""
ğŸ›¡ï¸ Application-Layer Reliable Transport (ALRT) Protocol
åœ¨gRPCä¹‹ä¸Šå®ç°å¯é ä¼ è¾“ï¼Œè§£å†³æ— sudoæƒé™æ—¶çš„é™é»˜ä¸¢åŒ…é—®é¢˜

æ ¸å¿ƒæœºåˆ¶ï¼š
1. æ¶ˆæ¯åˆ†ç‰‡ï¼ˆFragmentationï¼‰- è§£å†³å¤§æ¶ˆæ¯è¶…è¿‡buffer zoneé—®é¢˜
2. æ¶ˆæ¯åºåˆ—å· + ACKç¡®è®¤
3. é€‰æ‹©æ€§é‡ä¼ ï¼ˆåªé‡ä¼ ä¸¢å¤±çš„åˆ†ç‰‡ï¼‰
4. æ»‘åŠ¨çª—å£æµæ§
5. åˆ†ç‰‡é‡ç»„ï¼ˆReassemblyï¼‰

å…³é”®é—®é¢˜ï¼š
  ç³»ç»ŸBuffer: rmem_max = wmem_max â‰ˆ 208KB (æ— sudoæ— æ³•ä¿®æ”¹)
  gRPCé…ç½®: max_message_length = 300MB
  å·®è·1400å€ï¼å¤§chunkè¶…è¿‡socket bufferå¯¼è‡´é™é»˜ä¸¢åŒ…

è§£å†³æ–¹æ¡ˆï¼š
  å°†å¤§æ¶ˆæ¯æ‹†åˆ†æˆ < FRAGMENT_SIZE (é»˜è®¤128KB) çš„å°ç‰‡æ®µ
  æ¯ä¸ªç‰‡æ®µç‹¬ç«‹ç¡®è®¤å’Œé‡ä¼ 
"""

import threading
import time
import logging
import queue
from dataclasses import dataclass, field
from typing import Dict, Optional, Any, Callable, Set, List, Tuple
from collections import OrderedDict
import hashlib
import pickle
import zlib
import uuid
import struct

logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ”§ æ ¸å¿ƒé…ç½®ï¼šåŸºäºç³»ç»Ÿbufferé™åˆ¶
# ============================================================================
# ç³»ç»Ÿé™åˆ¶: rmem_max = wmem_max â‰ˆ 208KB (æ— sudoæ— æ³•ä¿®æ”¹)
# å®‰å…¨ä½™é‡: ä½¿ç”¨128KBä½œä¸ºåˆ†ç‰‡å¤§å°ï¼Œç•™å‡ºç©ºé—´ç»™åè®®å¤´
FRAGMENT_SIZE = 128 * 1024  # 128KB - å®‰å…¨åˆ†ç‰‡å¤§å°
MAX_MESSAGE_SIZE = 300 * 1024 * 1024  # 300MB - gRPCé…ç½®çš„æœ€å¤§æ¶ˆæ¯


@dataclass
class Fragment:
    """æ¶ˆæ¯åˆ†ç‰‡"""
    message_id: str          # åŸå§‹æ¶ˆæ¯çš„å”¯ä¸€ID
    fragment_index: int      # åˆ†ç‰‡ç´¢å¼• (0-based)
    total_fragments: int     # æ€»åˆ†ç‰‡æ•°
    data: bytes              # åˆ†ç‰‡æ•°æ®
    checksum: str = ""       # åˆ†ç‰‡æ ¡éªŒå’Œ

    def compute_checksum(self) -> str:
        """è®¡ç®—åˆ†ç‰‡æ ¡éªŒå’Œ"""
        return hashlib.md5(self.data).hexdigest()[:8]

    def verify(self) -> bool:
        """éªŒè¯åˆ†ç‰‡å®Œæ•´æ€§"""
        if not self.checksum:
            return True
        return self.compute_checksum() == self.checksum


class MessageFragmenter:
    """
    ğŸ”ª æ¶ˆæ¯åˆ†ç‰‡å™¨
    å°†å¤§æ¶ˆæ¯æ‹†åˆ†æˆå°äºbuffer zoneçš„ç‰‡æ®µ
    """

    def __init__(self, fragment_size: int = FRAGMENT_SIZE, enable_compression: bool = True):
        self.fragment_size = fragment_size
        self.enable_compression = enable_compression
        self.stats = {
            'messages_fragmented': 0,
            'fragments_created': 0,
            'bytes_before_compression': 0,
            'bytes_after_compression': 0,
        }

    def fragment(self, payload: Any, msg_type: str) -> Tuple[str, List[Fragment]]:
        """
        å°†payloadåˆ†ç‰‡
        è¿”å›: (message_id, fragments_list)
        """
        # åºåˆ—åŒ–
        data = pickle.dumps(payload)
        original_size = len(data)
        self.stats['bytes_before_compression'] += original_size

        # å¯é€‰å‹ç¼©ï¼ˆå¯¹å¤§æ•°æ®æœ‰æ•ˆï¼‰
        if self.enable_compression and original_size > 1024:
            compressed = zlib.compress(data, level=6)
            if len(compressed) < original_size * 0.9:  # å‹ç¼©ç‡>10%æ‰ä½¿ç”¨
                data = b'ZLIB' + compressed  # æ·»åŠ å‹ç¼©æ ‡è®°

        self.stats['bytes_after_compression'] += len(data)

        # ç”Ÿæˆæ¶ˆæ¯ID
        message_id = f"{msg_type}_{uuid.uuid4().hex[:8]}"

        # åˆ†ç‰‡
        fragments = []
        total_fragments = (len(data) + self.fragment_size - 1) // self.fragment_size

        for i in range(total_fragments):
            start = i * self.fragment_size
            end = min(start + self.fragment_size, len(data))
            fragment_data = data[start:end]

            fragment = Fragment(
                message_id=message_id,
                fragment_index=i,
                total_fragments=total_fragments,
                data=fragment_data,
            )
            fragment.checksum = fragment.compute_checksum()
            fragments.append(fragment)

        self.stats['messages_fragmented'] += 1
        self.stats['fragments_created'] += len(fragments)

        if total_fragments > 1:
            logger.debug(f"[Fragmenter] Split message {message_id}: {original_size} bytes -> "
                        f"{len(data)} bytes (compressed) -> {total_fragments} fragments")

        return message_id, fragments

    def get_compression_ratio(self) -> float:
        """è·å–å‹ç¼©ç‡"""
        if self.stats['bytes_before_compression'] == 0:
            return 1.0
        return self.stats['bytes_after_compression'] / self.stats['bytes_before_compression']


# ============================================================================
# ğŸš€ è½»é‡çº§åˆ†ç‰‡ä¼ è¾“ (Lightweight Fragmented Transport, LFT)
# ============================================================================
# è®¾è®¡ç›®æ ‡ï¼šæœ€å°åŒ–CPUå¼€é”€ï¼Œé€‚ç”¨äº64æ ¸æ¨¡æ‹Ÿ50èŠ‚ç‚¹çš„åœºæ™¯
#
# ä¸å®Œæ•´ALRTçš„åŒºåˆ«ï¼š
# - æ— å‹ç¼©ï¼šçœå»zlibçš„CPUå¼€é”€
# - CRC32ä»£æ›¿MD5ï¼šå¿«5å€
# - æ— æ»‘åŠ¨çª—å£/ACKï¼šä¾èµ–gRPCçš„é‡è¯•æœºåˆ¶
# - é›¶æ‹·è´è®¾è®¡ï¼šç›´æ¥æ“ä½œbytesï¼Œé¿å…å¯¹è±¡åˆ›å»º
# - æ— é¢å¤–çº¿ç¨‹ï¼šæ‰€æœ‰æ“ä½œåŒæ­¥å®Œæˆ
# ============================================================================

# ä½¿ç”¨CRC32ä»£æ›¿MD5ï¼ˆå¿«5å€ï¼‰
def fast_checksum(data: bytes) -> int:
    """å¿«é€Ÿæ ¡éªŒå’Œï¼šCRC32æ¯”MD5å¿«5å€"""
    return zlib.crc32(data) & 0xffffffff


class LightweightFragmenter:
    """
    ğŸš€ è½»é‡çº§åˆ†ç‰‡å™¨ - æœ€å°CPUå¼€é”€

    ç‰¹ç‚¹ï¼š
    - æ— å‹ç¼©
    - CRC32æ ¡éªŒï¼ˆæ¯”MD5å¿«5å€ï¼‰
    - é›¶æ‹·è´ï¼šç›´æ¥memoryviewåˆ‡ç‰‡
    - æ— å¯¹è±¡åˆ›å»ºå¼€é”€
    """

    __slots__ = ('fragment_size', 'msg_counter')

    def __init__(self, fragment_size: int = FRAGMENT_SIZE):
        self.fragment_size = fragment_size
        self.msg_counter = 0

    def fragment_bytes(self, data: bytes) -> List[Tuple[int, int, int, bytes, int]]:
        """
        åˆ†ç‰‡bytesæ•°æ®

        è¿”å›: List[(msg_id, frag_idx, total_frags, data_slice, crc32)]
        ä½¿ç”¨tupleè€Œédataclassï¼Œå‡å°‘å¯¹è±¡åˆ›å»ºå¼€é”€
        """
        total_size = len(data)
        if total_size <= self.fragment_size:
            # å°æ¶ˆæ¯ä¸éœ€è¦åˆ†ç‰‡
            return [(self.msg_counter, 0, 1, data, fast_checksum(data))]

        msg_id = self.msg_counter
        self.msg_counter += 1

        # ä½¿ç”¨memoryviewå®ç°é›¶æ‹·è´åˆ‡ç‰‡
        mv = memoryview(data)
        total_frags = (total_size + self.fragment_size - 1) // self.fragment_size

        fragments = []
        for i in range(total_frags):
            start = i * self.fragment_size
            end = min(start + self.fragment_size, total_size)
            frag_data = bytes(mv[start:end])  # åªåœ¨è¿™é‡Œåˆ›å»ºbytes
            fragments.append((msg_id, i, total_frags, frag_data, fast_checksum(frag_data)))

        return fragments


class LightweightReassembler:
    """
    ğŸš€ è½»é‡çº§é‡ç»„å™¨ - æœ€å°CPUå¼€é”€

    ç‰¹ç‚¹ï¼š
    - æ— é”è®¾è®¡ï¼ˆæ¯ä¸ªpeerç‹¬ç«‹çš„reassemblerï¼‰
    - é¢„åˆ†é…ç¼“å†²åŒº
    - æœ€å°åŒ–dictæ“ä½œ
    """

    __slots__ = ('pending', 'timeout')

    def __init__(self, timeout: float = 60.0):
        # msg_id -> [total_frags, received_count, first_time, frags_dict]
        self.pending: Dict[int, List] = {}
        self.timeout = timeout

    def add_fragment(self, msg_id: int, frag_idx: int, total_frags: int,
                     data: bytes, checksum: int) -> Optional[bytes]:
        """
        æ·»åŠ åˆ†ç‰‡ï¼Œè¿”å›é‡ç»„åçš„å®Œæ•´æ•°æ®ï¼ˆå¦‚æœå®Œæˆï¼‰
        """
        # éªŒè¯CRC32
        if fast_checksum(data) != checksum:
            return None

        # åˆå§‹åŒ–æˆ–è·å–pending entry
        if msg_id not in self.pending:
            self.pending[msg_id] = [total_frags, 0, time.time(), {}]

        entry = self.pending[msg_id]
        frags_dict = entry[3]

        # é¿å…é‡å¤åˆ†ç‰‡
        if frag_idx in frags_dict:
            return None

        frags_dict[frag_idx] = data
        entry[1] += 1  # received_count++

        # æ£€æŸ¥æ˜¯å¦å®Œæ•´
        if entry[1] == entry[0]:
            # é‡ç»„ï¼šé¢„åˆ†é…bytearrayé¿å…å¤šæ¬¡æ‹·è´
            total_size = sum(len(frags_dict[i]) for i in range(total_frags))
            result = bytearray(total_size)
            offset = 0
            for i in range(total_frags):
                frag = frags_dict[i]
                result[offset:offset+len(frag)] = frag
                offset += len(frag)

            del self.pending[msg_id]
            return bytes(result)

        return None

    def cleanup(self) -> int:
        """æ¸…ç†è¶…æ—¶æ¶ˆæ¯ï¼Œè¿”å›æ¸…ç†æ•°é‡"""
        now = time.time()
        expired = [mid for mid, entry in self.pending.items()
                   if now - entry[2] > self.timeout]
        for mid in expired:
            del self.pending[mid]
        return len(expired)


class ZeroCopyTransport:
    """
    ğŸš€ é›¶æ‹·è´ä¼ è¾“å±‚ - ç›´æ¥é›†æˆåˆ°gRPCå‘é€

    ä½¿ç”¨æ–¹å¼ï¼š
        transport = ZeroCopyTransport(original_send_func)
        transport.send(large_data)  # è‡ªåŠ¨åˆ†ç‰‡å‘é€

    CPUå¼€é”€åˆ†æï¼ˆvs å®Œæ•´ALRTï¼‰ï¼š
    - æ— pickleï¼šæ•°æ®å·²ç»æ˜¯bytes
    - æ— å‹ç¼©ï¼šçœå»zlib compress
    - CRC32ä»£æ›¿MD5ï¼šå¿«5å€
    - æ— çº¿ç¨‹ï¼šåŒæ­¥æ“ä½œ
    - æ— é”ï¼šæ¯ä¸ªè¿æ¥ç‹¬ç«‹

    é¢„ä¼°CPUèŠ‚çœï¼š~70%
    """

    def __init__(self,
                 send_func: Callable[[bytes], bool],
                 fragment_size: int = FRAGMENT_SIZE,
                 max_retries: int = 3):
        self.send_func = send_func
        self.fragmenter = LightweightFragmenter(fragment_size)
        self.max_retries = max_retries

        # ç»Ÿè®¡ï¼ˆå¯é€‰ï¼Œè°ƒè¯•ç”¨ï¼‰
        self.stats_enabled = False
        self.bytes_sent = 0
        self.fragments_sent = 0
        self.retries = 0

    def send(self, data: bytes) -> bool:
        """
        å‘é€æ•°æ®ï¼Œè‡ªåŠ¨åˆ†ç‰‡
        è¿”å›æ˜¯å¦å…¨éƒ¨æˆåŠŸ
        """
        fragments = self.fragmenter.fragment_bytes(data)

        for msg_id, frag_idx, total_frags, frag_data, checksum in fragments:
            # æ„é€ åˆ†ç‰‡å¤´ï¼ˆ16å­—èŠ‚å›ºå®šå¼€é”€ï¼‰
            header = struct.pack('<IIII', msg_id, frag_idx, total_frags, checksum)
            packet = header + frag_data

            # å‘é€ï¼ˆå¸¦é‡è¯•ï¼‰
            success = False
            for attempt in range(self.max_retries):
                try:
                    if self.send_func(packet):
                        success = True
                        break
                except Exception:
                    if attempt < self.max_retries - 1:
                        time.sleep(0.1 * (attempt + 1))  # ç®€å•é€€é¿
                    self.retries += 1

            if not success:
                return False

            if self.stats_enabled:
                self.bytes_sent += len(packet)
                self.fragments_sent += 1

        return True

    def receive(self, packet: bytes, reassembler: LightweightReassembler) -> Optional[bytes]:
        """
        æ¥æ”¶æ•°æ®ï¼Œè‡ªåŠ¨é‡ç»„
        """
        if len(packet) < 16:
            return None

        # è§£æå¤´
        msg_id, frag_idx, total_frags, checksum = struct.unpack('<IIII', packet[:16])
        frag_data = packet[16:]

        return reassembler.add_fragment(msg_id, frag_idx, total_frags, frag_data, checksum)


class MessageReassembler:
    """
    ğŸ”§ æ¶ˆæ¯é‡ç»„å™¨
    å°†åˆ†ç‰‡é‡æ–°ç»„è£…æˆå®Œæ•´æ¶ˆæ¯
    """

    def __init__(self, timeout: float = 60.0):
        self.timeout = timeout
        self.pending: Dict[str, Dict[int, Fragment]] = {}  # message_id -> {index: fragment}
        self.metadata: Dict[str, Tuple[int, float]] = {}   # message_id -> (total_fragments, first_received_time)
        self.lock = threading.Lock()
        self.stats = {
            'messages_reassembled': 0,
            'fragments_received': 0,
            'duplicate_fragments': 0,
            'corrupted_fragments': 0,
            'timeout_messages': 0,
        }

    def add_fragment(self, fragment: Fragment) -> Optional[Any]:
        """
        æ·»åŠ åˆ†ç‰‡ï¼Œå¦‚æœæ¶ˆæ¯å®Œæ•´åˆ™è¿”å›é‡ç»„åçš„payload
        """
        # éªŒè¯åˆ†ç‰‡å®Œæ•´æ€§
        if not fragment.verify():
            logger.warning(f"[Reassembler] Corrupted fragment: {fragment.message_id}[{fragment.fragment_index}]")
            self.stats['corrupted_fragments'] += 1
            return None

        self.stats['fragments_received'] += 1
        message_id = fragment.message_id

        with self.lock:
            # åˆå§‹åŒ–æ¶ˆæ¯ç¼“å†²
            if message_id not in self.pending:
                self.pending[message_id] = {}
                self.metadata[message_id] = (fragment.total_fragments, time.time())

            # æ£€æŸ¥é‡å¤åˆ†ç‰‡
            if fragment.fragment_index in self.pending[message_id]:
                self.stats['duplicate_fragments'] += 1
                logger.debug(f"[Reassembler] Duplicate fragment: {message_id}[{fragment.fragment_index}]")

            # å­˜å‚¨åˆ†ç‰‡
            self.pending[message_id][fragment.fragment_index] = fragment

            # æ£€æŸ¥æ˜¯å¦å®Œæ•´
            total_fragments, _ = self.metadata[message_id]
            if len(self.pending[message_id]) == total_fragments:
                return self._reassemble(message_id)

        return None

    def _reassemble(self, message_id: str) -> Optional[Any]:
        """é‡ç»„æ¶ˆæ¯"""
        fragments = self.pending.pop(message_id)
        self.metadata.pop(message_id, None)

        # æŒ‰é¡ºåºæ‹¼æ¥æ•°æ®
        sorted_fragments = sorted(fragments.values(), key=lambda f: f.fragment_index)
        data = b''.join(f.data for f in sorted_fragments)

        # è§£å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if data.startswith(b'ZLIB'):
            data = zlib.decompress(data[4:])

        # ååºåˆ—åŒ–
        try:
            payload = pickle.loads(data)
            self.stats['messages_reassembled'] += 1
            logger.debug(f"[Reassembler] Reassembled message {message_id}: {len(sorted_fragments)} fragments")
            return payload
        except Exception as e:
            logger.error(f"[Reassembler] Failed to deserialize {message_id}: {e}")
            return None

    def cleanup_timeout(self) -> List[str]:
        """æ¸…ç†è¶…æ—¶çš„æœªå®Œæˆæ¶ˆæ¯"""
        now = time.time()
        timeout_ids = []

        with self.lock:
            for message_id, (total, first_time) in list(self.metadata.items()):
                if now - first_time > self.timeout:
                    received = len(self.pending.get(message_id, {}))
                    logger.warning(f"[Reassembler] Timeout message {message_id}: {received}/{total} fragments")
                    self.pending.pop(message_id, None)
                    self.metadata.pop(message_id, None)
                    self.stats['timeout_messages'] += 1
                    timeout_ids.append(message_id)

        return timeout_ids

    def get_missing_fragments(self, message_id: str) -> List[int]:
        """è·å–ç¼ºå¤±çš„åˆ†ç‰‡ç´¢å¼•ï¼ˆç”¨äºè¯·æ±‚é‡ä¼ ï¼‰"""
        with self.lock:
            if message_id not in self.pending:
                return []

            total, _ = self.metadata.get(message_id, (0, 0))
            received = set(self.pending[message_id].keys())
            return [i for i in range(total) if i not in received]


@dataclass
class ReliableMessage:
    """å¸¦å¯é æ€§å…ƒæ•°æ®çš„æ¶ˆæ¯åŒ…è£…"""
    seq_num: int                    # åºåˆ—å·
    payload: Any                    # åŸå§‹æ¶ˆæ¯å†…å®¹
    msg_type: str                   # æ¶ˆæ¯ç±»å‹
    sender_id: int                  # å‘é€è€…ID
    receiver_id: int                # æ¥æ”¶è€…ID
    timestamp: float = field(default_factory=time.time)
    checksum: str = ""              # æ ¡éªŒå’Œï¼ˆå¯é€‰ï¼Œç”¨äºå®Œæ•´æ€§éªŒè¯ï¼‰
    is_ack: bool = False            # æ˜¯å¦æ˜¯ACKæ¶ˆæ¯
    ack_seq: int = -1               # ACKçš„åºåˆ—å·ï¼ˆå¦‚æœis_ack=Trueï¼‰
    is_retransmit: bool = False     # æ˜¯å¦æ˜¯é‡ä¼ 

    def compute_checksum(self) -> str:
        """è®¡ç®—payloadçš„æ ¡éªŒå’Œ"""
        if self.payload is None:
            return ""
        try:
            data = pickle.dumps(self.payload)
            return hashlib.md5(data).hexdigest()[:8]
        except:
            return ""


class SlidingWindow:
    """æ»‘åŠ¨çª—å£ç®¡ç†å™¨"""

    def __init__(self, window_size: int = 32):
        self.window_size = window_size
        self.base = 0                    # çª—å£åŸºå€ï¼ˆæœ€å°æœªç¡®è®¤åºåˆ—å·ï¼‰
        self.next_seq = 0                # ä¸‹ä¸€ä¸ªå¯ç”¨åºåˆ—å·
        self.pending: OrderedDict[int, ReliableMessage] = OrderedDict()  # å¾…ç¡®è®¤æ¶ˆæ¯
        self.send_times: Dict[int, float] = {}  # å‘é€æ—¶é—´è®°å½•
        self.lock = threading.Lock()

    def can_send(self) -> bool:
        """æ£€æŸ¥çª—å£æ˜¯å¦æœ‰ç©ºé—´å‘é€æ–°æ¶ˆæ¯"""
        with self.lock:
            return self.next_seq < self.base + self.window_size

    def allocate_seq(self) -> int:
        """åˆ†é…æ–°çš„åºåˆ—å·"""
        with self.lock:
            seq = self.next_seq
            self.next_seq += 1
            return seq

    def add_pending(self, msg: ReliableMessage):
        """æ·»åŠ å¾…ç¡®è®¤æ¶ˆæ¯"""
        with self.lock:
            self.pending[msg.seq_num] = msg
            self.send_times[msg.seq_num] = time.time()

    def ack_received(self, seq_num: int) -> bool:
        """å¤„ç†ACKï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
        with self.lock:
            if seq_num in self.pending:
                del self.pending[seq_num]
                if seq_num in self.send_times:
                    del self.send_times[seq_num]
                # æ»‘åŠ¨çª—å£åŸºå€
                while self.base not in self.pending and self.base < self.next_seq:
                    self.base += 1
                return True
            return False

    def get_timeout_messages(self, timeout: float = 5.0) -> list:
        """è·å–è¶…æ—¶æœªç¡®è®¤çš„æ¶ˆæ¯"""
        with self.lock:
            now = time.time()
            timeout_msgs = []
            for seq, msg in self.pending.items():
                if seq in self.send_times and now - self.send_times[seq] > timeout:
                    timeout_msgs.append(msg)
                    self.send_times[seq] = now  # é‡ç½®å‘é€æ—¶é—´
            return timeout_msgs

    def get_pending_count(self) -> int:
        """è·å–å¾…ç¡®è®¤æ¶ˆæ¯æ•°é‡"""
        with self.lock:
            return len(self.pending)


class ReliableTransportLayer:
    """
    ğŸ›¡ï¸ å¯é ä¼ è¾“å±‚
    åŒ…è£…ç°æœ‰çš„gRPCé€šä¿¡ï¼Œæä¾›åº”ç”¨å±‚å¯é æ€§ä¿è¯
    """

    def __init__(
        self,
        node_id: int,
        send_func: Callable[[Any], None],      # åº•å±‚å‘é€å‡½æ•°
        on_message_received: Callable[[Any], None],  # æ¶ˆæ¯æ¥æ”¶å›è°ƒ
        window_size: int = 32,
        ack_timeout: float = 5.0,
        max_retries: int = 5,
        heartbeat_interval: float = 30.0,
        enable_checksum: bool = True
    ):
        self.node_id = node_id
        self.send_func = send_func
        self.on_message_received = on_message_received

        # é…ç½®å‚æ•°
        self.window_size = window_size
        self.ack_timeout = ack_timeout
        self.max_retries = max_retries
        self.heartbeat_interval = heartbeat_interval
        self.enable_checksum = enable_checksum

        # æ¯ä¸ªpeerçš„æ»‘åŠ¨çª—å£
        self.send_windows: Dict[int, SlidingWindow] = {}

        # æ¥æ”¶ç«¯çŠ¶æ€
        self.expected_seq: Dict[int, int] = {}      # æ¯ä¸ªpeeræœŸæœ›çš„ä¸‹ä¸€ä¸ªåºåˆ—å·
        self.received_seqs: Dict[int, Set[int]] = {}  # æ¯ä¸ªpeerå·²æ¥æ”¶çš„åºåˆ—å·é›†åˆ
        self.out_of_order_buffer: Dict[int, Dict[int, ReliableMessage]] = {}  # ä¹±åºç¼“å†²åŒº

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'messages_sent': 0,
            'messages_received': 0,
            'acks_sent': 0,
            'acks_received': 0,
            'retransmits': 0,
            'duplicates_dropped': 0,
            'checksum_errors': 0,
        }

        # çº¿ç¨‹æ§åˆ¶
        self.running = False
        self.retransmit_thread = None
        self.lock = threading.Lock()

        logger.info(f"[ALRT] Node {node_id}: Reliable transport initialized "
                   f"(window={window_size}, timeout={ack_timeout}s, max_retries={max_retries})")

    def start(self):
        """å¯åŠ¨å¯é ä¼ è¾“å±‚"""
        self.running = True
        self.retransmit_thread = threading.Thread(
            target=self._retransmit_loop,
            daemon=True,
            name=f"ALRT-Retransmit-{self.node_id}"
        )
        self.retransmit_thread.start()
        logger.info(f"[ALRT] Node {self.node_id}: Transport layer started")

    def stop(self):
        """åœæ­¢å¯é ä¼ è¾“å±‚"""
        self.running = False
        if self.retransmit_thread:
            self.retransmit_thread.join(timeout=2.0)
        logger.info(f"[ALRT] Node {self.node_id}: Transport layer stopped")

    def _get_send_window(self, peer_id: int) -> SlidingWindow:
        """è·å–æˆ–åˆ›å»ºpeerçš„å‘é€çª—å£"""
        if peer_id not in self.send_windows:
            self.send_windows[peer_id] = SlidingWindow(self.window_size)
        return self.send_windows[peer_id]

    def send_reliable(self, receiver_id: int, msg_type: str, payload: Any) -> bool:
        """
        å¯é å‘é€æ¶ˆæ¯
        è¿”å›æ˜¯å¦æˆåŠŸåŠ å…¥å‘é€é˜Ÿåˆ—ï¼ˆä¸ä¿è¯å·²é€è¾¾ï¼‰
        """
        window = self._get_send_window(receiver_id)

        # æ£€æŸ¥çª—å£æ˜¯å¦å·²æ»¡
        if not window.can_send():
            logger.warning(f"[ALRT] Node {self.node_id}: Send window full for peer {receiver_id}")
            # å¯ä»¥é€‰æ‹©é˜»å¡ç­‰å¾…æˆ–ç›´æ¥è¿”å›å¤±è´¥
            # è¿™é‡Œé€‰æ‹©ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            wait_start = time.time()
            while not window.can_send() and time.time() - wait_start < 1.0:
                time.sleep(0.01)
            if not window.can_send():
                return False

        # åˆ†é…åºåˆ—å·
        seq_num = window.allocate_seq()

        # åˆ›å»ºå¯é æ¶ˆæ¯
        reliable_msg = ReliableMessage(
            seq_num=seq_num,
            payload=payload,
            msg_type=msg_type,
            sender_id=self.node_id,
            receiver_id=receiver_id,
        )

        if self.enable_checksum:
            reliable_msg.checksum = reliable_msg.compute_checksum()

        # æ·»åŠ åˆ°å¾…ç¡®è®¤é˜Ÿåˆ—
        window.add_pending(reliable_msg)

        # å‘é€
        self._do_send(reliable_msg)
        self.stats['messages_sent'] += 1

        logger.debug(f"[ALRT] Node {self.node_id}: Sent msg seq={seq_num} to peer {receiver_id}")
        return True

    def _do_send(self, msg: ReliableMessage):
        """å®é™…å‘é€æ¶ˆæ¯"""
        try:
            # å°†å¯é æ¶ˆæ¯åŒ…è£…åé€šè¿‡åº•å±‚å‘é€
            wrapped = {
                '_alrt_wrapper': True,
                'reliable_msg': msg
            }
            self.send_func(wrapped)
        except Exception as e:
            logger.error(f"[ALRT] Node {self.node_id}: Send failed: {e}")

    def _send_ack(self, receiver_id: int, ack_seq: int):
        """å‘é€ACKç¡®è®¤"""
        ack_msg = ReliableMessage(
            seq_num=-1,  # ACKä¸éœ€è¦åºåˆ—å·
            payload=None,
            msg_type='_alrt_ack',
            sender_id=self.node_id,
            receiver_id=receiver_id,
            is_ack=True,
            ack_seq=ack_seq
        )
        self._do_send(ack_msg)
        self.stats['acks_sent'] += 1
        logger.debug(f"[ALRT] Node {self.node_id}: Sent ACK for seq={ack_seq} to peer {receiver_id}")

    def on_raw_message_received(self, raw_msg: Any):
        """
        å¤„ç†æ¥æ”¶åˆ°çš„åŸå§‹æ¶ˆæ¯
        éœ€è¦åœ¨gRPCæ¥æ”¶ç«¯è°ƒç”¨æ­¤æ–¹æ³•
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯ALRTåŒ…è£…çš„æ¶ˆæ¯
        if isinstance(raw_msg, dict) and raw_msg.get('_alrt_wrapper'):
            reliable_msg = raw_msg['reliable_msg']
            self._handle_reliable_message(reliable_msg)
        else:
            # éALRTæ¶ˆæ¯ï¼Œç›´æ¥é€ä¼ 
            self.on_message_received(raw_msg)

    def _handle_reliable_message(self, msg: ReliableMessage):
        """å¤„ç†å¯é æ¶ˆæ¯"""
        sender_id = msg.sender_id

        # å¤„ç†ACKæ¶ˆæ¯
        if msg.is_ack:
            self._handle_ack(sender_id, msg.ack_seq)
            return

        # æ ¡éªŒchecksum
        if self.enable_checksum and msg.checksum:
            computed = msg.compute_checksum()
            if computed != msg.checksum:
                logger.warning(f"[ALRT] Node {self.node_id}: Checksum mismatch for seq={msg.seq_num} from peer {sender_id}")
                self.stats['checksum_errors'] += 1
                # ä¸å‘é€ACKï¼Œç­‰å¾…é‡ä¼ 
                return

        # åˆå§‹åŒ–æ¥æ”¶ç«¯çŠ¶æ€
        if sender_id not in self.expected_seq:
            self.expected_seq[sender_id] = 0
            self.received_seqs[sender_id] = set()
            self.out_of_order_buffer[sender_id] = {}

        seq_num = msg.seq_num

        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤æ¶ˆæ¯
        if seq_num in self.received_seqs[sender_id]:
            logger.debug(f"[ALRT] Node {self.node_id}: Duplicate msg seq={seq_num} from peer {sender_id}, sending ACK")
            self.stats['duplicates_dropped'] += 1
            self._send_ack(sender_id, seq_num)  # é‡å¤æ¶ˆæ¯ä¹Ÿè¦å‘ACK
            return

        # è®°å½•å·²æ¥æ”¶
        self.received_seqs[sender_id].add(seq_num)
        self._send_ack(sender_id, seq_num)

        # æ£€æŸ¥æ˜¯å¦æŒ‰åºåˆ°è¾¾
        expected = self.expected_seq[sender_id]

        if seq_num == expected:
            # æŒ‰åºåˆ°è¾¾ï¼Œå¤„ç†æ¶ˆæ¯
            self._deliver_message(msg)
            self.expected_seq[sender_id] = seq_num + 1

            # æ£€æŸ¥ä¹±åºç¼“å†²åŒºæ˜¯å¦æœ‰åç»­æ¶ˆæ¯å¯ä»¥å¤„ç†
            self._process_out_of_order_buffer(sender_id)
        else:
            # ä¹±åºåˆ°è¾¾ï¼Œç¼“å­˜
            self.out_of_order_buffer[sender_id][seq_num] = msg
            logger.debug(f"[ALRT] Node {self.node_id}: Out-of-order msg seq={seq_num} from peer {sender_id}, expected={expected}")

    def _process_out_of_order_buffer(self, sender_id: int):
        """å¤„ç†ä¹±åºç¼“å†²åŒº"""
        buffer = self.out_of_order_buffer[sender_id]
        while self.expected_seq[sender_id] in buffer:
            seq = self.expected_seq[sender_id]
            msg = buffer.pop(seq)
            self._deliver_message(msg)
            self.expected_seq[sender_id] = seq + 1

    def _deliver_message(self, msg: ReliableMessage):
        """äº¤ä»˜æ¶ˆæ¯ç»™ä¸Šå±‚"""
        self.stats['messages_received'] += 1
        logger.debug(f"[ALRT] Node {self.node_id}: Delivering msg seq={msg.seq_num} type={msg.msg_type}")
        # è°ƒç”¨ä¸Šå±‚å›è°ƒï¼Œä¼ é€’åŸå§‹payload
        self.on_message_received(msg.payload)

    def _handle_ack(self, sender_id: int, ack_seq: int):
        """å¤„ç†æ”¶åˆ°çš„ACK"""
        window = self._get_send_window(sender_id)
        if window.ack_received(ack_seq):
            self.stats['acks_received'] += 1
            logger.debug(f"[ALRT] Node {self.node_id}: ACK received for seq={ack_seq} from peer {sender_id}")

    def _retransmit_loop(self):
        """é‡ä¼ å¾ªç¯"""
        retry_counts: Dict[tuple, int] = {}  # (peer_id, seq_num) -> retry_count

        while self.running:
            time.sleep(1.0)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡

            for peer_id, window in list(self.send_windows.items()):
                timeout_msgs = window.get_timeout_messages(self.ack_timeout)

                for msg in timeout_msgs:
                    key = (peer_id, msg.seq_num)
                    retry_counts[key] = retry_counts.get(key, 0) + 1

                    if retry_counts[key] > self.max_retries:
                        logger.error(f"[ALRT] Node {self.node_id}: Max retries exceeded for seq={msg.seq_num} to peer {peer_id}")
                        # å¯ä»¥é€‰æ‹©ï¼šé€šçŸ¥ä¸Šå±‚ã€æ”¾å¼ƒæ¶ˆæ¯ã€æˆ–ç»§ç»­å°è¯•
                        continue

                    # é‡ä¼ 
                    msg.is_retransmit = True
                    self._do_send(msg)
                    self.stats['retransmits'] += 1
                    logger.warning(f"[ALRT] Node {self.node_id}: Retransmit seq={msg.seq_num} to peer {peer_id} "
                                 f"(attempt {retry_counts[key]}/{self.max_retries})")

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        stats['pending_messages'] = sum(w.get_pending_count() for w in self.send_windows.values())
        return stats

    def get_stats_summary(self) -> str:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        stats = self.get_stats()
        return (f"[ALRT Stats] sent={stats['messages_sent']}, recv={stats['messages_received']}, "
                f"retrans={stats['retransmits']}, acks={stats['acks_received']}, "
                f"pending={stats['pending_messages']}, dups={stats['duplicates_dropped']}")


class ReliableCommManagerWrapper:
    """
    ğŸ›¡ï¸ å¯é é€šä¿¡ç®¡ç†å™¨åŒ…è£…å™¨
    å°†ç°æœ‰çš„gRPCCommManageråŒ…è£…æˆå¯é ç‰ˆæœ¬
    """

    def __init__(self, comm_manager, node_id: int, **kwargs):
        """
        Args:
            comm_manager: åŸå§‹çš„gRPCCommManagerå®ä¾‹
            node_id: å½“å‰èŠ‚ç‚¹ID
            **kwargs: ä¼ é€’ç»™ReliableTransportLayerçš„å‚æ•°
        """
        self.comm_manager = comm_manager
        self.node_id = node_id

        # ä¿å­˜åŸå§‹çš„receiveå¤„ç†æ–¹æ³•
        self._original_receive = comm_manager.receive if hasattr(comm_manager, 'receive') else None

        # æ¶ˆæ¯å›è°ƒé˜Ÿåˆ—
        self.received_queue = queue.Queue()

        # åˆ›å»ºå¯é ä¼ è¾“å±‚
        self.transport = ReliableTransportLayer(
            node_id=node_id,
            send_func=self._raw_send,
            on_message_received=self._on_reliable_message,
            **kwargs
        )

        self.transport.start()
        logger.info(f"[ALRT] ReliableCommManagerWrapper initialized for node {node_id}")

    def _raw_send(self, wrapped_msg):
        """åº•å±‚å‘é€ï¼ˆåŒ…è£…åçš„ALRTæ¶ˆæ¯ï¼‰"""
        # è¿™é‡Œéœ€è¦å°†ALRTæ¶ˆæ¯é€šè¿‡åŸå§‹comm_managerå‘é€
        # å‡è®¾comm_manager.sendæ¥å—Messageå¯¹è±¡
        from federatedscope.core.message import Message

        reliable_msg = wrapped_msg.get('reliable_msg')
        if reliable_msg:
            # åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„Messageç”¨äºä¼ è¾“ALRTæ¶ˆæ¯
            msg = Message(
                msg_type='_alrt_transport',
                sender=self.node_id,
                receiver=[reliable_msg.receiver_id],
                content=wrapped_msg
            )
            self.comm_manager.send(msg)

    def _on_reliable_message(self, payload):
        """å¯é æ¶ˆæ¯åˆ°è¾¾å›è°ƒ"""
        self.received_queue.put(payload)

    def send(self, message):
        """
        å¯é å‘é€æ¶ˆæ¯
        """
        receiver = message.receiver
        if receiver is not None:
            if not isinstance(receiver, list):
                receiver = [receiver]

            for each_receiver in receiver:
                self.transport.send_reliable(
                    receiver_id=each_receiver,
                    msg_type=message.msg_type,
                    payload=message
                )
        else:
            # å¹¿æ’­ç»™æ‰€æœ‰é‚»å±…
            for each_receiver in self.comm_manager.neighbors:
                self.transport.send_reliable(
                    receiver_id=each_receiver,
                    msg_type=message.msg_type,
                    payload=message
                )

    def receive(self):
        """
        æ¥æ”¶æ¶ˆæ¯ï¼ˆéœ€è¦ä¸åŸå§‹receiveåè°ƒï¼‰
        """
        # å¦‚æœæœ‰å¯é ä¼ è¾“å±‚çš„æ¶ˆæ¯ï¼Œä¼˜å…ˆè¿”å›
        try:
            return self.received_queue.get_nowait()
        except queue.Empty:
            pass

        # å¦åˆ™è°ƒç”¨åŸå§‹receive
        if self._original_receive:
            raw_msg = self._original_receive()

            # æ£€æŸ¥æ˜¯å¦æ˜¯ALRTä¼ è¾“æ¶ˆæ¯
            if hasattr(raw_msg, 'msg_type') and raw_msg.msg_type == '_alrt_transport':
                self.transport.on_raw_message_received(raw_msg.content)
                # é€’å½’è·å–å®é™…æ¶ˆæ¯
                return self.receive()

            return raw_msg

        return None

    def get_stats(self) -> str:
        """è·å–ALRTç»Ÿè®¡ä¿¡æ¯"""
        return self.transport.get_stats_summary()

    def stop(self):
        """åœæ­¢å¯é ä¼ è¾“å±‚"""
        self.transport.stop()

    # ä»£ç†å…¶ä»–æ–¹æ³•ç»™åŸå§‹comm_manager
    def __getattr__(self, name):
        return getattr(self.comm_manager, name)


# ============================================================================
# Transparent Wrapper Layer for gRPC Communication
# ============================================================================
# Usage: Minimal modification to existing code
#   - Call lft_pack() before sending large data
#   - Call lft_unpack() after receiving data
# ============================================================================

# Global instances (thread-safe for single sender per connection)
_fragmenter_cache: Dict[int, LightweightFragmenter] = {}
_reassembler_cache: Dict[int, LightweightReassembler] = {}

# LFT packet header format: magic(4) + msg_id(4) + frag_idx(4) + total(4) + crc(4) = 20 bytes
LFT_HEADER_FORMAT = '<IIIII'
LFT_HEADER_SIZE = 20
LFT_MAGIC = 0x4C465421  # "LFT!" identifier


def get_fragmenter(node_id: int = 0) -> LightweightFragmenter:
    """Get or create fragmenter for a node (cached)"""
    if node_id not in _fragmenter_cache:
        _fragmenter_cache[node_id] = LightweightFragmenter(FRAGMENT_SIZE)
    return _fragmenter_cache[node_id]


def get_reassembler(node_id: int = 0) -> LightweightReassembler:
    """Get or create reassembler for a node (cached)"""
    if node_id not in _reassembler_cache:
        _reassembler_cache[node_id] = LightweightReassembler(timeout=60.0)
    return _reassembler_cache[node_id]


def lft_pack(data: bytes, node_id: int = 0) -> List[bytes]:
    """
    Pack data into LFT fragments.

    Args:
        data: Raw bytes to send
        node_id: Sender node ID (for fragmenter cache)

    Returns:
        List of fragment packets, each with header + payload

    Usage:
        packets = lft_pack(large_data)
        for pkt in packets:
            grpc_send(pkt)
    """
    if len(data) <= FRAGMENT_SIZE:
        # Small message: single fragment with header
        crc = fast_checksum(data)
        header = struct.pack(LFT_HEADER_FORMAT, LFT_MAGIC, 0, 0, 1, crc)
        return [header + data]

    # Large message: fragment
    fragmenter = get_fragmenter(node_id)
    fragments = fragmenter.fragment_bytes(data)

    packets = []
    for msg_id, frag_idx, total_frags, frag_data, crc in fragments:
        header = struct.pack(LFT_HEADER_FORMAT, LFT_MAGIC, msg_id, frag_idx, total_frags, crc)
        packets.append(header + frag_data)

    return packets


def lft_unpack(packet: bytes, node_id: int = 0) -> Optional[bytes]:
    """
    Unpack LFT fragment packet.

    Args:
        packet: Received packet (header + payload)
        node_id: Receiver node ID (for reassembler cache)

    Returns:
        - Complete reassembled data if all fragments received
        - None if waiting for more fragments
        - Original packet if not an LFT packet

    Usage:
        data = lft_unpack(received_packet)
        if data is not None:
            process(data)
    """
    # Check if it's an LFT packet
    if len(packet) < LFT_HEADER_SIZE:
        return packet  # Not LFT, return as-is

    magic, msg_id, frag_idx, total_frags, crc = struct.unpack(LFT_HEADER_FORMAT, packet[:LFT_HEADER_SIZE])

    if magic != LFT_MAGIC:
        return packet  # Not LFT packet, return as-is

    payload = packet[LFT_HEADER_SIZE:]

    # Verify CRC32
    if fast_checksum(payload) != crc:
        logger.warning(f"[LFT] CRC mismatch: msg={msg_id}, frag={frag_idx}")
        return None

    # Single fragment: return directly
    if total_frags == 1:
        return payload

    # Multi-fragment: reassemble
    reassembler = get_reassembler(node_id)
    return reassembler.add_fragment(msg_id, frag_idx, total_frags, payload, crc)


def lft_is_enabled() -> bool:
    """Check if LFT is enabled (based on buffer size detection)"""
    return FRAGMENT_SIZE < 200 * 1024  # Enabled if fragment size is reasonable


def lft_get_stats(node_id: int = 0) -> dict:
    """Get LFT statistics for a node"""
    stats = {
        'fragment_size': FRAGMENT_SIZE,
        'enabled': lft_is_enabled(),
    }

    if node_id in _fragmenter_cache:
        stats['messages_fragmented'] = _fragmenter_cache[node_id].msg_counter

    if node_id in _reassembler_cache:
        reassembler = _reassembler_cache[node_id]
        stats['pending_messages'] = len(reassembler.pending)

    return stats


def lft_cleanup(node_id: int = 0) -> int:
    """Cleanup timeout pending messages"""
    if node_id in _reassembler_cache:
        return _reassembler_cache[node_id].cleanup()
    return 0
