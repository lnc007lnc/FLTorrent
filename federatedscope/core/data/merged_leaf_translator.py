"""
MergedLeafTranslator for re-distributing LEAF datasets.

This translator merges all users from LEAF datasets (femnist, celeba, etc.)
and re-splits them using the configured splitter (LDA, IID, etc.).

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. åŸå§‹æ¨¡å¼ï¼šåŠ è½½å…¨éƒ¨æ•°æ®ååˆå¹¶åˆ†å‰²
2. å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼šåªåŠ è½½æ ‡ç­¾ï¼Œåˆ†å‰²åå†å»¶è¿ŸåŠ è½½å®é™…æ•°æ®ï¼ˆå†…å­˜é«˜æ•ˆï¼‰
"""

import logging
import numpy as np
from torch.utils.data import ConcatDataset, Dataset, Subset
from federatedscope.core.data.base_translator import BaseDataTranslator
from federatedscope.core.data import ClientData, StandaloneDataDict
from federatedscope.core.auxiliaries.splitter_builder import get_splitter
from federatedscope.core.splitters.utils import dirichlet_distribution_noniid_slice

logger = logging.getLogger(__name__)


class MergedLeafTranslator(BaseDataTranslator):
    """
    Translator that merges all LEAF users' data and re-distributes using splitter.

    For LEAF datasets (femnist, celeba, shakespeare, etc.), this translator:
    1. Merges all pre-split users into a single dataset
    2. Re-splits using the configured splitter (LDA, IID, etc.)
    3. Creates balanced or heterogeneous clients based on splitter config

    æ”¯æŒå»¶è¿ŸåŠ è½½æ¨¡å¼ï¼ˆå½“æ£€æµ‹åˆ° __lazy_merged__ æ ‡è®°æ—¶è‡ªåŠ¨å¯ç”¨ï¼‰ï¼š
    - åªåŠ è½½æ ‡ç­¾è¿›è¡Œ LDA åˆ†å‰²
    - åˆ†å‰²åæ¯ä¸ª client åªåŠ è½½è‡ªå·±çš„æ•°æ®
    - å†…å­˜å³°å€¼ä» NÃ—å…¨é‡æ•°æ® é™åˆ° å…¨é‡æ•°æ®/N

    Args:
        global_cfg: Global configuration
        client_cfgs: Client-specific configurations
    """

    def split(self, dataset):
        """
        Merge LEAF users and re-split using splitter.

        Args:
            dataset: LEAF dataset dict with format:
                {client_id: {'train': data, 'test': data, 'val': data}}
                æˆ–å»¶è¿ŸåŠ è½½æ ¼å¼:
                {'__lazy_merged__': {'train': LazyLoadDataset, ...}}

        Returns:
            dict: dict of ClientData with client_idx as key
        """
        if not isinstance(dataset, dict):
            raise TypeError(f'Expected dict for LEAF dataset, got {type(dataset)}')

        # æ£€æŸ¥æ˜¯å¦æ˜¯å»¶è¿ŸåŠ è½½æ¨¡å¼
        if '__lazy_merged__' in dataset:
            datadict = self._split_lazy(dataset['__lazy_merged__'])
        else:
            datadict = self._split_original(dataset)

        # === å…³é”®ä¼˜åŒ–ï¼šåªä¿ç•™å½“å‰è¿›ç¨‹éœ€è¦çš„ clientï¼Œä¸¢æ‰å…¶ä»– client çš„æ•°æ® ===
        # è¿™æ ·æ¯ä¸ªè¿›ç¨‹çš„å†…å­˜ä½¿ç”¨ä¸ client_num æ— å…³ï¼Œåªä¸è‡ªå·±è´Ÿè´£çš„ client æœ‰å…³
        datadict = self._filter_to_current_client(datadict)

        return datadict

    def _filter_to_current_client(self, datadict):
        """
        è¿‡æ»¤ datadictï¼Œåªä¿ç•™å½“å‰è¿›ç¨‹è´Ÿè´£çš„ client æ•°æ®ã€‚

        åœ¨åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªè¿›ç¨‹åªéœ€è¦è‡ªå·±é‚£ä¸ª client çš„æ•°æ®ï¼Œ
        ä¸éœ€è¦ä¸ºæ‰€æœ‰ client åˆ›å»º DataLoaderï¼Œè¿™æ ·å¯ä»¥å¤§å¹…å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

        å†…å­˜ä¼˜åŒ–æ•ˆæœï¼š
        - ä¿®æ”¹å‰ï¼šæ¯ä¸ªè¿›ç¨‹å†…å­˜ âˆ client_numï¼ˆæ‰€æœ‰ client çš„ DataLoaderï¼‰
        - ä¿®æ”¹åï¼šæ¯ä¸ªè¿›ç¨‹å†…å­˜ âˆ 1ï¼ˆåªæœ‰è‡ªå·±çš„ DataLoaderï¼‰
        """
        # è·å–å½“å‰è¿›ç¨‹è´Ÿè´£çš„ client ID
        # distribute.data_idx: 0 è¡¨ç¤º serverï¼Œ1~N è¡¨ç¤º client
        my_data_idx = getattr(self.global_cfg.distribute, 'data_idx', None)

        # å¦‚æœæ²¡æœ‰è®¾ç½® data_idx æˆ–è€…æ˜¯ standalone æ¨¡å¼ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®
        if my_data_idx is None:
            return datadict

        # Server (data_idx=0) éœ€è¦ä¿ç•™ server æ•°æ®
        if my_data_idx == 0:
            # Server åªéœ€è¦ key=0 çš„æ•°æ®ï¼Œä¸éœ€è¦ client æ•°æ®
            if 0 in datadict:
                return {0: datadict[0]}
            return datadict

        # Client è¿›ç¨‹ï¼šåªä¿ç•™è‡ªå·±çš„æ•°æ®å’Œ server çš„åŸºæœ¬æ•°æ®
        filtered = {}

        # ä¿ç•™ server æ•°æ®ï¼ˆä½†æ¸…ç©º train ä»¥èŠ‚çœå†…å­˜ï¼‰
        if 0 in datadict:
            server_cd = datadict[0]
            # Server çš„ train æ•°æ®å¯¹ client æ²¡ç”¨ï¼Œæ¸…ç©ºä»¥èŠ‚çœå†…å­˜
            server_cd.train_data = None
            filtered[0] = server_cd

        # ä¿ç•™å½“å‰ client çš„æ•°æ®
        if my_data_idx in datadict:
            filtered[my_data_idx] = datadict[my_data_idx]
            logger.info(f"ğŸ¯ Filtered datadict: keeping only client {my_data_idx} "
                       f"(dropped {len(datadict) - len(filtered)} other clients)")
        else:
            raise ValueError(f"client data_idx {my_data_idx} not in datadict keys {list(datadict.keys())}")

        return filtered

    def _split_lazy(self, lazy_data):
        """
        å»¶è¿ŸåŠ è½½æ¨¡å¼çš„åˆ†å‰²é€»è¾‘ã€‚

        åªä½¿ç”¨æ ‡ç­¾è¿›è¡Œ LDA åˆ†å‰²ï¼Œç„¶ååˆ›å»ºæŒ‡å‘åŸå§‹æ•°æ®çš„å­é›†ã€‚
        å†…å­˜é«˜æ•ˆï¼šåˆ†å‰²é˜¶æ®µåªéœ€è¦ ~1MBï¼ˆæ ‡ç­¾ï¼‰ï¼Œè®­ç»ƒé˜¶æ®µæ¯ä¸ª client åªåŠ è½½è‡ªå·±çš„æ•°æ®ã€‚

        Args:
            lazy_data: dict with {'train': LazyLoadDataset, 'val': ..., 'test': ...}

        Returns:
            dict: dict of ClientData with client_idx as key
        """
        from federatedscope.cv.dataset.leaf_cv import LazyLoadDataset

        client_num = self.global_cfg.federate.client_num

        logger.info(f"ğŸš€ MergedLeafTranslator: Using LAZY-LOAD mode")
        logger.info(f"   Memory efficient: only labels loaded for splitting")

        train_dataset = lazy_data['train']
        val_dataset = lazy_data['val']
        test_dataset = lazy_data['test']

        logger.info(f"ğŸ“Š Dataset sizes - Train: {len(train_dataset)}, "
                   f"Val: {len(val_dataset)}, Test: {len(test_dataset)}")

        # è·å–æ ‡ç­¾ï¼ˆå†…å­˜å ç”¨æå°ï¼‰
        train_labels = np.array(train_dataset.get_labels())
        val_labels = np.array(val_dataset.get_labels()) if len(val_dataset) > 0 else None
        test_labels = np.array(test_dataset.get_labels()) if len(test_dataset) > 0 else None

        # ä½¿ç”¨ LDA åˆ†å‰²ï¼ˆåªåŸºäºæ ‡ç­¾ï¼Œä¸éœ€è¦åŠ è½½å®é™…æ•°æ®ï¼‰
        splitter_type = self.global_cfg.data.splitter
        if splitter_type == 'lda':
            alpha = 0.5  # é»˜è®¤å€¼
            if self.global_cfg.data.splitter_args:
                for arg in self.global_cfg.data.splitter_args:
                    if 'alpha' in arg:
                        alpha = arg['alpha']

            logger.info(f"ğŸ² Using LDA splitter with alpha={alpha}")

            # åˆ†å‰²è®­ç»ƒé›†
            train_idx_slice = dirichlet_distribution_noniid_slice(
                train_labels, client_num, alpha)

            # ä½¿ç”¨ç›¸åŒçš„æ ‡ç­¾åˆ†å¸ƒåˆ†å‰²éªŒè¯é›†å’Œæµ‹è¯•é›†
            train_label_distribution = [train_labels[idxs].tolist() for idxs in train_idx_slice]

            if val_labels is not None and len(val_labels) > 0:
                val_idx_slice = dirichlet_distribution_noniid_slice(
                    val_labels, client_num, alpha, prior=train_label_distribution)
            else:
                val_idx_slice = [[] for _ in range(client_num)]

            if test_labels is not None and len(test_labels) > 0:
                if self.global_cfg.data.share_test_dataset:
                    # æ‰€æœ‰ client å…±äº«å®Œæ•´æµ‹è¯•é›†
                    test_idx_slice = [list(range(len(test_labels))) for _ in range(client_num)]
                else:
                    test_idx_slice = dirichlet_distribution_noniid_slice(
                        test_labels, client_num, alpha, prior=train_label_distribution)
            else:
                test_idx_slice = [[] for _ in range(client_num)]

        else:
            # IID åˆ†å‰²
            logger.info(f"ğŸ² Using IID splitter")
            indices = np.random.permutation(len(train_labels))
            split_size = len(train_labels) // client_num
            train_idx_slice = [indices[i*split_size:(i+1)*split_size].tolist()
                              for i in range(client_num)]

            if val_labels is not None and len(val_labels) > 0:
                val_indices = np.random.permutation(len(val_labels))
                val_split_size = len(val_labels) // client_num
                val_idx_slice = [val_indices[i*val_split_size:(i+1)*val_split_size].tolist()
                                for i in range(client_num)]
            else:
                val_idx_slice = [[] for _ in range(client_num)]

            if test_labels is not None and len(test_labels) > 0:
                if self.global_cfg.data.share_test_dataset:
                    test_idx_slice = [list(range(len(test_labels))) for _ in range(client_num)]
                else:
                    test_indices = np.random.permutation(len(test_labels))
                    test_split_size = len(test_labels) // client_num
                    test_idx_slice = [test_indices[i*test_split_size:(i+1)*test_split_size].tolist()
                                     for i in range(client_num)]
            else:
                test_idx_slice = [[] for _ in range(client_num)]

        # === å…³é”®ä¼˜åŒ–ï¼šåªä¸ºå½“å‰è¿›ç¨‹è´Ÿè´£çš„ client åˆ›å»ºæ•°æ® ===
        my_data_idx = getattr(self.global_cfg.distribute, 'data_idx', None)

        data_dict = {}

        # Server æ•°æ® (key=0)
        if my_data_idx is None or my_data_idx == 0:
            # Standalone æˆ– Serverï¼šä¿ç•™å®Œæ•´æ•°æ®
            data_dict[0] = ClientData(self.global_cfg,
                                     train=train_dataset,
                                     val=val_dataset,
                                     test=test_dataset)
        else:
            # Client è¿›ç¨‹ï¼šserver ä¸éœ€è¦ train
            data_dict[0] = ClientData(self.global_cfg,
                                     train=None,
                                     val=val_dataset,
                                     test=test_dataset)

        for client_id in range(1, client_num + 1):
            # === å…³é”®ï¼šè·³è¿‡ä¸å±äºå½“å‰è¿›ç¨‹çš„ client ===
            if my_data_idx is not None and my_data_idx > 0:
                if client_id != my_data_idx:
                    continue  # ä¸åˆ›å»º subsetï¼Œä¸åŠ è½½æ•°æ®

            idx = client_id - 1

            # åˆ›å»ºè¯¥ client çš„å­é›†ï¼ˆlazy=False ä¼šé¢„åŠ è½½æ•°æ®ï¼‰
            client_train = train_dataset.subset(train_idx_slice[idx]) if train_idx_slice[idx] else []
            client_val = val_dataset.subset(val_idx_slice[idx]) if val_idx_slice[idx] else []
            client_test = test_dataset.subset(test_idx_slice[idx]) if test_idx_slice[idx] else []

            if self.client_cfgs is not None:
                client_cfg = self.global_cfg.clone()
                client_cfg.merge_from_other_cfg(
                    self.client_cfgs.get(f'client_{client_id}'))
            else:
                client_cfg = self.global_cfg

            data_dict[client_id] = ClientData(client_cfg,
                                              train=client_train,
                                              val=client_val,
                                              test=client_test)

            logger.info(f"ğŸ¯ _split_lazy: Created data for client {client_id} only")

        logger.info(f"âœ… Re-distributed into {client_num} clients using lazy-load mode")
        logger.info(f"   Each client will only load its own data subset during training")

        return data_dict

    def _split_original(self, dataset):
        """
        åŸå§‹æ¨¡å¼çš„åˆ†å‰²é€»è¾‘ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰ã€‚

        Args:
            dataset: LEAF dataset dict with format:
                {client_id: {'train': data, 'test': data, 'val': data}}

        Returns:
            dict: dict of ClientData with client_idx as key
        """
        logger.info(f"ğŸ”„ MergedLeafTranslator: Merging {len(dataset)} LEAF users "
                   f"before re-splitting into {self.global_cfg.federate.client_num} clients")

        # 1. Collect all data from LEAF users
        all_train_datasets = []
        all_val_datasets = []
        all_test_datasets = []

        for client_id, client_data in dataset.items():
            if 'train' in client_data and len(client_data['train']) > 0:
                all_train_datasets.append(client_data['train'])
            if 'val' in client_data and len(client_data['val']) > 0:
                all_val_datasets.append(client_data['val'])
            if 'test' in client_data and len(client_data['test']) > 0:
                all_test_datasets.append(client_data['test'])

        # 2. Merge into single datasets using ConcatDataset
        # ConcatDataset preserves the original dataset structure while concatenating
        merged_train = ConcatDataset(all_train_datasets) if all_train_datasets else []
        merged_val = ConcatDataset(all_val_datasets) if all_val_datasets else []
        merged_test = ConcatDataset(all_test_datasets) if all_test_datasets else []

        logger.info(f"ğŸ“Š Merged dataset sizes - Train: {len(merged_train)}, "
                   f"Val: {len(merged_val)}, Test: {len(merged_test)}")

        # 3. Use BaseDataTranslator's split_to_client to re-distribute
        # This will use the configured splitter (LDA/IID) to create new client splits
        datadict = self.split_to_client(merged_train, merged_val, merged_test)

        logger.info(f"âœ… Re-distributed into {len(datadict)-1} clients using "
                   f"'{self.global_cfg.data.splitter}' splitter")

        return datadict
