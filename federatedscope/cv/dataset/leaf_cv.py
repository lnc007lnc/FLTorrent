import os
import random
import json
import torch
import math
import logging

import numpy as np
import os.path as osp

from PIL import Image
from tqdm import tqdm

from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset

from federatedscope.core.data.utils import save_local_data, download_url
from federatedscope.cv.dataset.leaf import LEAF

logger = logging.getLogger(__name__)

IMAGE_SIZE = {'femnist': (28, 28), 'celeba': (84, 84, 3)}
MODE = {'femnist': 'L', 'celeba': 'RGB'}


class LazyLoadDataset(Dataset):
    """
    å»¶è¿ŸåŠ è½½æ•°æ®é›†ï¼Œç”¨äºè§£å†³åˆ†å¸ƒå¼ FL ä¸­å†…å­˜å³°å€¼é—®é¢˜ã€‚

    ä¸¤é˜¶æ®µåŠ è½½ç­–ç•¥ï¼š
    1. åˆå§‹é˜¶æ®µï¼ˆlazy=Trueï¼‰ï¼šåªå­˜å‚¨æ ‡ç­¾å’Œç´¢å¼•ï¼Œç”¨äº LDA åˆ†å‰²ï¼ˆ~1MBï¼‰
    2. åˆ†å‰²åï¼ˆlazy=Falseï¼‰ï¼šé¢„åŠ è½½è¯¥ client çš„æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼ˆåªæœ‰ 1/N æ•°æ®ï¼‰

    è¿™æ ·æ—¢é¿å…äº†åˆ†å‰²é˜¶æ®µçš„å†…å­˜å³°å€¼ï¼Œåˆä¿è¯äº†è®­ç»ƒé˜¶æ®µçš„é€Ÿåº¦ã€‚
    """
    def __init__(self, task_indices, processed_dir, name, transform=None, target_transform=None, lazy=True):
        """
        Args:
            task_indices: List of (task_id, local_idx, label, split_type) å…ƒç»„
            processed_dir: processed æ•°æ®ç›®å½•è·¯å¾„
            name: æ•°æ®é›†åç§° ('femnist' or 'celeba')
            transform: å›¾åƒå˜æ¢
            target_transform: æ ‡ç­¾å˜æ¢
            lazy: æ˜¯å¦å»¶è¿ŸåŠ è½½ã€‚True=åªå­˜ç´¢å¼•ï¼ŒFalse=ç«‹å³é¢„åŠ è½½æ•°æ®
        """
        self.task_indices = task_indices  # [(task_id, local_idx, label, split_type), ...]
        self.processed_dir = processed_dir
        self.name = name
        self.transform = transform
        self.target_transform = target_transform

        # ç¼“å­˜å·²åŠ è½½çš„ task æ•°æ®
        self._task_cache = {}

        # å¦‚æœä¸æ˜¯å»¶è¿Ÿæ¨¡å¼ï¼Œç«‹å³é¢„åŠ è½½æ‰€æœ‰æ•°æ®
        if not lazy and task_indices:
            self._preload()

    def __len__(self):
        return len(self.task_indices)

    def __getitem__(self, idx):
        task_id, local_idx, label, split_type = self.task_indices[idx]

        # å»¶è¿ŸåŠ è½½ï¼šåªåœ¨éœ€è¦æ—¶æ‰åŠ è½½è¯¥ task çš„æ•°æ®
        cache_key = (task_id, split_type)
        if cache_key not in self._task_cache:
            task_path = osp.join(self.processed_dir, f"task_{task_id}", f"{split_type}.pt")
            data, targets = torch.load(task_path, weights_only=False)
            self._task_cache[cache_key] = (data, targets)

        data, targets = self._task_cache[cache_key]
        img_data = data[local_idx]
        target = targets[local_idx].item() if hasattr(targets[local_idx], 'item') else label

        # è½¬æ¢ä¸ºå›¾åƒ
        img = np.resize(img_data.numpy().astype(np.uint8), IMAGE_SIZE[self.name])
        img = Image.fromarray(img, mode=MODE[self.name])

        if self.transform is not None:
            img = self.transform(img)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return img, target

    def get_labels(self):
        """è¿”å›æ‰€æœ‰æ ‡ç­¾ï¼Œç”¨äº LDA åˆ†å‰²"""
        return [item[2] for item in self.task_indices]

    def subset(self, indices):
        """
        åˆ›å»ºå­é›†ï¼ŒåŒ…å«æŒ‡å®šç´¢å¼•çš„æ•°æ®ã€‚
        å­é›†åˆ›å»ºæ—¶ä¼šç«‹å³é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜ï¼ˆå› ä¸ºæ­¤æ—¶å·²å®Œæˆåˆ†å‰²ï¼Œæ•°æ®é‡æ˜¯ 1/Nï¼‰ã€‚
        """
        new_task_indices = [self.task_indices[i] for i in indices]
        # åˆ†å‰²åçš„å­é›†ä½¿ç”¨ lazy=Falseï¼Œç«‹å³é¢„åŠ è½½æ•°æ®
        return LazyLoadDataset(
            new_task_indices,
            self.processed_dir,
            self.name,
            self.transform,
            self.target_transform,
            lazy=False  # åˆ†å‰²å®Œæˆåç«‹å³åŠ è½½åˆ°å†…å­˜
        )

    def _preload(self):
        """é¢„åŠ è½½è¯¥æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®åˆ°å†…å­˜"""
        # æ”¶é›†éœ€è¦åŠ è½½çš„ task æ–‡ä»¶
        tasks_to_load = set()
        for task_id, local_idx, label, split_type in self.task_indices:
            tasks_to_load.add((task_id, split_type))

        logger.info(f"ğŸ“¥ Preloading {len(tasks_to_load)} task files ({len(self.task_indices)} samples)...")

        # ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰éœ€è¦çš„ task æ•°æ®
        for task_id, split_type in tasks_to_load:
            cache_key = (task_id, split_type)
            if cache_key not in self._task_cache:
                task_path = osp.join(self.processed_dir, f"task_{task_id}", f"{split_type}.pt")
                data, targets = torch.load(task_path, weights_only=False)
                self._task_cache[cache_key] = (data, targets)

        logger.info(f"âœ… Preload complete. Ready for fast training.")

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜ä»¥é‡Šæ”¾å†…å­˜"""
        self._task_cache.clear()


class LEAF_CV(LEAF):
    """
    LEAF CV dataset from "LEAF: A Benchmark for Federated Settings"

    leaf.cmu.edu

    Arguments:
        root (str): root path.
        name (str): name of dataset, â€˜femnistâ€™ or â€˜celebaâ€™.
        s_frac (float): fraction of the dataset to be used; default=0.3.
        tr_frac (float): train set proportion for each task; default=0.8.
        val_frac (float): valid set proportion for each task; default=0.0.
        train_tasks_frac (float): fraction of test tasks; default=1.0.
        transform: transform for x.
        target_transform: transform for y.

    """
    def __init__(self,
                 root,
                 name,
                 s_frac=0.3,
                 tr_frac=0.8,
                 val_frac=0.0,
                 train_tasks_frac=1.0,
                 seed=123,
                 transform=None,
                 target_transform=None):
        self.s_frac = s_frac
        self.tr_frac = tr_frac
        self.val_frac = val_frac
        self.seed = seed
        self.train_tasks_frac = train_tasks_frac
        super(LEAF_CV, self).__init__(root, name, transform, target_transform)
        files = os.listdir(self.processed_dir)
        files = [f for f in files if f.startswith('task_')]
        if len(files):
            # Sort by idx
            files.sort(key=lambda k: int(k[5:]))

            for file in files:
                train_data, train_targets = torch.load(
                    osp.join(self.processed_dir, file, 'train.pt'))
                test_data, test_targets = torch.load(
                    osp.join(self.processed_dir, file, 'test.pt'))
                self.data_dict[int(file[5:])] = {
                    'train': (train_data, train_targets),
                    'test': (test_data, test_targets)
                }
                if osp.exists(osp.join(self.processed_dir, file, 'val.pt')):
                    val_data, val_targets = torch.load(
                        osp.join(self.processed_dir, file, 'val.pt'))
                    self.data_dict[int(file[5:])]['val'] = (val_data,
                                                            val_targets)
        else:
            raise RuntimeError(
                'Please delete â€˜processedâ€™ folder and try again!')

    @property
    def raw_file_names(self):
        names = [f'{self.name}_all_data.zip']
        return names

    def download(self):
        # Download to `self.raw_dir`.
        url = 'https://federatedscope.oss-cn-beijing.aliyuncs.com'
        os.makedirs(self.raw_dir, exist_ok=True)
        for name in self.raw_file_names:
            download_url(f'{url}/{name}', self.raw_dir)

    def __getitem__(self, index):
        """
        Arguments:
            index (int): Index

        :returns:
            dict: {'train':[(image, target)],
                   'test':[(image, target)],
                   'val':[(image, target)]}
            where target is the target class.
        """
        img_dict = {}
        data = self.data_dict[index]
        for key in data:
            img_dict[key] = []
            imgs, targets = data[key]
            for idx in range(targets.shape[0]):
                img = np.resize(imgs[idx].numpy().astype(np.uint8),
                                IMAGE_SIZE[self.name])
                img = Image.fromarray(img, mode=MODE[self.name])
                if self.transform is not None:
                    img = self.transform(img)

                if self.target_transform is not None:
                    targets[idx] = self.target_transform(targets[idx])

                img_dict[key].append((img, targets[idx]))

        return img_dict

    def process(self):
        raw_path = osp.join(self.raw_dir, "all_data")
        files = os.listdir(raw_path)
        files = [f for f in files if f.endswith('.json')]

        n_tasks = math.ceil(len(files) * self.s_frac)
        random.shuffle(files)
        files = files[:n_tasks]

        print("Preprocess data (Please leave enough space)...")

        idx = 0
        for num, file in enumerate(tqdm(files)):

            with open(osp.join(raw_path, file), 'r') as f:
                raw_data = json.load(f)

            # Numpy to Tensor
            for writer, v in raw_data['user_data'].items():
                data, targets = v['x'], v['y']

                if len(v['x']) > 2:
                    data = torch.tensor(np.stack(data))
                    targets = torch.LongTensor(np.stack(targets))
                else:
                    data = torch.tensor(data)
                    targets = torch.LongTensor(targets)

                train_data, test_data, train_targets, test_targets =\
                    train_test_split(
                        data,
                        targets,
                        train_size=self.tr_frac,
                        random_state=self.seed
                    )

                if self.val_frac > 0:
                    val_data, test_data, val_targets, test_targets = \
                        train_test_split(
                            test_data,
                            test_targets,
                            train_size=self.val_frac / (1.-self.tr_frac),
                            random_state=self.seed
                        )

                else:
                    val_data, val_targets = None, None
                save_path = osp.join(self.processed_dir, f"task_{idx}")
                os.makedirs(save_path, exist_ok=True)

                save_local_data(dir_path=save_path,
                                train_data=train_data,
                                train_targets=train_targets,
                                test_data=test_data,
                                test_targets=test_targets,
                                val_data=val_data,
                                val_targets=val_targets)
                idx += 1


def load_leaf_cv_lazy(root, name, transform=None, target_transform=None):
    """
    åˆ›å»ºå»¶è¿ŸåŠ è½½çš„ LEAF CV æ•°æ®é›†ã€‚

    åªåŠ è½½æ ‡ç­¾ä¿¡æ¯åˆ°å†…å­˜ï¼Œå›¾åƒæ•°æ®åœ¨å®é™…è®¿é—®æ—¶æ‰åŠ è½½ã€‚
    å†…å­˜ä½¿ç”¨ï¼šä» ~9GB (å…¨éƒ¨å›¾åƒ) é™åˆ° ~1MB (åªæœ‰æ ‡ç­¾å’Œç´¢å¼•)

    Args:
        root: æ•°æ®æ ¹ç›®å½•
        name: æ•°æ®é›†åç§° ('femnist' or 'celeba')
        transform: å›¾åƒå˜æ¢
        target_transform: æ ‡ç­¾å˜æ¢

    Returns:
        dict: {'train': LazyLoadDataset, 'val': LazyLoadDataset, 'test': LazyLoadDataset}
    """
    processed_dir = osp.join(root, name, 'processed')

    if not osp.exists(processed_dir):
        raise RuntimeError(f"Processed directory not found: {processed_dir}. "
                           "Please run data preprocessing first.")

    files = os.listdir(processed_dir)
    files = [f for f in files if f.startswith('task_')]
    files.sort(key=lambda k: int(k[5:]))

    logger.info(f"ğŸ“‚ Loading labels from {len(files)} tasks (lazy mode)...")

    train_indices = []
    val_indices = []
    test_indices = []

    # åªåŠ è½½æ ‡ç­¾ï¼Œä¸åŠ è½½å›¾åƒæ•°æ®
    # ä½¿ç”¨ mmap æ¨¡å¼é¿å…å°†æ•´ä¸ªæ–‡ä»¶åŠ è½½åˆ°å†…å­˜
    for file in tqdm(files, desc="Loading labels only"):
        task_id = int(file[5:])
        task_path = osp.join(processed_dir, file)

        # åŠ è½½ train æ ‡ç­¾ï¼ˆä½¿ç”¨ mmap é¿å…åŠ è½½å›¾åƒæ•°æ®åˆ°å†…å­˜ï¼‰
        train_path = osp.join(task_path, 'train.pt')
        if osp.exists(train_path):
            loaded = torch.load(train_path, weights_only=False, mmap=True)
            train_targets = loaded[1]  # åªå– targets
            for local_idx, label in enumerate(train_targets.tolist()):
                train_indices.append((task_id, local_idx, label, 'train'))
            del loaded  # æ˜¾å¼é‡Šæ”¾

        # åŠ è½½ val æ ‡ç­¾
        val_path = osp.join(task_path, 'val.pt')
        if osp.exists(val_path):
            loaded = torch.load(val_path, weights_only=False, mmap=True)
            val_targets = loaded[1]
            for local_idx, label in enumerate(val_targets.tolist()):
                val_indices.append((task_id, local_idx, label, 'val'))
            del loaded

        # åŠ è½½ test æ ‡ç­¾
        test_path = osp.join(task_path, 'test.pt')
        if osp.exists(test_path):
            loaded = torch.load(test_path, weights_only=False, mmap=True)
            test_targets = loaded[1]
            for local_idx, label in enumerate(test_targets.tolist()):
                test_indices.append((task_id, local_idx, label, 'test'))
            del loaded

    logger.info(f"âœ… Labels loaded - Train: {len(train_indices)}, "
                f"Val: {len(val_indices)}, Test: {len(test_indices)}")

    # åˆ›å»ºå»¶è¿ŸåŠ è½½æ•°æ®é›†
    result = {
        'train': LazyLoadDataset(train_indices, processed_dir, name, transform, target_transform),
        'val': LazyLoadDataset(val_indices, processed_dir, name, transform, target_transform),
        'test': LazyLoadDataset(test_indices, processed_dir, name, transform, target_transform),
    }

    return result
