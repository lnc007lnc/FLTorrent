#!/usr/bin/env python3
"""
æœ€ç»ˆchunkæ¢å¤åŠŸèƒ½æ¼”ç¤º
å±•ç¤ºå®Œæ•´çš„æ¨¡å‹åˆ†å—ã€å­˜å‚¨ã€æ¢å¤æµç¨‹
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from federatedscope.core.chunk_manager import ChunkManager


class DemoModel(nn.Module):
    """æ¼”ç¤ºç”¨çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
    def __init__(self):
        super(DemoModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


def demonstrate_chunk_recovery():
    """æ¼”ç¤ºå®Œæ•´çš„chunkæ¢å¤æµç¨‹"""
    print("ğŸ¯ Chunkæ¢å¤åŠŸèƒ½å®Œæ•´æ¼”ç¤º")
    print("=" * 80)
    
    # æ­¥éª¤1: åˆ›å»ºåŸå§‹æ¨¡å‹
    print("ğŸ“ æ­¥éª¤1: åˆ›å»ºå¹¶åˆå§‹åŒ–åŸå§‹æ¨¡å‹")
    original_model = DemoModel()
    torch.manual_seed(42)
    with torch.no_grad():
        for param in original_model.parameters():
            param.data.normal_(0, 0.1)
    
    # è®¡ç®—æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
    total_params = sum(p.numel() for p in original_model.parameters())
    trainable_params = sum(p.numel() for p in original_model.parameters() if p.requires_grad)
    
    print(f"   æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024:.1f} KB (float32)")
    
    # æ­¥éª¤2: åˆ†å—å’Œä¿å­˜
    print(f"\nğŸ“¦ æ­¥éª¤2: å°†æ¨¡å‹åˆ†å—å¹¶ä¿å­˜åˆ°æ•°æ®åº“")
    chunk_manager = ChunkManager(client_id=999)
    
    num_chunks = 8
    saved_hashes = chunk_manager.save_model_chunks(
        model=original_model,
        round_num=1,
        num_chunks=num_chunks
    )
    
    print(f"   âœ… æˆåŠŸä¿å­˜ {len(saved_hashes)} ä¸ªchunks")
    
    # æ˜¾ç¤ºchunkä¿¡æ¯
    params = ChunkManager.model_to_params(original_model)
    chunk_infos = ChunkManager.split_model(params, num_chunks)
    
    print(f"   ğŸ“Š Chunkåˆ†å¸ƒ:")
    for i, chunk_info in enumerate(chunk_infos):
        print(f"     Chunk {i}: {chunk_info['flat_size']:,} ä¸ªå…ƒç´ ")
        param_names = list(chunk_info['parts'].keys())
        print(f"       åŒ…å«å‚æ•°: {', '.join(param_names[:3])}{'...' if len(param_names) > 3 else ''}")
    
    # æ­¥éª¤3: æ¨¡æ‹Ÿä»æ•°æ®åº“æ¢å¤
    print(f"\nğŸ”„ æ­¥éª¤3: ä»æ•°æ®åº“chunksæ¢å¤æ¨¡å‹")
    
    # åˆ›å»ºæ–°çš„ç©ºæ¨¡å‹
    recovered_model = DemoModel()
    
    # ä»chunksæ¢å¤
    success = chunk_manager.reconstruct_model_from_chunks(
        round_num=1,
        target_model=recovered_model
    )
    
    if not success:
        print("âŒ æ¨¡å‹æ¢å¤å¤±è´¥!")
        return False
    
    print("   âœ… æ¨¡å‹æ¢å¤æˆåŠŸ!")
    
    # æ­¥éª¤4: éªŒè¯æ¢å¤å‡†ç¡®æ€§
    print(f"\nğŸ” æ­¥éª¤4: éªŒè¯æ¢å¤å‡†ç¡®æ€§")
    
    # æ¯”è¾ƒæ‰€æœ‰å¯è®­ç»ƒå‚æ•°
    original_state = original_model.state_dict()
    recovered_state = recovered_model.state_dict()
    
    perfect_match = True
    param_differences = []
    
    for name in original_state:
        if original_state[name].requires_grad:  # åªæ£€æŸ¥å¯è®­ç»ƒå‚æ•°
            orig_param = original_state[name].cpu().numpy()
            rec_param = recovered_state[name].cpu().numpy()
            
            max_diff = np.abs(orig_param - rec_param).max()
            mean_diff = np.abs(orig_param - rec_param).mean()
            
            param_differences.append((name, max_diff, mean_diff))
            
            if max_diff > 1e-6:
                perfect_match = False
    
    if perfect_match:
        print("   âœ… æ‰€æœ‰å¯è®­ç»ƒå‚æ•°å®Œç¾åŒ¹é…!")
        for name, max_diff, mean_diff in param_differences[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"     {name}: max_diff={max_diff:.2e}, mean_diff={mean_diff:.2e}")
    else:
        print("   âš ï¸ å‘ç°å‚æ•°å·®å¼‚:")
        for name, max_diff, mean_diff in param_differences:
            if max_diff > 1e-6:
                print(f"     {name}: max_diff={max_diff:.2e}")
    
    # æ­¥éª¤5: åŠŸèƒ½éªŒè¯
    print(f"\nğŸ§ª æ­¥éª¤5: æ¨¡å‹åŠŸèƒ½éªŒè¯")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.randn(5, 1, 28, 28)
    
    # è®¡ç®—åŸå§‹æ¨¡å‹è¾“å‡º
    original_model.eval()
    recovered_model.eval()
    
    with torch.no_grad():
        original_output = original_model(test_input)
        recovered_output = recovered_model(test_input)
    
    # æ¯”è¾ƒè¾“å‡º
    output_diff = torch.abs(original_output - recovered_output).max().item()
    
    if output_diff < 1e-4:
        print(f"   âœ… æ¨¡å‹è¾“å‡ºä¸€è‡´ (max_diff: {output_diff:.2e})")
        
        # æ˜¾ç¤ºä¸€äº›å…·ä½“çš„è¾“å‡ºå€¼
        print(f"   ğŸ“Š è¾“å‡ºæ ·æœ¬å¯¹æ¯”:")
        for i in range(min(3, test_input.size(0))):
            orig_pred = torch.argmax(original_output[i]).item()
            rec_pred = torch.argmax(recovered_output[i]).item()
            print(f"     æ ·æœ¬ {i}: åŸå§‹é¢„æµ‹={orig_pred}, æ¢å¤é¢„æµ‹={rec_pred}")
    else:
        print(f"   âš ï¸ æ¨¡å‹è¾“å‡ºæœ‰å·®å¼‚ (max_diff: {output_diff:.2e})")
    
    # æ­¥éª¤6: å­˜å‚¨ç»Ÿè®¡
    print(f"\nğŸ“Š æ­¥éª¤6: å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯")
    stats = chunk_manager.get_storage_stats()
    
    print(f"   æ•°æ®åº“è·¯å¾„: {stats.get('db_path', 'N/A')}")
    print(f"   å­˜å‚¨çš„chunkæ•°: {stats.get('unique_chunks', 0)}")
    print(f"   æ•°æ®åº“å¤§å°: {stats.get('storage_size_mb', 0):.2f} MB")
    print(f"   å‹ç¼©ç‡: {(total_params * 4 / 1024 / 1024) / max(stats.get('storage_size_mb', 1), 0.001):.1f}x")
    
    # æ­¥éª¤7: æ¸…ç†éªŒè¯
    print(f"\nğŸ§¹ æ­¥éª¤7: æµ‹è¯•æ¸…ç†åŠŸèƒ½")
    
    # ä¿å­˜å¤šè½®æ•°æ®
    for round_num in range(2, 6):
        # ç¨å¾®ä¿®æ”¹æ¨¡å‹
        with torch.no_grad():
            for param in original_model.parameters():
                param.data += torch.randn_like(param.data) * 0.01
        
        chunk_manager.save_model_chunks(original_model, round_num, num_chunks)
    
    print(f"   ä¿å­˜äº†é¢å¤–4è½®æ•°æ®")
    
    # æ¸…ç†æ—§æ•°æ®
    chunk_manager.cleanup_old_rounds(keep_rounds=2)
    print(f"   æ¸…ç†å®Œæˆï¼Œä¿ç•™æœ€è¿‘2è½®")
    
    # éªŒè¯æ¸…ç†ç»“æœ
    final_stats = chunk_manager.get_storage_stats()
    print(f"   æ¸…ç†åchunkæ•°: {final_stats.get('unique_chunks', 0)}")
    print(f"   è½®æ¬¡èŒƒå›´: {final_stats.get('round_range', (None, None))}")
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ! Chunkæ¢å¤ç³»ç»Ÿå®Œå…¨æ­£å¸¸å·¥ä½œ!")
    return True


def main():
    """è¿è¡Œæ¼”ç¤º"""
    try:
        success = demonstrate_chunk_recovery()
        if success:
            print(f"\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! ")
            print(f"ğŸ“‹ æ€»ç»“:")
            print(f"   1. âœ… æ¨¡å‹å¯ä»¥æˆåŠŸåˆ†å‰²ä¸ºchunkså¹¶ä¿å­˜åˆ°SQLiteæ•°æ®åº“")
            print(f"   2. âœ… æ‰€æœ‰å¯è®­ç»ƒå‚æ•°å¯ä»¥å®Œç¾æ¢å¤")
            print(f"   3. âœ… æ¢å¤çš„æ¨¡å‹åŠŸèƒ½å®Œå…¨æ­£å¸¸")
            print(f"   4. âœ… æ•°æ®åº“æ”¯æŒå¤šè½®å­˜å‚¨å’Œæ¸…ç†")
            print(f"   5. âœ… å­˜å‚¨æ ¼å¼ä¸º (client_id, round_num, chunk_id) -> chunk_data")
            return True
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥!")
            return False
    except Exception as e:
        print(f"\nğŸ’¥ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)