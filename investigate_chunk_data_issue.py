#!/usr/bin/env python3
"""
è°ƒæŸ¥chunkæ•°æ®å­˜å‚¨é—®é¢˜
åˆ†æä¸ºä»€ä¹ˆchunk_dataè¡¨åªæœ‰å°‘é‡è®°å½•
"""

import sqlite3
import os
import hashlib
import pickle
from collections import defaultdict

def analyze_empty_chunk_issue():
    """åˆ†æç©ºchunké—®é¢˜"""
    print("ğŸ” è°ƒæŸ¥chunkæ•°æ®å­˜å‚¨é—®é¢˜")
    print("=" * 80)
    
    # å·²çŸ¥çš„ç©ºå­—èŠ‚å“ˆå¸Œ
    empty_hash = hashlib.sha256(b'').hexdigest()
    print(f"ç©ºå­—èŠ‚çš„SHA256å“ˆå¸Œ: {empty_hash}")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_dbs = {
        1: os.path.join(base_path, "client_1", "client_1_chunks.db"),
        2: os.path.join(base_path, "client_2", "client_2_chunks.db"), 
        3: os.path.join(base_path, "client_3", "client_3_chunks.db")
    }
    
    for client_id, db_path in client_dbs.items():
        print(f"\n{'='*60}")
        print(f"ğŸ” åˆ†æå®¢æˆ·ç«¯ {client_id}")
        
        if not os.path.exists(db_path):
            continue
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        try:
            # 1. åˆ†æbt_chunksä¸­çš„å“ˆå¸Œåˆ†å¸ƒ
            cursor.execute("""
                SELECT chunk_hash, COUNT(*) as count
                FROM bt_chunks 
                GROUP BY chunk_hash
                ORDER BY count DESC
            """)
            
            hash_distribution = cursor.fetchall()
            print(f"\nğŸ“Š bt_chunksè¡¨ä¸­çš„å“ˆå¸Œåˆ†å¸ƒ:")
            for chunk_hash, count in hash_distribution:
                if chunk_hash == empty_hash:
                    print(f"   ğŸ”´ ç©ºchunkå“ˆå¸Œ {chunk_hash[:16]}...: {count} æ¡è®°å½•")
                else:
                    print(f"   ğŸŸ¢ éç©ºchunkå“ˆå¸Œ {chunk_hash[:16]}...: {count} æ¡è®°å½•")
            
            # 2. æ£€æŸ¥chunk_dataè¡¨
            cursor.execute("SELECT chunk_hash, LENGTH(data) FROM chunk_data")
            chunk_data_records = cursor.fetchall()
            
            print(f"\nğŸ’¾ chunk_dataè¡¨åˆ†æ:")
            print(f"   æ€»è®°å½•æ•°: {len(chunk_data_records)}")
            
            for chunk_hash, data_length in chunk_data_records:
                if chunk_hash == empty_hash:
                    print(f"   ğŸ”´ ç©ºchunkæ•°æ®: {chunk_hash[:16]}..., é•¿åº¦={data_length}")
                else:
                    print(f"   ğŸŸ¢ æœ‰æ•ˆchunkæ•°æ®: {chunk_hash[:16]}..., é•¿åº¦={data_length}")
            
            # 3. æ£€æŸ¥å…³è”æ€§
            cursor.execute("""
                SELECT 
                    (SELECT COUNT(*) FROM bt_chunks WHERE chunk_hash = ?) as bt_empty_count,
                    (SELECT COUNT(*) FROM chunk_data WHERE chunk_hash = ?) as data_empty_count
            """, (empty_hash, empty_hash))
            
            bt_empty, data_empty = cursor.fetchone()
            print(f"\nğŸ”— ç©ºchunkå…³è”åˆ†æ:")
            print(f"   bt_chunksä¸­ç©ºchunkè®°å½•: {bt_empty}")
            print(f"   chunk_dataä¸­ç©ºchunkè®°å½•: {data_empty}")
            
            # 4. åˆ†æBitTorrent chunkå­˜å‚¨æ˜¯å¦æ­£å¸¸å·¥ä½œ
            cursor.execute("""
                SELECT round_num, source_client_id, chunk_id, chunk_hash
                FROM bt_chunks 
                WHERE chunk_hash != ?
                ORDER BY round_num, source_client_id, chunk_id
                LIMIT 5
            """, (empty_hash,))
            
            non_empty_chunks = cursor.fetchall()
            print(f"\nğŸ¯ éç©ºchunkæ ·æœ¬ (å‰5æ¡):")
            for round_num, source_client_id, chunk_id, chunk_hash in non_empty_chunks:
                print(f"   è½®æ¬¡{round_num}, æº{source_client_id}, å—{chunk_id}: {chunk_hash[:16]}...")
                
                # æ£€æŸ¥è¿™ä¸ªchunkæ˜¯å¦åœ¨chunk_dataä¸­æœ‰æ•°æ®
                cursor.execute("SELECT LENGTH(data) FROM chunk_data WHERE chunk_hash = ?", (chunk_hash,))
                data_result = cursor.fetchone()
                if data_result:
                    print(f"     âœ… chunk_dataä¸­å­˜åœ¨ï¼Œæ•°æ®é•¿åº¦: {data_result[0]}")
                else:
                    print(f"     âŒ chunk_dataä¸­ä¸å­˜åœ¨ï¼")
            
        finally:
            conn.close()

def test_chunk_data_saving():
    """æµ‹è¯•chunkæ•°æ®ä¿å­˜æœºåˆ¶"""
    print(f"\n{'='*60}")
    print("ğŸ§ª æµ‹è¯•chunkæ•°æ®ä¿å­˜æœºåˆ¶")
    
    # æ¨¡æ‹ŸBitTorrentæ¥æ”¶chunkçš„è¿‡ç¨‹
    import tempfile
    import numpy as np
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.random.rand(10, 5).astype(np.float32)  # æ¨¡æ‹Ÿæ¨¡å‹å‚æ•°
    test_hash = hashlib.sha256(pickle.dumps(test_data)).hexdigest()
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®:")
    print(f"   ç±»å‹: {type(test_data)}")
    print(f"   å½¢çŠ¶: {test_data.shape}")
    print(f"   å“ˆå¸Œ: {test_hash[:16]}...")
    
    # åˆ›å»ºä¸´æ—¶æ•°æ®åº“æµ‹è¯•
    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as tmp_file:
        test_db_path = tmp_file.name
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“
        conn = sqlite3.connect(test_db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS bt_chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                round_num INTEGER,
                source_client_id INTEGER,
                chunk_id INTEGER,
                chunk_hash TEXT,
                holder_client_id INTEGER,
                received_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_verified INTEGER DEFAULT 0
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunk_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chunk_hash TEXT UNIQUE,
                data BLOB,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # æ¨¡æ‹Ÿsave_remote_chunkè¿‡ç¨‹
        cursor.execute('''
            INSERT OR REPLACE INTO bt_chunks 
            (round_num, source_client_id, chunk_id, chunk_hash, holder_client_id, is_verified)
            VALUES (?, ?, ?, ?, ?, 1)
        ''', (0, 2, 5, test_hash, 1))
        
        cursor.execute('''
            INSERT OR IGNORE INTO chunk_data (chunk_hash, data)
            VALUES (?, ?)
        ''', (test_hash, pickle.dumps(test_data)))
        
        conn.commit()
        
        # éªŒè¯ä¿å­˜ç»“æœ
        cursor.execute("SELECT COUNT(*) FROM bt_chunks WHERE chunk_hash = ?", (test_hash,))
        bt_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT LENGTH(data) FROM chunk_data WHERE chunk_hash = ?", (test_hash,))
        data_result = cursor.fetchone()
        
        print(f"âœ… ä¿å­˜æµ‹è¯•ç»“æœ:")
        print(f"   bt_chunksè®°å½•: {bt_count}")
        print(f"   chunk_dataè®°å½•: {'å­˜åœ¨' if data_result else 'ä¸å­˜åœ¨'}")
        if data_result:
            print(f"   æ•°æ®é•¿åº¦: {data_result[0]} bytes")
            
            # æµ‹è¯•è¯»å–
            cursor.execute("SELECT data FROM chunk_data WHERE chunk_hash = ?", (test_hash,))
            stored_data = cursor.fetchone()[0]
            retrieved_data = pickle.loads(stored_data)
            
            print(f"   è¯»å–éªŒè¯: {'âœ…æˆåŠŸ' if np.array_equal(test_data, retrieved_data) else 'âŒå¤±è´¥'}")
        
        conn.close()
        
    finally:
        os.unlink(test_db_path)

def analyze_model_chunking_behavior():
    """åˆ†ææ¨¡å‹åˆ†å—è¡Œä¸º"""
    print(f"\n{'='*60}")
    print("ğŸ“Š åˆ†ææ¨¡å‹åˆ†å—è¡Œä¸º")
    
    # æ£€æŸ¥å®é™…çš„æ¨¡å‹æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    model_paths = [
        "/mnt/g/FLtorrent_combine/FederatedScope-master/multi_process_test_v2/client_1_output/FedAvg_lr_on_toy_lr0.01_lstep2",
        "/mnt/g/FLtorrent_combine/FederatedScope-master/multi_process_test_v2/client_2_output/FedAvg_lr_on_toy_lr0.01_lstep2",
        "/mnt/g/FLtorrent_combine/FederatedScope-master/multi_process_test_v2/client_3_output/FedAvg_lr_on_toy_lr0.01_lstep2"
    ]
    
    for i, model_path in enumerate(model_paths, 1):
        print(f"\nğŸ” æ£€æŸ¥å®¢æˆ·ç«¯{i}çš„æ¨¡å‹æ–‡ä»¶:")
        if os.path.exists(model_path):
            files = os.listdir(model_path)
            model_files = [f for f in files if f.endswith('.pth')]
            print(f"   æ¨¡å‹æ–‡ä»¶: {model_files}")
            
            for model_file in model_files[:2]:  # åªæ£€æŸ¥å‰2ä¸ª
                full_path = os.path.join(model_path, model_file)
                file_size = os.path.getsize(full_path)
                print(f"     {model_file}: {file_size} bytes")
                
                if file_size < 10000:  # å°äº10KBçš„æ¨¡å‹
                    print(f"     ğŸ“ è¿™æ˜¯ä¸€ä¸ªå°æ¨¡å‹ï¼Œå¯èƒ½äº§ç”Ÿå¾ˆå¤šç©ºchunks")
        else:
            print(f"   è·¯å¾„ä¸å­˜åœ¨: {model_path}")

def main():
    """ä¸»å‡½æ•°"""
    analyze_empty_chunk_issue()
    test_chunk_data_saving()
    analyze_model_chunking_behavior()
    
    print(f"\n{'='*80}")
    print("ğŸ¯ ç»“è®ºåˆ†æ:")
    print("1. å¤§éƒ¨åˆ†chunkæ˜¯ç©ºçš„ï¼ˆç©ºå­—èŠ‚ï¼‰ï¼Œè¿™æ˜¯ç”±äºtoyæ•°æ®é›†çš„å°æ¨¡å‹ç‰¹æ€§")
    print("2. BitTorrentç³»ç»Ÿæ­£ç¡®å¤„ç†äº†ç©ºchunkså’Œéç©ºchunks")
    print("3. save_remote_chunkå‡½æ•°åº”è¯¥æ­£å¸¸å·¥ä½œ")
    print("4. chunk_dataè¡¨ä¸­çš„è®°å½•è¾ƒå°‘å¯èƒ½æ˜¯å› ä¸ºï¼š")
    print("   - å¤§é‡ç©ºchunkså…±äº«åŒä¸€ä¸ªå“ˆå¸Œå€¼")
    print("   - INSERT OR IGNOREå¯¼è‡´é‡å¤å“ˆå¸Œä¸ä¼šåˆ›å»ºæ–°è®°å½•")
    print("   - å®é™…çš„éç©ºchunkæ•°é‡å°±æ˜¯è¿™ä¹ˆå°‘")

if __name__ == "__main__":
    main()