#!/usr/bin/env python3
"""
æµ‹è¯•å®é™…å®¢æˆ·ç«¯ä¸­çš„æ¨¡å‹ä¿å­˜åŠŸèƒ½
ä½¿ç”¨çœŸå®çš„FederatedScopeç»„ä»¶
"""

import sys
import os
sys.path.insert(0, os.getcwd())

# Set protobuf implementation
os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'

import time
import shutil
import logging
from federatedscope.core.configs.config import global_cfg

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_client_model_saving():
    """æµ‹è¯•å®¢æˆ·ç«¯æ¨¡å‹ä¿å­˜åŠŸèƒ½"""
    
    print("ğŸ§ª Testing Real Client Model Saving")
    print("=" * 60)
    
    try:
        # æ¸…ç†ä¹‹å‰çš„æµ‹è¯•æ–‡ä»¶
        test_tmp_dir = os.path.join(os.getcwd(), "tmp")
        if os.path.exists(test_tmp_dir):
            shutil.rmtree(test_tmp_dir)
            print("ğŸ§¹ Cleaned previous test files")
        
        # åˆ›å»ºé…ç½®
        from federatedscope.core.configs.config import global_cfg
        cfg = global_cfg.clone()
        
        # è®¾ç½®åŸºæœ¬é…ç½®
        cfg.use_gpu = False
        cfg.backend = 'torch'
        cfg.seed = 2023
        cfg.federate.client_num = 2
        cfg.federate.mode = 'standalone'
        cfg.federate.total_round_num = 2
        cfg.federate.sample_client_num = 2
        cfg.federate.method = 'FedAvg'
        cfg.federate.make_global_eval = False
        cfg.federate.share_local_model = False
        cfg.federate.online_aggr = False
        cfg.federate.use_ss = False
        cfg.data.type = 'toy'
        cfg.model.type = 'lr'
        cfg.train.local_update_steps = 2
        cfg.train.optimizer.lr = 0.01
        cfg.train.optimizer.type = 'SGD'
        cfg.quantization.method = 'none'
        
        cfg.freeze()
        
        print("âœ… Configuration prepared")
        
        # åˆ›å»ºè®­ç»ƒå™¨å’Œæ•°æ®
        from federatedscope.core.auxiliaries.data_builder import get_data
        from federatedscope.core.auxiliaries.model_builder import get_model
        from federatedscope.core.auxiliaries.trainer_builder import get_trainer
        
        # å‡†å¤‡æ•°æ®
        data, modified_cfg = get_data(cfg)
        print("âœ… Data prepared")
        
        # å‡†å¤‡æ¨¡å‹
        model = get_model(modified_cfg, data)
        print("âœ… Model prepared")
        
        # åˆ›å»ºç®€åŒ–çš„å®¢æˆ·ç«¯ç±»è¿›è¡Œæµ‹è¯•
        class TestClientForModelSaving:
            def __init__(self, client_id, cfg, data, model):
                self.ID = client_id
                self.state = 1  # ç¬¬1è½®
                self.cur_timestamp = time.time()
                self._cfg = cfg
                
                # åˆ›å»ºè®­ç»ƒå™¨
                self.trainer = get_trainer(model=model,
                                         data=data['train'],
                                         device=cfg.device,
                                         config=cfg)
                print(f"âœ… Client {client_id}: Trainer prepared")
                
            def _save_local_model_after_training(self):
                """ä¿å­˜æœ¬åœ°æ¨¡å‹"""
                try:
                    import os
                    import torch
                    
                    # Get client name/identifier
                    client_name = f"client_{self.ID}"
                    
                    # Create directory path: /tmp/client_name/
                    save_dir = os.path.join(os.getcwd(), "tmp", client_name)
                    os.makedirs(save_dir, exist_ok=True)
                    
                    # Create filename: client_name + round
                    filename = f"{client_name}_round_{self.state}.pth"
                    save_path = os.path.join(save_dir, filename)
                    
                    # Save model using trainer's save_model method
                    if hasattr(self.trainer, 'save_model'):
                        self.trainer.save_model(save_path, cur_round=self.state)
                        print(f"ğŸ’¾ Client {self.ID}: Saved local model to {save_path}")
                        return save_path
                    else:
                        # Fallback: save model state dict directly
                        if hasattr(self.trainer, 'ctx') and hasattr(self.trainer.ctx, 'model'):
                            model_state = self.trainer.ctx.model.state_dict()
                            checkpoint = {
                                'client_id': self.ID,
                                'round': self.state,
                                'model': model_state,
                                'timestamp': self.cur_timestamp
                            }
                            torch.save(checkpoint, save_path)
                            print(f"ğŸ’¾ Client {self.ID}: Saved local model to {save_path}")
                            return save_path
                        else:
                            print(f"âš ï¸ Client {self.ID}: Cannot save model - no accessible model found")
                            return None
                            
                except Exception as e:
                    print(f"âŒ Client {self.ID}: Failed to save local model: {e}")
                    return None
                    
            def simulate_training_and_save(self):
                """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹å¹¶ä¿å­˜"""
                print(f"ğŸƒ Client {self.ID}: Starting training simulation...")
                
                # æ¨¡æ‹Ÿè®­ç»ƒï¼ˆä¸éœ€è¦çœŸå®è®­ç»ƒï¼Œåªéœ€è¦æ¨¡å‹å­˜åœ¨ï¼‰
                try:
                    # æ‰§è¡Œä¸€äº›ç®€å•çš„å‰å‘ä¼ æ’­ä»¥ç¡®ä¿æ¨¡å‹æ­£å¸¸
                    if hasattr(self.trainer, 'ctx') and hasattr(self.trainer.ctx, 'model'):
                        model = self.trainer.ctx.model
                        # åˆ›å»ºä¸€äº›å‡æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­æµ‹è¯•
                        import torch
                        if hasattr(model, 'fc'):  # LogisticRegressionæ¨¡å‹
                            fake_input = torch.randn(1, model.fc.in_features)
                            output = model(fake_input)
                            print(f"   âœ… Model forward pass successful, output shape: {output.shape}")
                        
                    # ä¿å­˜æ¨¡å‹
                    saved_path = self._save_local_model_after_training()
                    return saved_path
                    
                except Exception as e:
                    print(f"   âŒ Training simulation failed: {e}")
                    return None
        
        # æµ‹è¯•å¤šä¸ªå®¢æˆ·ç«¯
        print("\nğŸ¤– Creating and testing clients...")
        
        saved_files = []
        for client_id in [1, 2]:
            print(f"\nğŸ“± Testing Client {client_id}:")
            
            try:
                # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯å‡†å¤‡æ•°æ®
                client_data = data[client_id]
                client_model = get_model(modified_cfg, data)
                
                # åˆ›å»ºå®¢æˆ·ç«¯
                client = TestClientForModelSaving(client_id, modified_cfg, client_data, client_model)
                
                # æ‰§è¡Œè®­ç»ƒå’Œä¿å­˜
                saved_path = client.simulate_training_and_save()
                if saved_path:
                    saved_files.append(saved_path)
                    
            except Exception as e:
                print(f"   âŒ Client {client_id} failed: {e}")
        
        # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
        print(f"\nğŸ” Verifying {len(saved_files)} saved files...")
        
        success_count = 0
        for saved_file in saved_files:
            if os.path.exists(saved_file):
                try:
                    import torch
                    checkpoint = torch.load(saved_file, map_location='cpu')
                    print(f"   âœ… {os.path.basename(saved_file)}")
                    
                    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿¡æ¯
                    if 'cur_round' in checkpoint:
                        print(f"      - Round: {checkpoint['cur_round']}")
                    elif 'round' in checkpoint:
                        print(f"      - Round: {checkpoint['round']}")
                        
                    if 'model' in checkpoint:
                        model_keys = list(checkpoint['model'].keys())
                        print(f"      - Model keys: {model_keys}")
                        
                    file_size = os.path.getsize(saved_file)
                    print(f"      - File size: {file_size} bytes")
                    
                    success_count += 1
                    
                except Exception as e:
                    print(f"   âŒ {os.path.basename(saved_file)}: Failed to load - {e}")
            else:
                print(f"   âŒ {os.path.basename(saved_file)}: File not found")
        
        # æ˜¾ç¤ºç›®å½•ç»“æ„
        print(f"\nğŸ“ Generated directory structure:")
        tmp_dir = os.path.join(os.getcwd(), "tmp")
        if os.path.exists(tmp_dir):
            for root, dirs, files in os.walk(tmp_dir):
                level = root.replace(tmp_dir, '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                subindent = ' ' * 2 * (level + 1)
                for file in files:
                    file_path = os.path.join(root, file)
                    file_size = os.path.getsize(file_path)
                    print(f"{subindent}{file} ({file_size} bytes)")
        
        print(f"\nğŸ“Š Test Results:")
        print(f"   Clients tested: 2")
        print(f"   Successfully saved models: {success_count}")
        print(f"   Directory format: tmp/client_X/client_X_round_Y.pth")
        
        if success_count >= 1:
            print("\nğŸ‰ Client model saving test PASSED!")
            print("âœ… Local model saving functionality works in real FederatedScope environment")
            return True
        else:
            print(f"\nâš ï¸ Test had issues: only {success_count} models saved successfully")
            return False
            
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ§ª REAL CLIENT MODEL SAVING TEST")
    print("=" * 60)
    
    success = test_client_model_saving()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ TEST PASSED!")
        print("âœ… Client model saving functionality is working correctly")
        print("âœ… Ready for real federated learning scenarios")
    else:
        print("âŒ TEST HAD ISSUES!")
        print("   Check the implementation and error messages above")
    
    print(f"\nTest completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}")