#!/usr/bin/env python3

import sqlite3
import pandas as pd
import numpy as np
from pathlib import Path
import logging
from collections import defaultdict
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def analyze_cross_client_importance_prioritization():
    """åˆ†ææ‰€æœ‰å®¢æˆ·ç«¯çš„é‡è¦åº¦ä¼˜å…ˆå¤„ç†æƒ…å†µ"""
    
    # æ•°æ®åº“è·¯å¾„
    db_paths = {
        'client_1': 'multi_process_test_v2/client_1_output/chunk_storage.db',
        'client_2': 'multi_process_test_v2/client_2_output/chunk_storage.db', 
        'client_3': 'multi_process_test_v2/client_3_output/chunk_storage.db'
    }
    
    logger.info("ğŸ” å¼€å§‹è·¨å®¢æˆ·ç«¯é‡è¦åº¦ä¼˜å…ˆå¤„ç†åˆ†æ...")
    
    all_client_data = {}
    
    for client_name, db_path in db_paths.items():
        if not Path(db_path).exists():
            logger.warning(f"âŒ æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
            continue
            
        logger.info(f"ğŸ“Š åˆ†æ {client_name} æ•°æ®åº“...")
        
        try:
            conn = sqlite3.connect(db_path)
            
            # è·å–chunké‡è¦åº¦åˆ†æ•°
            importance_query = """
            SELECT round_num, source_id, chunk_id, importance_score, timestamp
            FROM chunk_importance 
            ORDER BY round_num, timestamp
            """
            importance_df = pd.read_sql_query(importance_query, conn)
            
            # è·å–æ¥æ”¶åˆ°çš„chunkæ•°æ®
            received_query = """
            SELECT round_num, source_id, chunk_id, timestamp, data_size
            FROM chunk_data
            WHERE source_id != ?
            ORDER BY round_num, timestamp
            """
            # è·å–å½“å‰å®¢æˆ·ç«¯IDï¼ˆå‡è®¾client_1çš„IDæ˜¯1ï¼Œclient_2çš„IDæ˜¯2ï¼Œetcï¼‰
            client_id = int(client_name.split('_')[1])
            received_df = pd.read_sql_query(received_query, conn, params=(client_id,))
            
            conn.close()
            
            # åˆå¹¶é‡è¦åº¦å’Œæ¥æ”¶æ•°æ®
            if not received_df.empty and not importance_df.empty:
                merged_df = received_df.merge(
                    importance_df, 
                    on=['round_num', 'source_id', 'chunk_id'], 
                    how='left'
                )
                
                # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æ¥æ”¶é¡ºåº
                merged_df = merged_df.sort_values('timestamp_x')  # timestamp_xæ¥è‡ªreceived_df
                
                all_client_data[client_name] = {
                    'importance_df': importance_df,
                    'received_df': received_df,
                    'merged_df': merged_df,
                    'client_id': client_id
                }
                
                logger.info(f"âœ… {client_name}: æ‰¾åˆ° {len(received_df)} ä¸ªæ¥æ”¶åˆ°çš„chunkï¼Œ{len(importance_df)} ä¸ªé‡è¦åº¦åˆ†æ•°")
            else:
                logger.warning(f"âš ï¸ {client_name}: æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œåˆ†æ")
                
        except Exception as e:
            logger.error(f"âŒ åˆ†æ {client_name} æ—¶å‡ºé”™: {e}")
    
    if not all_client_data:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å®¢æˆ·ç«¯æ•°æ®")
        return
        
    logger.info("\n" + "="*80)
    logger.info("ğŸ“ˆ é‡è¦åº¦ä¼˜å…ˆå¤„ç†åˆ†æç»“æœ")
    logger.info("="*80)
    
    total_importance_correlation = []
    total_perfect_order_ratio = []
    
    for client_name, data in all_client_data.items():
        merged_df = data['merged_df']
        
        if merged_df.empty or merged_df['importance_score'].isna().all():
            logger.warning(f"âš ï¸ {client_name}: æ²¡æœ‰æœ‰æ•ˆçš„é‡è¦åº¦-æ¥æ”¶æ•°æ®")
            continue
            
        logger.info(f"\nğŸ¯ {client_name.upper()} åˆ†æ:")
        
        # 1. æŒ‰é‡è¦åº¦æ’åºçš„ç†æƒ³é¡ºåº vs å®é™…æ¥æ”¶é¡ºåº
        valid_chunks = merged_df.dropna(subset=['importance_score'])
        
        if len(valid_chunks) == 0:
            logger.warning(f"âš ï¸ {client_name}: æ²¡æœ‰æœ‰æ•ˆçš„é‡è¦åº¦åˆ†æ•°")
            continue
            
        # ç†æƒ³é¡ºåºï¼ˆæŒ‰é‡è¦åº¦é™åºï¼‰
        ideal_order = valid_chunks.sort_values('importance_score', ascending=False)
        
        # å®é™…æ¥æ”¶é¡ºåºï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰
        actual_order = valid_chunks.sort_values('timestamp_x')
        
        logger.info(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(valid_chunks)} ä¸ªchunk")
        
        # 2. è®¡ç®—Spearmanç›¸å…³ç³»æ•°ï¼ˆé‡è¦åº¦ä¸æ¥æ”¶é¡ºåºçš„ç›¸å…³æ€§ï¼‰
        if len(valid_chunks) > 1:
            # ä¸ºæ¯ä¸ªchunkåˆ†é…æ¥æ”¶é¡ºåºå·ï¼ˆ1è¡¨ç¤ºæœ€æ—©æ¥æ”¶ï¼‰
            actual_order_rank = actual_order.reset_index(drop=True)
            actual_order_rank['reception_order'] = range(1, len(actual_order_rank) + 1)
            
            # è®¡ç®—é‡è¦åº¦ä¸æ¥æ”¶é¡ºåºçš„ç›¸å…³æ€§ï¼ˆæœŸæœ›è´Ÿç›¸å…³ï¼šé‡è¦åº¦é«˜çš„åº”è¯¥å…ˆæ¥æ”¶ï¼‰
            correlation = np.corrcoef(
                actual_order_rank['importance_score'], 
                actual_order_rank['reception_order']
            )[0, 1]
            
            total_importance_correlation.append(correlation)
            
            logger.info(f"ğŸ“ˆ é‡è¦åº¦-æ¥æ”¶é¡ºåºç›¸å…³ç³»æ•°: {correlation:.4f}")
            logger.info(f"   (æœŸæœ›è´Ÿå€¼ï¼šé‡è¦åº¦è¶Šé«˜ï¼Œæ¥æ”¶é¡ºåºè¶Šé å‰)")
        
        # 3. åˆ†æå‰Nä¸ªchunkçš„é‡è¦åº¦åŒ¹é…æƒ…å†µ
        top_n_values = [3, 5, min(10, len(valid_chunks))]
        
        for n in top_n_values:
            if n > len(valid_chunks):
                continue
                
            # ç†æƒ³å‰Nä¸ªï¼ˆé‡è¦åº¦æœ€é«˜ï¼‰
            ideal_top_n = set(ideal_order.head(n).index)
            
            # å®é™…å‰Nä¸ªï¼ˆæœ€æ—©æ¥æ”¶ï¼‰
            actual_top_n = set(actual_order.head(n).index)
            
            # è®¡ç®—åŒ¹é…ç‡
            match_count = len(ideal_top_n.intersection(actual_top_n))
            match_ratio = match_count / n
            
            if n == 3:
                total_perfect_order_ratio.append(match_ratio)
            
            logger.info(f"ğŸ¯ å‰{n}ä¸ªchunkåŒ¹é…ç‡: {match_ratio:.2%} ({match_count}/{n})")
        
        # 4. æ˜¾ç¤ºå…·ä½“çš„æ¥æ”¶é¡ºåºvsé‡è¦åº¦
        logger.info(f"\nğŸ“‹ {client_name} è¯¦ç»†æ¥æ”¶é¡ºåº (å‰10ä¸ª):")
        display_df = actual_order.head(10)[['round_num', 'source_id', 'chunk_id', 'importance_score', 'timestamp_x']]
        display_df['reception_rank'] = range(1, len(display_df) + 1)
        
        for _, row in display_df.iterrows():
            logger.info(f"   #{row['reception_rank']:2d}: Round {row['round_num']}, "
                       f"Source {row['source_id']}, Chunk {row['chunk_id']}, "
                       f"é‡è¦åº¦: {row['importance_score']:.6f}")
        
        # 5. é‡è¦åº¦åˆ†å¸ƒç»Ÿè®¡
        logger.info(f"\nğŸ“Š {client_name} é‡è¦åº¦åˆ†æ•°ç»Ÿè®¡:")
        importance_stats = valid_chunks['importance_score'].describe()
        logger.info(f"   å¹³å‡å€¼: {importance_stats['mean']:.6f}")
        logger.info(f"   æ ‡å‡†å·®: {importance_stats['std']:.6f}") 
        logger.info(f"   æœ€å¤§å€¼: {importance_stats['max']:.6f}")
        logger.info(f"   æœ€å°å€¼: {importance_stats['min']:.6f}")
    
    # 6. æ€»ä½“åˆ†æ
    if total_importance_correlation:
        avg_correlation = np.mean(total_importance_correlation)
        logger.info(f"\nğŸŒŸ æ€»ä½“åˆ†æ:")
        logger.info(f"ğŸ“ˆ å¹³å‡é‡è¦åº¦-æ¥æ”¶é¡ºåºç›¸å…³ç³»æ•°: {avg_correlation:.4f}")
        
        if avg_correlation < -0.3:
            logger.info("âœ… ä¼˜ç§€ï¼ç®—æ³•æ˜¾è‘—ä¼˜å…ˆå¤„ç†é«˜é‡è¦åº¦chunk")
        elif avg_correlation < -0.1:
            logger.info("âœ… è‰¯å¥½ï¼ç®—æ³•å€¾å‘äºä¼˜å…ˆå¤„ç†é«˜é‡è¦åº¦chunk")
        elif avg_correlation < 0.1:
            logger.info("âš ï¸ ä¸€èˆ¬ï¼šç®—æ³•å¯¹é‡è¦åº¦çš„ä¼˜å…ˆå¤„ç†ä¸æ˜æ˜¾")
        else:
            logger.info("âŒ è¾ƒå·®ï¼šç®—æ³•æ²¡æœ‰æŒ‰é‡è¦åº¦ä¼˜å…ˆå¤„ç†")
    
    if total_perfect_order_ratio:
        avg_top3_match = np.mean(total_perfect_order_ratio)
        logger.info(f"ğŸ¯ å¹³å‡å‰3ä¸ªchunkåŒ¹é…ç‡: {avg_top3_match:.2%}")
        
        if avg_top3_match > 0.8:
            logger.info("âœ… ä¼˜ç§€ï¼å‰3ä¸ªchunké«˜åº¦åŒ¹é…é‡è¦åº¦æ’åº")
        elif avg_top3_match > 0.5:
            logger.info("âœ… è‰¯å¥½ï¼å‰3ä¸ªchunkè¾ƒå¥½åŒ¹é…é‡è¦åº¦æ’åº")
        elif avg_top3_match > 0.3:
            logger.info("âš ï¸ ä¸€èˆ¬ï¼šå‰3ä¸ªchunkéƒ¨åˆ†åŒ¹é…é‡è¦åº¦æ’åº")
        else:
            logger.info("âŒ è¾ƒå·®ï¼šå‰3ä¸ªchunkå¾ˆå°‘åŒ¹é…é‡è¦åº¦æ’åº")
    
    logger.info("\n" + "="*80)
    logger.info("âœ… è·¨å®¢æˆ·ç«¯é‡è¦åº¦ä¼˜å…ˆå¤„ç†åˆ†æå®Œæˆ")
    logger.info("="*80)

if __name__ == "__main__":
    analyze_cross_client_importance_prioritization()