2025-12-25 03:19:17,887 - __main__ - INFO - ğŸš€ Start Ray V2 federated learning
2025-12-25 03:19:17,887 - __main__ - INFO - ğŸ“Š Configuration: 50 clients, 100 rounds training, total nodes: 51
2025-12-25 03:19:17,887 - __main__ - INFO - ğŸ§¹ Clean environment...
2025-12-25 03:19:17,916 - __main__ - INFO - ğŸ—‘ï¸  Cleaned tmpfs chunk directory: /tmp/fl_chunks
2025-12-25 03:19:19,213 - __main__ - INFO - âœ… Environment cleanup completed
2025-12-25 03:19:19,213 - __main__ - INFO - ğŸš€ Applying host network optimizations for ultra-high concurrency...
2025-12-25 03:19:19,329 - __main__ - WARNING - âš ï¸  22 network optimizations could not be applied (may need sudo access)
2025-12-25 03:19:19,329 - __main__ - WARNING - âš ï¸  No host network optimizations applied! High-concurrency FL may experience issues.
2025-12-25 03:19:19,329 - __main__ - WARNING -    Consider running with sudo or contacting HPC admin to apply these settings.
2025-12-25 03:19:19,329 - __main__ - INFO - ğŸ¦­ Initializing Podman rootless environment...
2025-12-25 03:19:19,332 - __main__ - WARNING - âš ï¸  Podman unavailable, trying Docker...
2025-12-25 03:19:19,333 - __main__ - INFO - ğŸ“¦ Using non-container mode (Fallback)
2025-12-25 03:19:21,366 - __main__ - INFO - ğŸ” Found available port for Ray dashboard: 8265
2025-12-25 03:19:24,024	INFO worker.py:1998 -- Started a local Ray instance. View the dashboard at [1m[32m10.0.0.2:8265 [39m[22m
/home/naicheng_li/.conda/envs/flv2/lib/python3.10/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
2025-12-25 03:19:25,030 - __main__ - INFO - ğŸš€ Ray cluster initialization completed:
2025-12-25 03:19:25,031 - __main__ - INFO -    ğŸ“Š Resources: {'accelerator_type:A100': 1.0, 'memory': 376276730880.0, 'node:10.0.0.2': 1.0, 'object_store_memory': 145967201280.0, 'node:__internal_head__': 1.0, 'GPU': 4.0, 'CPU': 64.0}
2025-12-25 03:19:25,031 - __main__ - INFO -    ğŸŒ Dashboard: http://127.0.0.1:8265
2025-12-25 03:19:25,031 - __main__ - INFO - ğŸŒ Ray Cluster Topology:
2025-12-25 03:19:25,031 - __main__ - INFO -    ğŸ“ Head node: 36f0c6d02df5... (Server will run here)
2025-12-25 03:19:25,031 - __main__ - WARNING -    âš ï¸ No worker nodes found! Clients will run on head node (not recommended)
2025-12-25 03:19:25,032 - __main__ - INFO - ğŸ® Detected GPU type on node 36f0c6d02df5...: A100 -> 40.0GB per GPU
2025-12-25 03:19:25,032 - __main__ - INFO - ğŸ“ Head node (36f0c6d02df5...): 4.0 GPUs, 160.0GB total memory
2025-12-25 03:19:25,032 - __main__ - INFO - ğŸ“Š Total GPU memory across all nodes: 160.0GB
2025-12-25 03:19:25,032 - __main__ - INFO - ğŸ“Š Memory per client: 3.0GB â†’ Max capacity: 53 clients
2025-12-25 03:19:25,032 - __main__ - INFO -    Node 36f0c6d02df5...: 50 clients (160.0GB / 50 = 3.2GB/client)
2025-12-25 03:19:25,032 - __main__ - INFO -    Node 36f0c6d02df5...: 4.0 GPU, 50 clients, max 13 clients/GPU -> 0.0731 GPU/client
2025-12-25 03:19:25,032 - __main__ - INFO - ğŸ¯ GPU allocation summary: {'total_available_gpus': 4.0, 'total_allocated_gpus': 3.655, 'utilization_rate': '91.38%', 'server': 'CPU only', 'per_node_allocation': {'36f0c6d02df5': {'count': 50, 'total_gpu': 3.6550000000000025}}, 'gpu_clients': 50, 'cpu_clients': 0}
2025-12-25 03:19:25,032 - __main__ - INFO - ğŸ“‹ Client distribution: 50 GPU clients, 0 CPU clients
2025-12-25 03:19:25,033 - __main__ - INFO - ğŸ“ Server scheduled to head node: 36f0c6d02df5...
2025-12-25 03:19:25,124 - __main__ - INFO - ğŸ“¦ Using Fallback (no container) for Server
2025-12-25 03:19:25,191 - __main__ - INFO - âœ… Server started: 10.0.0.2:35183
slurmstepd-srv-it-node02: error: *** JOB 536 ON srv-it-node02 CANCELLED AT 2025-12-25T03:19:38 ***
