# ============================================================================
# Experiments for GPU3 Node - TinyBERT + SST-2 Study (Updated for fair comparison)
# ============================================================================
#
# Usage:
#   python run_ray_hpc.py --batch experiments_gpu3.yaml
#
# Node: GPU3 (srv-it-node03) - 128 CPUs, 1.5TB RAM, 2x GPUs
# Model: TinyBERT (14.35M parameters)
# Dataset: SST-2 (Stanford Sentiment Treebank, binary classification)
#
# Updated:
#   - TOTAL_ROUNDS: 50 (same as CFL for fair comparison)
#   - BITTORRENT_TIMEOUT: 40s
#   - BT_ENABLE_COMPENSATION: True
#   - BT_RARITY_WEIGHT: 0.01 (importance-based scheduling)
#
# ============================================================================

# Global settings for TinyBERT + SST-2 on GPU3
global:
  code_snapshot_dir: "./experiments_snapshots"
  output_base_dir: "./experiments_output_gpu3"

  # GPU3 Node Resources (1.5TB RAM for TinyBERT)
  partition: "gpu3"
  nodes: 1
  gpus_per_node: 2
  cpus_per_task: 128
  mem: "1400G"
  time_limit: "48:00:00"

experiments:
  # ==========================================================================
  # Set 1: Baseline - COMMENTED OUT
  # ==========================================================================
  # # --- Timeout = infinite (Full BitTorrent, wait for 100% chunks) ---
  # - name: "set1_inf_alpha10000"
  #   description: "Infinite timeout (100% chunks), IID data (alpha=10000)"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     CHUNK_NUM: 16
  #     TOPOLOGY_TYPE: "mesh"
  #     TOPOLOGY_CONNECTIONS: 4
  #     DATASET: "sst2@huggingface_datasets"
  #     DATA_ROOT: "glue"
  #     DATA_MAX_LEN: 128
  #     DATA_HF_HALF_VAL_DUMMY_TEST: True
  #     DATA_SHARE_TEST_DATASET: True
  #     DATA_SPLITTER: "lda"
  #     DATA_SPLIT_ALPHA: 10000.0
  #     DATA_SPLITS: [0.8, 0.1, 0.1]
  #     DATA_MERGE_LEAF_BEFORE_SPLIT: False
  #     BATCH_SIZE: 32
  #     MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
  #     MODEL_TASK: "SequenceClassification"
  #     MODEL_OUT_CHANNELS: 2
  #     TRAINER_TYPE: "nlptrainer"
  #     LOCAL_UPDATE_STEPS: 1
  #     LEARNING_RATE: 0.00002
  #     OPTIMIZER: "AdamW"
  #     WEIGHT_DECAY: 0.01
  #     GRAD_CLIP: 1.0
  #     CRITERION_TYPE: "CrossEntropyLoss"
  #     LR_SCHEDULER_TYPE: ""
  #     EVAL_FREQ: 1
  #     EVAL_METRICS: ["acc", "f1"]
  #     BT_ENABLE_COMPENSATION: False
  #     BITTORRENT_TIMEOUT: 1000000.0
  #     BT_RARITY_WEIGHT: 0.01
  #     CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
  #     CHUNK_NFS_COMPATIBLE: False

  # - name: "set1_inf_alpha1"
  #   description: "Infinite timeout (100% chunks), moderate non-IID (alpha=1.0)"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     CHUNK_NUM: 16
  #     TOPOLOGY_TYPE: "mesh"
  #     TOPOLOGY_CONNECTIONS: 4
  #     DATASET: "sst2@huggingface_datasets"
  #     DATA_ROOT: "glue"
  #     DATA_MAX_LEN: 128
  #     DATA_HF_HALF_VAL_DUMMY_TEST: True
  #     DATA_SHARE_TEST_DATASET: True
  #     DATA_SPLITTER: "lda"
  #     DATA_SPLIT_ALPHA: 1.0
  #     DATA_SPLITS: [0.8, 0.1, 0.1]
  #     DATA_MERGE_LEAF_BEFORE_SPLIT: False
  #     BATCH_SIZE: 32
  #     MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
  #     MODEL_TASK: "SequenceClassification"
  #     MODEL_OUT_CHANNELS: 2
  #     TRAINER_TYPE: "nlptrainer"
  #     LOCAL_UPDATE_STEPS: 1
  #     LEARNING_RATE: 0.00002
  #     OPTIMIZER: "AdamW"
  #     WEIGHT_DECAY: 0.01
  #     GRAD_CLIP: 1.0
  #     CRITERION_TYPE: "CrossEntropyLoss"
  #     LR_SCHEDULER_TYPE: ""
  #     EVAL_FREQ: 1
  #     EVAL_METRICS: ["acc", "f1"]
  #     BT_ENABLE_COMPENSATION: False
  #     BITTORRENT_TIMEOUT: 1000000.0
  #     BT_RARITY_WEIGHT: 0.01
  #     CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
  #     CHUNK_NFS_COMPATIBLE: False

  # - name: "set1_inf_alpha03"
  #   description: "Infinite timeout (100% chunks), high non-IID (alpha=0.3)"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     CHUNK_NUM: 16
  #     TOPOLOGY_TYPE: "mesh"
  #     TOPOLOGY_CONNECTIONS: 4
  #     DATASET: "sst2@huggingface_datasets"
  #     DATA_ROOT: "glue"
  #     DATA_MAX_LEN: 128
  #     DATA_HF_HALF_VAL_DUMMY_TEST: True
  #     DATA_SHARE_TEST_DATASET: True
  #     DATA_SPLITTER: "lda"
  #     DATA_SPLIT_ALPHA: 0.3
  #     DATA_SPLITS: [0.8, 0.1, 0.1]
  #     DATA_MERGE_LEAF_BEFORE_SPLIT: False
  #     BATCH_SIZE: 32
  #     MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
  #     MODEL_TASK: "SequenceClassification"
  #     MODEL_OUT_CHANNELS: 2
  #     TRAINER_TYPE: "nlptrainer"
  #     LOCAL_UPDATE_STEPS: 1
  #     LEARNING_RATE: 0.00002
  #     OPTIMIZER: "AdamW"
  #     WEIGHT_DECAY: 0.01
  #     GRAD_CLIP: 1.0
  #     CRITERION_TYPE: "CrossEntropyLoss"
  #     LR_SCHEDULER_TYPE: ""
  #     EVAL_FREQ: 1
  #     EVAL_METRICS: ["acc", "f1"]
  #     BT_ENABLE_COMPENSATION: False
  #     BITTORRENT_TIMEOUT: 1000000.0
  #     BT_RARITY_WEIGHT: 0.01
  #     CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
  #     CHUNK_NFS_COMPATIBLE: False

  # # --- Timeout = 0s (No BitTorrent, local training only) ---
  # - name: "set1_t0_alpha10000"
  #   description: "No BitTorrent (timeout=0), local only, IID data (alpha=10000)"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     CHUNK_NUM: 16
  #     TOPOLOGY_TYPE: "mesh"
  #     TOPOLOGY_CONNECTIONS: 4
  #     DATASET: "sst2@huggingface_datasets"
  #     DATA_ROOT: "glue"
  #     DATA_MAX_LEN: 128
  #     DATA_HF_HALF_VAL_DUMMY_TEST: True
  #     DATA_SHARE_TEST_DATASET: True
  #     DATA_SPLITTER: "lda"
  #     DATA_SPLIT_ALPHA: 10000.0
  #     DATA_SPLITS: [0.8, 0.1, 0.1]
  #     DATA_MERGE_LEAF_BEFORE_SPLIT: False
  #     BATCH_SIZE: 32
  #     MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
  #     MODEL_TASK: "SequenceClassification"
  #     MODEL_OUT_CHANNELS: 2
  #     TRAINER_TYPE: "nlptrainer"
  #     LOCAL_UPDATE_STEPS: 1
  #     LEARNING_RATE: 0.00002
  #     OPTIMIZER: "AdamW"
  #     WEIGHT_DECAY: 0.01
  #     GRAD_CLIP: 1.0
  #     CRITERION_TYPE: "CrossEntropyLoss"
  #     LR_SCHEDULER_TYPE: ""
  #     EVAL_FREQ: 1
  #     EVAL_METRICS: ["acc", "f1"]
  #     BT_ENABLE_COMPENSATION: False
  #     BITTORRENT_TIMEOUT: 0.0
  #     BT_RARITY_WEIGHT: 0.01
  #     CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
  #     CHUNK_NFS_COMPATIBLE: False

  # - name: "set1_t0_alpha1"
  #   description: "No BitTorrent (timeout=0), local only, moderate non-IID (alpha=1.0)"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     CHUNK_NUM: 16
  #     TOPOLOGY_TYPE: "mesh"
  #     TOPOLOGY_CONNECTIONS: 4
  #     DATASET: "sst2@huggingface_datasets"
  #     DATA_ROOT: "glue"
  #     DATA_MAX_LEN: 128
  #     DATA_HF_HALF_VAL_DUMMY_TEST: True
  #     DATA_SHARE_TEST_DATASET: True
  #     DATA_SPLITTER: "lda"
  #     DATA_SPLIT_ALPHA: 1.0
  #     DATA_SPLITS: [0.8, 0.1, 0.1]
  #     DATA_MERGE_LEAF_BEFORE_SPLIT: False
  #     BATCH_SIZE: 32
  #     MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
  #     MODEL_TASK: "SequenceClassification"
  #     MODEL_OUT_CHANNELS: 2
  #     TRAINER_TYPE: "nlptrainer"
  #     LOCAL_UPDATE_STEPS: 1
  #     LEARNING_RATE: 0.00002
  #     OPTIMIZER: "AdamW"
  #     WEIGHT_DECAY: 0.01
  #     GRAD_CLIP: 1.0
  #     CRITERION_TYPE: "CrossEntropyLoss"
  #     LR_SCHEDULER_TYPE: ""
  #     EVAL_FREQ: 1
  #     EVAL_METRICS: ["acc", "f1"]
  #     BT_ENABLE_COMPENSATION: False
  #     BITTORRENT_TIMEOUT: 0.0
  #     BT_RARITY_WEIGHT: 0.01
  #     CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
  #     CHUNK_NFS_COMPATIBLE: False

  # - name: "set1_t0_alpha03"
  #   description: "No BitTorrent (timeout=0), local only, high non-IID (alpha=0.3)"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     CHUNK_NUM: 16
  #     TOPOLOGY_TYPE: "mesh"
  #     TOPOLOGY_CONNECTIONS: 4
  #     DATASET: "sst2@huggingface_datasets"
  #     DATA_ROOT: "glue"
  #     DATA_MAX_LEN: 128
  #     DATA_HF_HALF_VAL_DUMMY_TEST: True
  #     DATA_SHARE_TEST_DATASET: True
  #     DATA_SPLITTER: "lda"
  #     DATA_SPLIT_ALPHA: 0.3
  #     DATA_SPLITS: [0.8, 0.1, 0.1]
  #     DATA_MERGE_LEAF_BEFORE_SPLIT: False
  #     BATCH_SIZE: 32
  #     MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
  #     MODEL_TASK: "SequenceClassification"
  #     MODEL_OUT_CHANNELS: 2
  #     TRAINER_TYPE: "nlptrainer"
  #     LOCAL_UPDATE_STEPS: 1
  #     LEARNING_RATE: 0.00002
  #     OPTIMIZER: "AdamW"
  #     WEIGHT_DECAY: 0.01
  #     GRAD_CLIP: 1.0
  #     CRITERION_TYPE: "CrossEntropyLoss"
  #     LR_SCHEDULER_TYPE: ""
  #     EVAL_FREQ: 1
  #     EVAL_METRICS: ["acc", "f1"]
  #     BT_ENABLE_COMPENSATION: False
  #     BITTORRENT_TIMEOUT: 0.0
  #     BT_RARITY_WEIGHT: 0.01
  #     CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
  #     CHUNK_NFS_COMPATIBLE: False

  # ==========================================================================
  # NEW Set: Fair Comparison - 50 rounds, timeout=40s, compensation ON
  # For comparison with CFL (50 rounds)
  # ==========================================================================

  # --- IID data (alpha=100000) ---
  - name: "fltorrent_t40_r50_alpha100000"
    description: "FLTorrent fair comparison: 50 rounds, timeout=40s, compensation ON, IID data"
    config:
      CLIENT_NUM: 50
      TOTAL_ROUNDS: 50
      CHUNK_NUM: 16
      TOPOLOGY_TYPE: "mesh"
      TOPOLOGY_CONNECTIONS: 4
      DATASET: "sst2@huggingface_datasets"
      DATA_ROOT: "glue"
      DATA_MAX_LEN: 128
      DATA_HF_HALF_VAL_DUMMY_TEST: True
      DATA_SHARE_TEST_DATASET: True
      DATA_SPLITTER: "lda"
      DATA_SPLIT_ALPHA: 100000.0
      DATA_SPLITS: [0.8, 0.1, 0.1]
      DATA_MERGE_LEAF_BEFORE_SPLIT: False
      BATCH_SIZE: 32
      MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
      MODEL_TASK: "SequenceClassification"
      MODEL_OUT_CHANNELS: 2
      TRAINER_TYPE: "nlptrainer"
      LOCAL_UPDATE_STEPS: 1
      LEARNING_RATE: 0.00002
      OPTIMIZER: "AdamW"
      WEIGHT_DECAY: 0.01
      GRAD_CLIP: 1.0
      CRITERION_TYPE: "CrossEntropyLoss"
      LR_SCHEDULER_TYPE: ""
      EVAL_FREQ: 1
      EVAL_METRICS: ["acc", "f1"]
      BT_ENABLE_COMPENSATION: True
      BITTORRENT_TIMEOUT: 40.0
      BT_RARITY_WEIGHT: 0.01
      CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
      CHUNK_NFS_COMPATIBLE: False

  # --- Moderate non-IID (alpha=1.0) ---
  - name: "fltorrent_t40_r50_alpha1"
    description: "FLTorrent fair comparison: 50 rounds, timeout=40s, compensation ON, moderate non-IID"
    config:
      CLIENT_NUM: 50
      TOTAL_ROUNDS: 50
      CHUNK_NUM: 16
      TOPOLOGY_TYPE: "mesh"
      TOPOLOGY_CONNECTIONS: 4
      DATASET: "sst2@huggingface_datasets"
      DATA_ROOT: "glue"
      DATA_MAX_LEN: 128
      DATA_HF_HALF_VAL_DUMMY_TEST: True
      DATA_SHARE_TEST_DATASET: True
      DATA_SPLITTER: "lda"
      DATA_SPLIT_ALPHA: 1.0
      DATA_SPLITS: [0.8, 0.1, 0.1]
      DATA_MERGE_LEAF_BEFORE_SPLIT: False
      BATCH_SIZE: 32
      MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
      MODEL_TASK: "SequenceClassification"
      MODEL_OUT_CHANNELS: 2
      TRAINER_TYPE: "nlptrainer"
      LOCAL_UPDATE_STEPS: 1
      LEARNING_RATE: 0.00002
      OPTIMIZER: "AdamW"
      WEIGHT_DECAY: 0.01
      GRAD_CLIP: 1.0
      CRITERION_TYPE: "CrossEntropyLoss"
      LR_SCHEDULER_TYPE: ""
      EVAL_FREQ: 1
      EVAL_METRICS: ["acc", "f1"]
      BT_ENABLE_COMPENSATION: True
      BITTORRENT_TIMEOUT: 40.0
      BT_RARITY_WEIGHT: 0.01
      CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
      CHUNK_NFS_COMPATIBLE: False

  # --- High non-IID (alpha=0.3) ---
  - name: "fltorrent_t40_r50_alpha03"
    description: "FLTorrent fair comparison: 50 rounds, timeout=40s, compensation ON, high non-IID"
    config:
      CLIENT_NUM: 50
      TOTAL_ROUNDS: 50
      CHUNK_NUM: 16
      TOPOLOGY_TYPE: "mesh"
      TOPOLOGY_CONNECTIONS: 4
      DATASET: "sst2@huggingface_datasets"
      DATA_ROOT: "glue"
      DATA_MAX_LEN: 128
      DATA_HF_HALF_VAL_DUMMY_TEST: True
      DATA_SHARE_TEST_DATASET: True
      DATA_SPLITTER: "lda"
      DATA_SPLIT_ALPHA: 0.3
      DATA_SPLITS: [0.8, 0.1, 0.1]
      DATA_MERGE_LEAF_BEFORE_SPLIT: False
      BATCH_SIZE: 32
      MODEL_TYPE: "./pretrained_models/TinyBERT_General_4L_312D@transformers"
      MODEL_TASK: "SequenceClassification"
      MODEL_OUT_CHANNELS: 2
      TRAINER_TYPE: "nlptrainer"
      LOCAL_UPDATE_STEPS: 1
      LEARNING_RATE: 0.00002
      OPTIMIZER: "AdamW"
      WEIGHT_DECAY: 0.01
      GRAD_CLIP: 1.0
      CRITERION_TYPE: "CrossEntropyLoss"
      LR_SCHEDULER_TYPE: ""
      EVAL_FREQ: 1
      EVAL_METRICS: ["acc", "f1"]
      BT_ENABLE_COMPENSATION: True
      BITTORRENT_TIMEOUT: 40.0
      BT_RARITY_WEIGHT: 0.01
      CHUNK_TMP_DIR: "/tmp/fl_chunks_gpu3"
      CHUNK_NFS_COMPATIBLE: False

  # ==========================================================================
  # OLD Set 2: COMMENTED OUT (replaced by new fair comparison experiments)
  # ==========================================================================
  # # --- Timeout = 20s (10% chunks) ---
  # - name: "set2_comp_t20_alpha10000"
  #   description: "Compensation ON, tau=0.01, timeout=20s (10% chunks), IID data"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # - name: "set2_comp_t20_alpha1"
  #   description: "Compensation ON, tau=0.01, timeout=20s (10% chunks), moderate non-IID"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # - name: "set2_comp_t20_alpha03"
  #   description: "Compensation ON, tau=0.01, timeout=20s (10% chunks), high non-IID"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # # --- Timeout = 40s (20% chunks) ---
  # - name: "set2_comp_t40_alpha10000"
  #   description: "Compensation ON, tau=0.01, timeout=40s (20% chunks), IID data"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # - name: "set2_comp_t40_alpha1"
  #   description: "Compensation ON, tau=0.01, timeout=40s (20% chunks), moderate non-IID"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # - name: "set2_comp_t40_alpha03"
  #   description: "Compensation ON, tau=0.01, timeout=40s (20% chunks), high non-IID"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # # --- Timeout = 80s (40% chunks) ---
  # - name: "set2_comp_t80_alpha10000"
  #   description: "Compensation ON, tau=0.01, timeout=80s (40% chunks), IID data"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # - name: "set2_comp_t80_alpha1"
  #   description: "Compensation ON, tau=0.01, timeout=80s (40% chunks), moderate non-IID"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...

  # - name: "set2_comp_t80_alpha03"
  #   description: "Compensation ON, tau=0.01, timeout=80s (40% chunks), high non-IID"
  #   config:
  #     CLIENT_NUM: 50
  #     TOTAL_ROUNDS: 30
  #     ...
