#!/usr/bin/env python3
"""
ç‹¬ç«‹æµ‹è¯•chunké‡è¦åº¦è®¡ç®—ç®—æ³•
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Dict, List
import matplotlib.pyplot as plt
import seaborn as sns

# æ¨¡æ‹ŸChunkManagerçš„æ ¸å¿ƒé‡è¦åº¦è®¡ç®—æ–¹æ³•
class TestChunkImportance:
    
    def compute_chunk_importance(self, params: Dict[str, np.ndarray], chunks_info: List[Dict], 
                                method: str = 'magnitude') -> List[float]:
        """è®¡ç®—æ¯ä¸ªchunkçš„é‡è¦åº¦åˆ†æ•°"""
        importance_scores = []
        
        print(f"ğŸ” æµ‹è¯•é‡è¦åº¦è®¡ç®—æ–¹æ³•: {method}")
        print(f"   å‚æ•°æ€»æ•°: {len(params)}")
        print(f"   Chunkæ•°é‡: {len(chunks_info)}")
        
        for i, chunk_info in enumerate(chunks_info):
            if method == 'magnitude':
                score = self._compute_magnitude_importance(params, chunk_info)
            elif method == 'l2_norm':
                score = self._compute_l2_norm_importance(params, chunk_info)
            elif method == 'snip':
                score = self._compute_snip_importance(params, chunk_info)
            elif method == 'fisher':
                score = self._compute_fisher_importance(params, chunk_info)
            else:
                score = self._compute_magnitude_importance(params, chunk_info)
                
            importance_scores.append(float(score))
            print(f"   Chunk {i}: åŸå§‹åˆ†æ•° = {score:.8f}")
            
        print(f"\nğŸ“Š åŸå§‹åˆ†æ•°ç»Ÿè®¡:")
        print(f"   æœ€å¤§å€¼: {max(importance_scores):.8f}")
        print(f"   æœ€å°å€¼: {min(importance_scores):.8f}")
        print(f"   å‡å€¼: {np.mean(importance_scores):.8f}")
        print(f"   æ ‡å‡†å·®: {np.std(importance_scores):.8f}")
        print(f"   å˜å¼‚ç³»æ•°: {np.std(importance_scores)/np.mean(importance_scores):.6f}")
        
        # ä½¿ç”¨L1å½’ä¸€åŒ–ï¼ˆæ€»å’Œå½’ä¸€åŒ–ï¼‰ï¼Œä¿æŒåŸå§‹æ¯”ä¾‹å…³ç³»
        if importance_scores:
            total_score = sum(importance_scores)
            if total_score > 0:
                normalized_scores = [s / total_score for s in importance_scores]
            else:
                normalized_scores = [1.0 / len(importance_scores)] * len(importance_scores)
        
        print(f"\nğŸ¯ L1å½’ä¸€åŒ–å:")
        for i, score in enumerate(normalized_scores):
            print(f"   Chunk {i}: {score:.8f}")
            
        print(f"\nğŸ“ˆ å½’ä¸€åŒ–åç»Ÿè®¡:")
        print(f"   æœ€å¤§å€¼: {max(normalized_scores):.8f}")
        print(f"   æœ€å°å€¼: {min(normalized_scores):.8f}")
        print(f"   æ€»å’Œ: {sum(normalized_scores):.8f}")
        print(f"   æ ‡å‡†å·®: {np.std(normalized_scores):.8f}")
        
        return normalized_scores
    
    def _compute_magnitude_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºå‚æ•°å¹…åº¦çš„é‡è¦åº¦è®¡ç®—"""
        total_magnitude = 0.0
        total_elements = 0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    total_magnitude += np.sum(np.abs(chunk_slice))
                    total_elements += len(chunk_slice)
        
        return total_magnitude / (total_elements + 1e-8)
    
    def _compute_l2_norm_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºL2èŒƒæ•°çš„é‡è¦åº¦è®¡ç®—"""
        total_l2_norm = 0.0
        total_elements = 0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    total_l2_norm += np.sum(chunk_slice ** 2)
                    total_elements += len(chunk_slice)
        
        return np.sqrt(total_l2_norm) / (total_elements + 1e-8)
    
    def _compute_snip_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºSNIPçš„é‡è¦åº¦è®¡ç®—"""
        # æ”¹è¿›çš„SNIPå®ç°ï¼šè€ƒè™‘å‚æ•°å±‚çº§é‡è¦æ€§
        total_snip_score = 0.0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                
                # æ ¹æ®å‚æ•°ç±»å‹è®¾ç½®æƒé‡å› å­
                layer_weight = 1.0
                if 'weight' in param_name:
                    layer_weight = 2.0  # æƒé‡æ¯”åç½®æ›´é‡è¦
                if 'fc' in param_name or '4.' in param_name:  # è¾“å‡ºå±‚
                    layer_weight *= 1.5  # è¾“å‡ºå±‚æ›´é‡è¦
                
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    if len(chunk_slice) > 0:
                        # è®¡ç®—å‚æ•°çš„æ•æ„Ÿåº¦æŒ‡æ ‡
                        abs_values = np.abs(chunk_slice)
                        
                        # 1. å¤§å¹…åº¦å‚æ•°çš„é‡è¦æ€§
                        magnitude_score = np.sum(abs_values)
                        
                        # 2. å‚æ•°åˆ†æ•£ç¨‹åº¦ï¼ˆæ–¹å·®ï¼‰
                        variance_score = np.var(abs_values) + 1e-8
                        
                        # 3. éé›¶å‚æ•°æ¯”ä¾‹ï¼ˆç¨€ç–æ€§è€ƒè™‘ï¼‰
                        non_zero_ratio = np.count_nonzero(abs_values) / len(abs_values)
                        
                        # SNIPç»¼åˆè¯„åˆ†ï¼šç»“åˆå¹…åº¦ã€æ–¹å·®å’Œç¨€ç–æ€§
                        chunk_score = magnitude_score * (1 + np.sqrt(variance_score)) * (0.5 + non_zero_ratio)
                        total_snip_score += chunk_score * layer_weight
        
        return total_snip_score
    
    def _compute_fisher_importance(self, params: Dict[str, np.ndarray], chunk_info: Dict) -> float:
        """åŸºäºFisherä¿¡æ¯çŸ©é˜µçš„é‡è¦åº¦è®¡ç®—"""
        total_variance = 0.0
        total_elements = 0
        
        for param_name, parts in chunk_info['parts'].items():
            if param_name in params:
                param_array = params[param_name].flatten()
                for flat_start, flat_end, _ in parts:
                    chunk_slice = param_array[flat_start:flat_end]
                    if len(chunk_slice) > 0:
                        variance = np.var(chunk_slice) + 1e-8
                        total_variance += variance * len(chunk_slice)
                        total_elements += len(chunk_slice)
        
        return total_variance / (total_elements + 1e-8)

def create_test_model():
    """åˆ›å»ºæµ‹è¯•æ¨¡å‹"""
    model = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 10),
        nn.ReLU(),
        nn.Linear(10, 1)
    )
    return model

def simulate_training_effect(params: Dict[str, np.ndarray], scenario: str = "normal"):
    """æ¨¡æ‹Ÿè®­ç»ƒæ•ˆæœï¼Œè®©ä¸åŒå±‚çš„å‚æ•°æœ‰ä¸åŒçš„é‡è¦æ€§"""
    modified_params = {}
    
    for name, param in params.items():
        if scenario == "normal":
            # æ­£å¸¸æƒ…å†µï¼šä¸åŒå±‚æœ‰ä¸åŒçš„é‡è¦æ€§
            if "0.weight" in name:  # ç¬¬ä¸€å±‚æƒé‡
                modified_params[name] = param * 2.0  # æ›´é‡è¦
            elif "0.bias" in name:  # ç¬¬ä¸€å±‚åç½®
                modified_params[name] = param * 0.1  # è¾ƒä¸é‡è¦
            elif "2.weight" in name:  # ç¬¬äºŒå±‚æƒé‡
                modified_params[name] = param * 1.5  # ä¸­ç­‰é‡è¦
            elif "4.weight" in name:  # è¾“å‡ºå±‚æƒé‡
                modified_params[name] = param * 3.0  # æœ€é‡è¦
            else:
                modified_params[name] = param
                
        elif scenario == "sparse":
            # ç¨€ç–æƒ…å†µï¼šæŸäº›å‚æ•°è¢«å‰ªæ(è®¾ä¸º0)
            if "weight" in name:
                mask = np.random.random(param.shape) > 0.7  # 30%çš„æƒé‡ä¿ç•™
                modified_params[name] = param * mask
            else:
                modified_params[name] = param
                
        elif scenario == "diverse":
            # å¤šæ ·åŒ–æƒ…å†µï¼šå‚æ•°æœ‰å¾ˆå¤§å·®å¼‚
            scale = np.random.uniform(0.1, 5.0)
            if np.random.random() > 0.5:
                # æ·»åŠ ä¸€äº›å™ªå£°è®©å‚æ•°åˆ†å¸ƒæ›´ä¸å‡åŒ€
                noise = np.random.normal(0, 0.1, param.shape)
                modified_params[name] = param * scale + noise
            else:
                modified_params[name] = param * scale
        else:
            modified_params[name] = param
            
    return modified_params

def split_model(params: Dict[str, np.ndarray], num_chunks: int) -> List[Dict]:
    """å°†æ¨¡å‹å‚æ•°åˆ†å‰²ä¸ºchunks"""
    total_elements = sum(np.prod(v.shape) for v in params.values())
    elements_per_chunk = total_elements // num_chunks

    chunks = []
    current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
    
    for key in sorted(params.keys()):
        arr = params[key]
        n = int(np.prod(arr.shape))
        ptr = 0
        while ptr < n:
            if current_chunk['flat_size'] >= elements_per_chunk and len(chunks) < num_chunks - 1:
                chunks.append(current_chunk)
                current_chunk = {'parts': {}, 'flat_size': 0, 'chunk_id': len(chunks)}
            
            remaining_in_param = n - ptr
            remaining_in_chunk = elements_per_chunk - current_chunk['flat_size']
            take = min(remaining_in_param, remaining_in_chunk)
            
            if len(chunks) == num_chunks - 1:
                take = remaining_in_param
            
            flat_start = ptr
            flat_end = ptr + take
            
            if key not in current_chunk['parts']:
                current_chunk['parts'][key] = []
            current_chunk['parts'][key].append((flat_start, flat_end, arr.shape))
            
            current_chunk['flat_size'] += take
            ptr += take
    
    if current_chunk['parts']:
        chunks.append(current_chunk)
    
    return chunks

def test_importance_algorithms():
    """æµ‹è¯•æ‰€æœ‰é‡è¦åº¦ç®—æ³•"""
    print("ğŸ§ª chunké‡è¦åº¦ç®—æ³•ç‹¬ç«‹æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    model = create_test_model()
    
    # æå–å‚æ•°
    params = {}
    for name, param in model.named_parameters():
        params[name] = param.detach().numpy()
    
    print(f"ğŸ“‹ æ¨¡å‹å‚æ•°ä¿¡æ¯:")
    for name, param in params.items():
        print(f"   {name}: {param.shape} (å…ƒç´ æ•°: {np.prod(param.shape)})")
    
    tester = TestChunkImportance()
    
    # æµ‹è¯•ä¸åŒåœºæ™¯
    scenarios = ['normal', 'sparse', 'diverse']
    methods = ['magnitude', 'l2_norm', 'snip', 'fisher']
    
    for scenario in scenarios:
        print(f"\n{'='*20} åœºæ™¯: {scenario} {'='*20}")
        
        # ä¿®æ”¹å‚æ•°æ¨¡æ‹Ÿä¸åŒåœºæ™¯
        modified_params = simulate_training_effect(params, scenario)
        
        # åˆ†å‰²æˆchunks
        num_chunks = 5
        chunks_info = split_model(modified_params, num_chunks)
        
        print(f"ğŸ“¦ åˆ†å‰²æˆ {len(chunks_info)} ä¸ªchunks:")
        for i, chunk in enumerate(chunks_info):
            print(f"   Chunk {i}: {chunk['flat_size']} å…ƒç´ ")
        
        # æµ‹è¯•æ¯ç§æ–¹æ³•
        for method in methods:
            print(f"\n{'-'*40}")
            scores = tester.compute_chunk_importance(modified_params, chunks_info, method)
            
            # æ£€æŸ¥åˆ†æ•°å·®å¼‚
            max_score = max(scores)
            min_score = min(scores)
            ratio = max_score / min_score if min_score > 0 else float('inf')
            print(f"ğŸ“Š é‡è¦åº¦å·®å¼‚: æœ€å¤§/æœ€å° = {ratio:.2f}")
            
            if ratio > 2.0:
                print("âœ… ç®—æ³•èƒ½å¤ŸåŒºåˆ†ä¸åŒchunkçš„é‡è¦æ€§")
            else:
                print("âŒ ç®—æ³•æœªèƒ½æœ‰æ•ˆåŒºåˆ†chunké‡è¦æ€§")

def visualize_importance_distribution():
    """å¯è§†åŒ–é‡è¦åº¦åˆ†å¸ƒ"""
    try:
        import matplotlib.pyplot as plt
        
        model = create_test_model()
        params = {}
        for name, param in model.named_parameters():
            params[name] = param.detach().numpy()
        
        # åˆ›å»ºå¤šæ ·åŒ–çš„å‚æ•°
        diverse_params = simulate_training_effect(params, 'diverse')
        chunks_info = split_model(diverse_params, 8)
        
        tester = TestChunkImportance()
        methods = ['magnitude', 'l2_norm', 'snip', 'fisher']
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        axes = axes.flatten()
        
        for i, method in enumerate(methods):
            scores = tester.compute_chunk_importance(diverse_params, chunks_info, method)
            
            axes[i].bar(range(len(scores)), scores)
            axes[i].set_title(f'{method.upper()} é‡è¦åº¦åˆ†å¸ƒ')
            axes[i].set_xlabel('Chunk ID')
            axes[i].set_ylabel('é‡è¦åº¦åˆ†æ•°')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('chunk_importance_distribution.png', dpi=150, bbox_inches='tight')
        print("ğŸ“Š é‡è¦åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜: chunk_importance_distribution.png")
        
    except ImportError:
        print("âš ï¸ matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å¯è§†åŒ–")

if __name__ == "__main__":
    test_importance_algorithms()
    visualize_importance_distribution()