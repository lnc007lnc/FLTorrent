#!/usr/bin/env python3
"""
æµ‹è¯•save_remote_chunkå‡½æ•°
æ¨¡æ‹Ÿchunkæ•°æ®ï¼ŒéªŒè¯å‡½æ•°èƒ½å¦æˆåŠŸå†™å…¥
"""

import sqlite3
import os
import hashlib
import pickle
import numpy as np
import sys
import tempfile
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥å¯¼å…¥ChunkManager
sys.path.append('/mnt/g/FLtorrent_combine/FederatedScope-master')

def test_save_remote_chunk_with_real_manager():
    """ä½¿ç”¨çœŸå®çš„ChunkManageræµ‹è¯•save_remote_chunk"""
    print("ğŸ§ª ä½¿ç”¨çœŸå®ChunkManageræµ‹è¯•save_remote_chunk")
    print("=" * 80)
    
    try:
        from federatedscope.core.chunk_manager import ChunkManager
        
        # åˆå§‹åŒ–ChunkManager (å®ƒä¼šè‡ªåŠ¨åˆ›å»ºæ•°æ®åº“)
        chunk_manager = ChunkManager(client_id=999)  # ä½¿ç”¨ç‰¹æ®ŠIDé¿å…å†²çª
        test_db_path = chunk_manager.db_path
        
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®åº“: {test_db_path}")
        
        # æµ‹è¯•å„ç§ç±»å‹çš„chunkæ•°æ®
        test_scenarios = [
            ("ç©ºnumpyæ•°ç»„", np.array([])),
            ("å°numpyæ•°ç»„", np.random.rand(5).astype(np.float32)),
            ("å¤§numpyæ•°ç»„", np.random.rand(100, 50).astype(np.float32)),
            ("æ•´æ•°æ•°ç»„", np.array([1, 2, 3, 4, 5])),
            ("å­—å…¸æ•°æ®", {'weights': np.random.rand(3), 'bias': np.array([1.0])}),
            ("åˆ—è¡¨æ•°æ®", [1.0, 2.0, 3.0, 4.0]),
            ("å­—ç¬¦ä¸²æ•°æ®", "test_chunk_data"),
            ("Noneæ•°æ®", None)
        ]
        
        successful_saves = 0
        
        for i, (scenario_name, test_data) in enumerate(test_scenarios, 1):
            print(f"\nğŸ” æµ‹è¯•åœºæ™¯ {i}: {scenario_name}")
            print(f"   æ•°æ®ç±»å‹: {type(test_data)}")
            
            if test_data is not None and hasattr(test_data, 'shape'):
                print(f"   æ•°æ®å½¢çŠ¶: {test_data.shape}")
            elif test_data is not None and hasattr(test_data, '__len__'):
                print(f"   æ•°æ®é•¿åº¦: {len(test_data)}")
            
            try:
                # è°ƒç”¨save_remote_chunk
                chunk_manager.save_remote_chunk(
                    round_num=0,
                    source_client_id=2, 
                    chunk_id=i,
                    chunk_data=test_data
                )
                
                print(f"   âœ… è°ƒç”¨æˆåŠŸ")
                
                # éªŒè¯æ˜¯å¦çœŸçš„ä¿å­˜åˆ°æ•°æ®åº“
                conn = sqlite3.connect(test_db_path)
                cursor = conn.cursor()
                
                # æ£€æŸ¥bt_chunksè¡¨
                cursor.execute("""
                    SELECT chunk_hash FROM bt_chunks 
                    WHERE round_num=0 AND source_client_id=2 AND chunk_id=?
                """, (i,))
                bt_result = cursor.fetchone()
                
                if bt_result:
                    chunk_hash = bt_result[0]
                    print(f"   ğŸ“‹ bt_chunksè®°å½•: âœ…å­˜åœ¨ (å“ˆå¸Œ: {chunk_hash[:16]}...)")
                    
                    # æ£€æŸ¥chunk_dataè¡¨
                    cursor.execute("SELECT data FROM chunk_data WHERE chunk_hash=?", (chunk_hash,))
                    data_result = cursor.fetchone()
                    
                    if data_result:
                        print(f"   ğŸ’¾ chunk_dataè®°å½•: âœ…å­˜åœ¨ (å¤§å°: {len(data_result[0])} bytes)")
                        
                        # å°è¯•ååºåˆ—åŒ–éªŒè¯
                        try:
                            if test_data is not None:
                                restored_data = pickle.loads(data_result[0])
                                print(f"   ğŸ”„ ååºåˆ—åŒ–: âœ…æˆåŠŸ (ç±»å‹: {type(restored_data)})")
                                
                                # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
                                if isinstance(test_data, np.ndarray) and isinstance(restored_data, np.ndarray):
                                    is_equal = np.array_equal(test_data, restored_data)
                                    print(f"   ğŸ“Š æ•°æ®ä¸€è‡´æ€§: {'âœ…ä¸€è‡´' if is_equal else 'âŒä¸ä¸€è‡´'}")
                                elif test_data == restored_data:
                                    print(f"   ğŸ“Š æ•°æ®ä¸€è‡´æ€§: âœ…ä¸€è‡´")
                                else:
                                    print(f"   ğŸ“Š æ•°æ®ä¸€è‡´æ€§: âŒä¸ä¸€è‡´")
                            else:
                                # Noneæ•°æ®çš„ç‰¹æ®Šå¤„ç†
                                restored_data = pickle.loads(data_result[0])
                                print(f"   ğŸ”„ ååºåˆ—åŒ–: âœ…æˆåŠŸ (Noneæ•°æ®)")
                                print(f"   ğŸ“Š æ•°æ®ä¸€è‡´æ€§: {'âœ…ä¸€è‡´' if restored_data is None else 'âŒä¸ä¸€è‡´'}")
                            
                            successful_saves += 1
                            
                        except Exception as e:
                            print(f"   ğŸ”„ ååºåˆ—åŒ–: âŒå¤±è´¥ ({e})")
                    else:
                        print(f"   ğŸ’¾ chunk_dataè®°å½•: âŒä¸å­˜åœ¨ï¼")
                else:
                    print(f"   ğŸ“‹ bt_chunksè®°å½•: âŒä¸å­˜åœ¨ï¼")
                
                conn.close()
                
            except Exception as e:
                print(f"   âŒ è°ƒç”¨å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"   æˆåŠŸä¿å­˜: {successful_saves}/{len(test_scenarios)}")
        print(f"   æˆåŠŸç‡: {successful_saves/len(test_scenarios)*100:.1f}%")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®åº“å’Œç›®å½•
        if os.path.exists(test_db_path):
            os.unlink(test_db_path)
            print(f"ğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®åº“")
        
        # æ¸…ç†æµ‹è¯•ç›®å½•
        test_dir = os.path.dirname(test_db_path)
        if os.path.exists(test_dir) and "client_999" in test_dir:
            import shutil
            shutil.rmtree(test_dir)
            print(f"ğŸ§¹ æ¸…ç†æµ‹è¯•ç›®å½•")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥ChunkManagerå¤±è´¥: {e}")
        return False
        
    return successful_saves > 0

def test_with_actual_database():
    """ä½¿ç”¨å®é™…æ•°æ®åº“æµ‹è¯•save_remote_chunk"""
    print(f"\n{'='*80}")
    print("ğŸ” ä½¿ç”¨å®é™…æ•°æ®åº“æµ‹è¯•save_remote_chunk")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_1_db = os.path.join(base_path, "client_1", "client_1_chunks.db")
    
    if not os.path.exists(client_1_db):
        print(f"âŒ å®é™…æ•°æ®åº“ä¸å­˜åœ¨: {client_1_db}")
        return
    
    # å¤‡ä»½åŸæ•°æ®åº“
    backup_db = client_1_db + ".backup"
    import shutil
    shutil.copy2(client_1_db, backup_db)
    print(f"ğŸ“‹ å·²å¤‡ä»½æ•°æ®åº“åˆ°: {backup_db}")
    
    try:
        from federatedscope.core.chunk_manager import ChunkManager
        
        # ä½¿ç”¨å®é™…æ•°æ®åº“è·¯å¾„ï¼Œä½†æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è®¾ç½®
        # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªhackï¼Œæ­£å¸¸ChunkManagerä¼šè‡ªåŠ¨ç”Ÿæˆè·¯å¾„
        chunk_manager = ChunkManager(client_id=1)
        chunk_manager.db_path = client_1_db  # å¼ºåˆ¶è®¾ç½®ä¸ºå®é™…æ•°æ®åº“è·¯å¾„
        
        # ç”Ÿæˆæµ‹è¯•chunkæ•°æ®
        test_chunk_data = np.random.rand(10, 5).astype(np.float32)
        test_hash = hashlib.sha256(pickle.dumps(test_chunk_data)).hexdigest()
        
        print(f"ğŸ§ª æµ‹è¯•chunkæ•°æ®:")
        print(f"   ç±»å‹: {type(test_chunk_data)}")
        print(f"   å½¢çŠ¶: {test_chunk_data.shape}")  
        print(f"   é¢„æœŸå“ˆå¸Œ: {test_hash[:16]}...")
        
        # è®°å½•ä¿å­˜å‰çš„çŠ¶æ€
        conn = sqlite3.connect(client_1_db)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM bt_chunks")
        bt_count_before = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM chunk_data")
        data_count_before = cursor.fetchone()[0]
        
        conn.close()
        
        print(f"ğŸ“Š ä¿å­˜å‰çŠ¶æ€:")
        print(f"   bt_chunksè®°å½•æ•°: {bt_count_before}")
        print(f"   chunk_dataè®°å½•æ•°: {data_count_before}")
        
        # è°ƒç”¨save_remote_chunk
        print(f"\nğŸ”§ è°ƒç”¨save_remote_chunk...")
        chunk_manager.save_remote_chunk(
            round_num=99,  # ä½¿ç”¨ç‰¹æ®Šè½®æ¬¡é¿å…ä¸ç°æœ‰æ•°æ®å†²çª
            source_client_id=99,
            chunk_id=99,
            chunk_data=test_chunk_data
        )
        
        # æ£€æŸ¥ä¿å­˜åçš„çŠ¶æ€
        conn = sqlite3.connect(client_1_db)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM bt_chunks")
        bt_count_after = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM chunk_data")
        data_count_after = cursor.fetchone()[0]
        
        print(f"\nğŸ“Š ä¿å­˜åçŠ¶æ€:")
        print(f"   bt_chunksè®°å½•æ•°: {bt_count_after} (å¢åŠ : {bt_count_after - bt_count_before})")
        print(f"   chunk_dataè®°å½•æ•°: {data_count_after} (å¢åŠ : {data_count_after - data_count_before})")
        
        # éªŒè¯å…·ä½“è®°å½•
        cursor.execute("""
            SELECT chunk_hash FROM bt_chunks 
            WHERE round_num=99 AND source_client_id=99 AND chunk_id=99
        """)
        bt_result = cursor.fetchone()
        
        if bt_result:
            actual_hash = bt_result[0]
            print(f"âœ… bt_chunksè®°å½•å·²ä¿å­˜")
            print(f"   å®é™…å“ˆå¸Œ: {actual_hash}")
            print(f"   å“ˆå¸ŒåŒ¹é…: {'âœ…æ˜¯' if actual_hash == test_hash else 'âŒå¦'}")
            
            # æ£€æŸ¥chunk_data
            cursor.execute("SELECT data FROM chunk_data WHERE chunk_hash=?", (actual_hash,))
            data_result = cursor.fetchone()
            
            if data_result:
                print(f"âœ… chunk_dataè®°å½•å·²ä¿å­˜")
                print(f"   æ•°æ®å¤§å°: {len(data_result[0])} bytes")
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                try:
                    restored_data = pickle.loads(data_result[0])
                    is_equal = np.array_equal(test_chunk_data, restored_data)
                    print(f"   æ•°æ®å®Œæ•´æ€§: {'âœ…æ­£ç¡®' if is_equal else 'âŒæŸå'}")
                    
                    if is_equal:
                        print(f"ğŸ‰ save_remote_chunkå‡½æ•°å·¥ä½œæ­£å¸¸ï¼")
                    else:
                        print(f"âŒ æ•°æ®ä¿å­˜å­˜åœ¨é—®é¢˜")
                        
                except Exception as e:
                    print(f"âŒ æ•°æ®ååºåˆ—åŒ–å¤±è´¥: {e}")
            else:
                print(f"âŒ chunk_dataè®°å½•æœªä¿å­˜ï¼")
        else:
            print(f"âŒ bt_chunksè®°å½•æœªä¿å­˜ï¼")
        
        conn.close()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¢å¤åŸæ•°æ®åº“
        if os.path.exists(backup_db):
            shutil.move(backup_db, client_1_db)
            print(f"ğŸ”„ å·²æ¢å¤åŸæ•°æ®åº“")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•save_remote_chunkå‡½æ•°")
    
    # æµ‹è¯•1: ä½¿ç”¨ä¸´æ—¶æ•°æ®åº“
    success = test_save_remote_chunk_with_real_manager()
    
    if success:
        # æµ‹è¯•2: ä½¿ç”¨å®é™…æ•°æ®åº“
        test_with_actual_database()
    
    print(f"\n{'='*80}")
    print("ğŸ¯ æµ‹è¯•ç»“è®º:")
    if success:
        print("âœ… save_remote_chunkå‡½æ•°é€»è¾‘æ­£å¸¸")
        print("ğŸ” å¦‚æœå®é™…è¿è¡Œæ—¶æ•°æ®æœªä¿å­˜ï¼Œé—®é¢˜å¯èƒ½åœ¨äº:")
        print("   1. ä¼ å…¥çš„chunk_dataå‚æ•°ä¸ºç©ºæˆ–æ— æ•ˆ")
        print("   2. BitTorrentæ¥æ”¶è¿‡ç¨‹ä¸­æ•°æ®ä¸¢å¤±")  
        print("   3. å‡½æ•°å®é™…æœªè¢«è°ƒç”¨ï¼ˆå°½ç®¡bt_chunksæœ‰è®°å½•ï¼‰")
        print("   4. æ•°æ®åº“å†™å…¥æƒé™æˆ–é”å®šé—®é¢˜")
    else:
        print("âŒ save_remote_chunkå‡½æ•°å­˜åœ¨é—®é¢˜")

if __name__ == "__main__":
    main()