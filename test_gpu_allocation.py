#!/usr/bin/env python3
"""
GPUèµ„æºåˆ†é…æµ‹è¯•è„šæœ¬
éªŒè¯Rayæ˜¯å¦æ­£ç¡®æ£€æµ‹ç¡¬ä»¶èµ„æºå¹¶æŒ‰éœ€åˆ†é…
"""

import ray
import psutil
import time
import logging
from typing import Optional

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def detect_system_resources():
    """æ£€æµ‹ç³»ç»Ÿç¡¬ä»¶èµ„æº"""
    # CPUèµ„æº
    num_cpus = psutil.cpu_count()
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    # GPUèµ„æºæ£€æµ‹
    num_gpus = 0
    gpu_info = "æ— GPU"
    try:
        import torch
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            gpu_names = [torch.cuda.get_device_name(i) for i in range(num_gpus)]
            gpu_info = f"{num_gpus}ä¸ªGPU: {', '.join(gpu_names)}"
    except ImportError:
        pass
    
    return {
        "cpus": num_cpus,
        "memory_gb": memory_gb,
        "gpus": num_gpus,
        "gpu_info": gpu_info
    }

@ray.remote
class GPUTestActor:
    """GPUæµ‹è¯•Actor"""
    
    def __init__(self, actor_id: int, role: str):
        self.actor_id = actor_id
        self.role = role
        
    def test_gpu_access(self):
        """æµ‹è¯•GPUè®¿é—®"""
        import os
        result = {
            "actor_id": self.actor_id,
            "role": self.role,
            "node_ip": ray.util.get_node_ip_address(),
            "cuda_visible_devices": os.environ.get("CUDA_VISIBLE_DEVICES", "æœªè®¾ç½®"),
            "gpu_available": False,
            "gpu_count": 0,
            "gpu_devices": []
        }
        
        try:
            import torch
            if torch.cuda.is_available():
                result["gpu_available"] = True
                result["gpu_count"] = torch.cuda.device_count()
                result["gpu_devices"] = [
                    torch.cuda.get_device_name(i) for i in range(result["gpu_count"])
                ]
        except ImportError:
            result["error"] = "PyTorchæœªå®‰è£…"
        except Exception as e:
            result["error"] = str(e)
            
        return result

def test_ray_gpu_allocation():
    """æµ‹è¯•Ray GPUèµ„æºåˆ†é…"""
    
    # 1. æ£€æµ‹ç³»ç»Ÿèµ„æº
    system_resources = detect_system_resources()
    logger.info(f"ğŸ–¥ï¸ ç³»ç»Ÿèµ„æº: CPU={system_resources['cpus']}, å†…å­˜={system_resources['memory_gb']:.1f}GB, {system_resources['gpu_info']}")
    
    # 2. åˆå§‹åŒ–Ray
    ray_config = {
        "num_cpus": system_resources["cpus"],
        "num_gpus": system_resources["gpus"],
        "ignore_reinit_error": True
    }
    
    ray.init(**ray_config)
    
    # 3. æŸ¥çœ‹Rayé›†ç¾¤èµ„æº
    cluster_resources = ray.cluster_resources()
    available_resources = ray.available_resources()
    
    logger.info(f"â˜ï¸ Rayé›†ç¾¤èµ„æº: {dict(cluster_resources)}")
    logger.info(f"ğŸ’¾ å¯ç”¨èµ„æº: {dict(available_resources)}")
    
    # 4. æ¨¡æ‹ŸFederatedScopeçš„GPUåˆ†é…ç­–ç•¥
    total_gpus = int(cluster_resources.get('GPU', 0))
    client_num = 5
    
    # æœåŠ¡å™¨ï¼šå¼ºåˆ¶ä½¿ç”¨CPU
    server_resources = {"num_cpus": 2}
    logger.info(f"ğŸ–¥ï¸ æœåŠ¡å™¨èµ„æºé…ç½®: {server_resources}")
    
    # å®¢æˆ·ç«¯ï¼šæ ¹æ®è®¾å¤‡ç±»å‹åˆ†é…GPU
    device_types = ["smartphone_high", "smartphone_low", "raspberry_pi", "iot_device", "edge_server"]
    client_configs = []
    
    gpu_assigned = 0
    for i in range(client_num):
        client_id = i + 1
        device_type = device_types[i % len(device_types)]
        
        # åŸºç¡€èµ„æºé…ç½®
        if device_type == "smartphone_high" or device_type == "edge_server":
            client_resources = {"num_cpus": 1.0}
            # åˆ†é…GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if gpu_assigned < total_gpus:
                client_resources["num_gpus"] = 1
                gpu_assigned += 1
                gpu_status = f"GPU-{gpu_assigned-1}"
            else:
                gpu_status = "CPU"
        else:
            client_resources = {"num_cpus": 0.5}
            gpu_status = "CPU"
            
        client_configs.append({
            "client_id": client_id,
            "device_type": device_type,
            "resources": client_resources,
            "gpu_status": gpu_status
        })
        
        logger.info(f"ğŸ“± å®¢æˆ·ç«¯{client_id} ({device_type}): {client_resources}, ä½¿ç”¨{gpu_status}")
    
    # 5. åˆ›å»ºæµ‹è¯•Actors
    test_actors = []
    
    # åˆ›å»ºæœåŠ¡å™¨Actor
    try:
        server_actor = GPUTestActor.options(**server_resources).remote(0, "server")
        test_actors.append(("server", server_actor))
        logger.info("âœ… æœåŠ¡å™¨Actoråˆ›å»ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡å™¨Actoråˆ›å»ºå¤±è´¥: {e}")
    
    # åˆ›å»ºå®¢æˆ·ç«¯Actors
    for config in client_configs:
        try:
            client_actor = GPUTestActor.options(**config["resources"]).remote(
                config["client_id"], f"client_{config['device_type']}"
            )
            test_actors.append((f"client_{config['client_id']}", client_actor))
            logger.info(f"âœ… å®¢æˆ·ç«¯{config['client_id']} Actoråˆ›å»ºæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ å®¢æˆ·ç«¯{config['client_id']} Actoråˆ›å»ºå¤±è´¥: {e}")
    
    # 6. æµ‹è¯•GPUè®¿é—®
    logger.info("\nğŸ§ª å¼€å§‹GPUè®¿é—®æµ‹è¯•...")
    time.sleep(2)  # ç­‰å¾…Actorsåˆå§‹åŒ–
    
    gpu_test_futures = [actor.test_gpu_access.remote() for name, actor in test_actors]
    gpu_test_results = ray.get(gpu_test_futures)
    
    # 7. åˆ†æç»“æœ
    logger.info("\nğŸ“Š GPUè®¿é—®æµ‹è¯•ç»“æœ:")
    server_gpu_count = 0
    client_gpu_count = 0
    
    for result in gpu_test_results:
        role = result["role"]
        gpu_info = f"GPUæ•°é‡: {result['gpu_count']}" if result['gpu_available'] else "æ— GPUè®¿é—®"
        
        if "server" in role:
            server_gpu_count += result['gpu_count']
            logger.info(f"ğŸ–¥ï¸ {role}: {gpu_info}")
        else:
            client_gpu_count += result['gpu_count']
            logger.info(f"ğŸ“± {role}: {gpu_info}")
    
    # 8. éªŒè¯åˆ†é…ç­–ç•¥
    logger.info(f"\nğŸ¯ GPUèµ„æºåˆ†é…éªŒè¯:")
    logger.info(f"   æœåŠ¡å™¨GPUä½¿ç”¨: {server_gpu_count} (é¢„æœŸ: 0)")
    logger.info(f"   å®¢æˆ·ç«¯GPUä½¿ç”¨: {client_gpu_count} (é¢„æœŸ: {min(total_gpus, gpu_assigned)})")
    
    allocation_correct = (server_gpu_count == 0) and (client_gpu_count <= total_gpus)
    status = "âœ… æ­£ç¡®" if allocation_correct else "âŒ é”™è¯¯"
    
    logger.info(f"   åˆ†é…ç­–ç•¥: {status}")
    
    return {
        "system_resources": system_resources,
        "ray_resources": dict(cluster_resources),
        "server_gpu_count": server_gpu_count,
        "client_gpu_count": client_gpu_count,
        "allocation_correct": allocation_correct,
        "detailed_results": gpu_test_results
    }

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹GPUèµ„æºåˆ†é…æµ‹è¯•")
    
    try:
        results = test_ray_gpu_allocation()
        
        logger.info("\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
        logger.info(f"   ç³»ç»ŸGPUæ•°é‡: {results['system_resources']['gpus']}")
        logger.info(f"   Rayæ£€æµ‹GPUæ•°é‡: {results['ray_resources'].get('GPU', 0)}")
        logger.info(f"   æœåŠ¡å™¨GPUä½¿ç”¨: {results['server_gpu_count']}")
        logger.info(f"   å®¢æˆ·ç«¯GPUä½¿ç”¨: {results['client_gpu_count']}")
        logger.info(f"   åˆ†é…ç­–ç•¥æ­£ç¡®æ€§: {'âœ…' if results['allocation_correct'] else 'âŒ'}")
        
        if results['allocation_correct']:
            logger.info("\nğŸ‰ GPUèµ„æºåˆ†é…æµ‹è¯•é€šè¿‡ï¼")
        else:
            logger.info("\nâš ï¸ GPUèµ„æºåˆ†é…éœ€è¦è°ƒä¼˜")
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        ray.shutdown()
        logger.info("ğŸ§¹ Rayé›†ç¾¤å·²å…³é—­")

if __name__ == "__main__":
    main()