#!/usr/bin/env python3
"""
åˆ†æå®é™…å­˜å‚¨çš„chunkæ•°æ® vs é¢„æœŸçš„chunkæ•°æ®
æ‰¾å‡ºchunk_dataè¡¨ä¸­çš„æ•°æ®åˆ°åº•æ˜¯ä»€ä¹ˆ
"""

import sqlite3
import os
import hashlib
import pickle
import numpy as np
from collections import defaultdict

def analyze_stored_chunks():
    """åˆ†æå®é™…å­˜å‚¨çš„chunkæ•°æ®"""
    print("ğŸ” åˆ†æchunk_dataè¡¨ä¸­å®é™…å­˜å‚¨çš„æ•°æ®")
    print("=" * 80)
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    
    stored_data_analysis = {}
    
    for client_id in [1, 2, 3]:
        db_path = os.path.join(base_path, f"client_{client_id}", f"client_{client_id}_chunks.db")
        print(f"\nğŸ“Š å®¢æˆ·ç«¯ {client_id} è¯¦ç»†åˆ†æ:")
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # è·å–chunk_dataè¯¦ç»†ä¿¡æ¯
        cursor.execute("""
            SELECT chunk_hash, data, created_at
            FROM chunk_data
            ORDER BY created_at
        """)
        
        stored_records = cursor.fetchall()
        client_analysis = []
        
        for i, (chunk_hash, data_blob, created_at) in enumerate(stored_records, 1):
            print(f"\n   ğŸ“‹ è®°å½• {i}:")
            print(f"      å“ˆå¸Œ: {chunk_hash}")
            print(f"      åˆ›å»ºæ—¶é—´: {created_at}")
            print(f"      æ•°æ®å¤§å°: {len(data_blob)} bytes")
            
            try:
                # ååºåˆ—åŒ–æ•°æ®
                unpickled_data = pickle.loads(data_blob)
                print(f"      æ•°æ®ç±»å‹: {type(unpickled_data)}")
                
                if isinstance(unpickled_data, np.ndarray):
                    print(f"      numpyå½¢çŠ¶: {unpickled_data.shape}")
                    print(f"      numpy dtype: {unpickled_data.dtype}")
                    print(f"      å…ƒç´ æ•°é‡: {unpickled_data.size}")
                    if unpickled_data.size > 0:
                        print(f"      æ•°æ®èŒƒå›´: [{unpickled_data.min():.6f}, {unpickled_data.max():.6f}]")
                        print(f"      æ•°æ®æ ·æœ¬: {unpickled_data.flatten()[:5]}...")
                    else:
                        print(f"      ç©ºnumpyæ•°ç»„")
                elif hasattr(unpickled_data, '__len__'):
                    print(f"      æ•°æ®é•¿åº¦: {len(unpickled_data)}")
                    if len(unpickled_data) > 0:
                        print(f"      æ•°æ®å†…å®¹: {str(unpickled_data)[:100]}...")
                
                client_analysis.append({
                    'hash': chunk_hash,
                    'data': unpickled_data,
                    'type': type(unpickled_data),
                    'created_at': created_at
                })
                
            except Exception as e:
                print(f"      âŒ ååºåˆ—åŒ–å¤±è´¥: {e}")
                client_analysis.append({
                    'hash': chunk_hash,
                    'error': str(e),
                    'created_at': created_at
                })
        
        stored_data_analysis[client_id] = client_analysis
        
        # æ£€æŸ¥è¿™äº›chunk_hashæ˜¯å¦åœ¨bt_chunksæˆ–chunk_metadataä¸­
        print(f"\n   ğŸ”— æ•°æ®æ¥æºåˆ†æ:")
        for record in client_analysis:
            chunk_hash = record['hash']
            
            # æ£€æŸ¥æ˜¯å¦åœ¨bt_chunksä¸­ï¼ˆæ¥è‡ªBitTorrentäº¤æ¢ï¼‰
            cursor.execute("SELECT COUNT(*) FROM bt_chunks WHERE chunk_hash = ?", (chunk_hash,))
            bt_count = cursor.fetchone()[0]
            
            # æ£€æŸ¥æ˜¯å¦åœ¨chunk_metadataä¸­ï¼ˆæœ¬åœ°ç”Ÿæˆï¼‰
            cursor.execute("SELECT COUNT(*) FROM chunk_metadata WHERE chunk_hash = ?", (chunk_hash,))
            meta_count = cursor.fetchone()[0]
            
            print(f"      å“ˆå¸Œ {chunk_hash[:16]}...: bt_chunks={bt_count}, chunk_metadata={meta_count}")
            
            if bt_count > 0 and meta_count == 0:
                print(f"        ğŸ“¥ æ¥æºï¼šBitTorrentäº¤æ¢")
            elif bt_count == 0 and meta_count > 0:
                print(f"        ğŸ  æ¥æºï¼šæœ¬åœ°ç”Ÿæˆ")
            elif bt_count > 0 and meta_count > 0:
                print(f"        ğŸ”„ æ¥æºï¼šæœ¬åœ°ç”Ÿæˆä¸”é€šè¿‡BitTorrentäº¤æ¢")
            else:
                print(f"        â“ æ¥æºï¼šæœªçŸ¥")
        
        conn.close()
    
    return stored_data_analysis

def compare_across_clients(stored_data_analysis):
    """æ¯”è¾ƒå®¢æˆ·ç«¯é—´å­˜å‚¨çš„æ•°æ®"""
    print(f"\nğŸ” è·¨å®¢æˆ·ç«¯æ•°æ®æ¯”è¾ƒ")
    print("=" * 60)
    
    # æ”¶é›†æ‰€æœ‰å“ˆå¸Œ
    all_hashes = set()
    for client_data in stored_data_analysis.values():
        for record in client_data:
            if 'hash' in record:
                all_hashes.add(record['hash'])
    
    print(f"æ€»å…±å‘ç° {len(all_hashes)} ä¸ªä¸åŒçš„å“ˆå¸Œ")
    
    # åˆ†ææ¯ä¸ªå“ˆå¸Œåœ¨å„å®¢æˆ·ç«¯çš„æƒ…å†µ
    for chunk_hash in sorted(all_hashes):
        print(f"\nğŸ“Š å“ˆå¸Œ {chunk_hash[:16]}...:")
        
        hash_data = {}
        for client_id, client_data in stored_data_analysis.items():
            for record in client_data:
                if record.get('hash') == chunk_hash:
                    hash_data[client_id] = record
                    break
        
        if len(hash_data) > 1:
            # å¤šä¸ªå®¢æˆ·ç«¯æœ‰è¿™ä¸ªå“ˆå¸Œï¼Œæ£€æŸ¥æ•°æ®æ˜¯å¦ç›¸åŒ
            print(f"   å‡ºç°åœ¨å®¢æˆ·ç«¯: {list(hash_data.keys())}")
            
            # æ¯”è¾ƒæ•°æ®ä¸€è‡´æ€§
            first_client_data = None
            data_consistent = True
            
            for client_id, record in hash_data.items():
                if 'data' in record:
                    if first_client_data is None:
                        first_client_data = record['data']
                    else:
                        try:
                            if isinstance(first_client_data, np.ndarray) and isinstance(record['data'], np.ndarray):
                                if not np.array_equal(first_client_data, record['data']):
                                    data_consistent = False
                            elif first_client_data != record['data']:
                                data_consistent = False
                        except:
                            data_consistent = False
                
                print(f"   å®¢æˆ·ç«¯ {client_id}: {record.get('type', 'æœªçŸ¥ç±»å‹')}")
            
            print(f"   æ•°æ®ä¸€è‡´æ€§: {'âœ…ä¸€è‡´' if data_consistent else 'âŒä¸ä¸€è‡´'}")
        else:
            # åªæœ‰ä¸€ä¸ªå®¢æˆ·ç«¯æœ‰è¿™ä¸ªå“ˆå¸Œ
            client_id, record = list(hash_data.items())[0]
            print(f"   ä»…åœ¨å®¢æˆ·ç«¯ {client_id}: {record.get('type', 'æœªçŸ¥ç±»å‹')}")

def investigate_chunk_generation_pattern():
    """è°ƒæŸ¥chunkç”Ÿæˆæ¨¡å¼"""
    print(f"\nğŸ” è°ƒæŸ¥chunkç”Ÿæˆæ¨¡å¼")
    print("=" * 60)
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    
    for client_id in [1, 2, 3]:
        db_path = os.path.join(base_path, f"client_{client_id}", f"client_{client_id}_chunks.db")
        print(f"\nğŸ“‹ å®¢æˆ·ç«¯ {client_id} chunkç”Ÿæˆåˆ†æ:")
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # æŸ¥çœ‹chunk_metadataï¼ˆæœ¬åœ°ç”Ÿæˆçš„chunkï¼‰
        cursor.execute("""
            SELECT round_num, chunk_id, chunk_hash, parts_info, flat_size
            FROM chunk_metadata
            ORDER BY round_num, chunk_id
        """)
        
        local_chunks = cursor.fetchall()
        print(f"   æœ¬åœ°ç”Ÿæˆçš„chunkæ•°é‡: {len(local_chunks)}")
        
        if local_chunks:
            print("   æœ¬åœ°chunkè¯¦æƒ…:")
            for round_num, chunk_id, chunk_hash, parts_info, flat_size in local_chunks[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"     è½®æ¬¡{round_num}, å—{chunk_id}: å“ˆå¸Œ={chunk_hash[:16]}..., å¤§å°={flat_size}")
                
                # æ£€æŸ¥è¿™ä¸ªå“ˆå¸Œæ˜¯å¦åœ¨chunk_dataä¸­æœ‰æ•°æ®
                cursor.execute("SELECT COUNT(*) FROM chunk_data WHERE chunk_hash = ?", (chunk_hash,))
                data_exists = cursor.fetchone()[0] > 0
                print(f"       æ•°æ®å­˜åœ¨: {'âœ…æ˜¯' if data_exists else 'âŒå¦'}")
        
        # ç»Ÿè®¡BitTorrent chunkä¸­å“ªäº›åº”è¯¥æœ‰æ•°æ®ä½†æ²¡æœ‰
        empty_hash = hashlib.sha256(b'').hexdigest()
        cursor.execute("""
            SELECT COUNT(*) FROM bt_chunks bc
            LEFT JOIN chunk_data cd ON bc.chunk_hash = cd.chunk_hash
            WHERE bc.chunk_hash != ? AND cd.chunk_hash IS NULL
        """, (empty_hash,))
        
        missing_bt_data = cursor.fetchone()[0]
        print(f"   BitTorrentç¼ºå¤±æ•°æ®çš„éç©ºchunk: {missing_bt_data}")
        
        conn.close()

def main():
    """ä¸»å‡½æ•°"""
    # åˆ†æå­˜å‚¨çš„chunkæ•°æ®
    stored_data_analysis = analyze_stored_chunks()
    
    # æ¯”è¾ƒå®¢æˆ·ç«¯é—´çš„æ•°æ®
    compare_across_clients(stored_data_analysis)
    
    # è°ƒæŸ¥chunkç”Ÿæˆæ¨¡å¼
    investigate_chunk_generation_pattern()
    
    print(f"\n{'='*80}")
    print("ğŸ¯ æœ€ç»ˆåˆ†æç»“è®º:")
    print("1. chunk_dataè¡¨ä¸­å­˜å‚¨çš„æ˜¯ä»€ä¹ˆç±»å‹çš„æ•°æ®")
    print("2. è¿™äº›æ•°æ®æ˜¯æœ¬åœ°ç”Ÿæˆçš„è¿˜æ˜¯BitTorrentäº¤æ¢çš„")
    print("3. BitTorrentæ¥æ”¶åˆ°çš„chunkæ•°æ®æ˜¯å¦è¢«æ­£ç¡®ä¿å­˜")

if __name__ == "__main__":
    main()