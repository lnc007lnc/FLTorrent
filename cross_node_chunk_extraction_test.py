#!/usr/bin/env python3
"""
è·¨èŠ‚ç‚¹chunkæå–æµ‹è¯• - è½®æ¬¡3ã€4
éªŒè¯æ¯ä¸ªèŠ‚ç‚¹èƒ½å¦æå–åˆ°ç½‘ç»œä¸­å…¨éƒ¨chunkæ•°æ®
"""

import sqlite3
import os
import hashlib
import pickle
from collections import defaultdict

def cross_node_chunk_extraction_test():
    """è·¨èŠ‚ç‚¹chunkæå–æµ‹è¯•"""
    print("ğŸ” è·¨èŠ‚ç‚¹chunkæå–æµ‹è¯• - è½®æ¬¡3ã€4")
    print("=" * 80)
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_dbs = {
        1: os.path.join(base_path, "client_1", "client_1_chunks.db"),
        2: os.path.join(base_path, "client_2", "client_2_chunks.db"), 
        3: os.path.join(base_path, "client_3", "client_3_chunks.db")
    }
    
    empty_hash = hashlib.sha256(b'').hexdigest()
    
    # 1. æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹åœ¨è½®æ¬¡3ã€4çš„chunkä¿¡æ¯
    print("ğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„chunkä¿¡æ¯")
    print("-" * 60)
    
    all_chunks = {}  # {round: {client: {chunk_id: chunk_hash}}}
    all_chunk_data = {}  # {client: {chunk_hash: data}}
    
    for client_id, db_path in client_dbs.items():
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        try:
            print(f"\nğŸ” åˆ†æå®¢æˆ·ç«¯ {client_id}:")
            
            # æ”¶é›†è½®æ¬¡3ã€4çš„chunkä¿¡æ¯
            cursor.execute("""
                SELECT round_num, chunk_id, chunk_hash 
                FROM chunk_metadata 
                WHERE round_num IN (3, 4)
                ORDER BY round_num, chunk_id
            """)
            
            local_chunks = cursor.fetchall()
            
            for round_num, chunk_id, chunk_hash in local_chunks:
                if round_num not in all_chunks:
                    all_chunks[round_num] = {}
                if client_id not in all_chunks[round_num]:
                    all_chunks[round_num][client_id] = {}
                all_chunks[round_num][client_id][chunk_id] = chunk_hash
            
            # æ”¶é›†è¯¥èŠ‚ç‚¹å¯è®¿é—®çš„æ‰€æœ‰chunk_data
            cursor.execute("""
                SELECT chunk_hash, data FROM chunk_data
            """)
            
            chunk_data_records = cursor.fetchall()
            all_chunk_data[client_id] = {}
            
            for chunk_hash, data in chunk_data_records:
                all_chunk_data[client_id][chunk_hash] = pickle.loads(data)
            
            print(f"   æœ¬åœ°chunkè®°å½•: è½®æ¬¡3({len(all_chunks.get(3, {}).get(client_id, {}))}) + è½®æ¬¡4({len(all_chunks.get(4, {}).get(client_id, {}))})")
            print(f"   å¯è®¿é—®chunk_data: {len(all_chunk_data[client_id])} æ¡è®°å½•")
            
        finally:
            conn.close()
    
    # 2. åˆ†æç½‘ç»œä¸­chunkçš„å…¨å±€åˆ†å¸ƒ
    print(f"\nğŸ“Š ç¬¬äºŒé˜¶æ®µï¼šç½‘ç»œchunkå…¨å±€åˆ†å¸ƒåˆ†æ")
    print("-" * 60)
    
    for round_num in sorted(all_chunks.keys()):
        print(f"\nğŸ”„ è½®æ¬¡ {round_num} çš„chunkåˆ†å¸ƒ:")
        
        # ç»Ÿè®¡æ¯ä¸ªchunk_idåœ¨æ‰€æœ‰å®¢æˆ·ç«¯çš„hashåˆ†å¸ƒ
        chunk_distribution = defaultdict(lambda: defaultdict(list))  # {chunk_id: {hash: [client_list]}}
        
        for client_id in all_chunks[round_num]:
            for chunk_id, chunk_hash in all_chunks[round_num][client_id].items():
                chunk_distribution[chunk_id][chunk_hash].append(client_id)
        
        for chunk_id in sorted(chunk_distribution.keys()):
            hash_info = chunk_distribution[chunk_id]
            print(f"   chunk_{chunk_id}:")
            for chunk_hash, clients in hash_info.items():
                chunk_type = "ğŸ—‚ï¸ç©º" if chunk_hash == empty_hash else "ğŸ“¦éç©º"
                print(f"     {chunk_type} {chunk_hash[:16]}... â†’ å®¢æˆ·ç«¯{clients}")
    
    # 3. æ¨¡æ‹Ÿè·¨èŠ‚ç‚¹æå–æµ‹è¯•
    print(f"\nğŸ¯ ç¬¬ä¸‰é˜¶æ®µï¼šè·¨èŠ‚ç‚¹chunkæå–æ¨¡æ‹Ÿæµ‹è¯•")
    print("-" * 60)
    
    test_results = {}
    
    for round_num in sorted(all_chunks.keys()):
        print(f"\nğŸ”„ è½®æ¬¡ {round_num} æå–æµ‹è¯•:")
        test_results[round_num] = {}
        
        # æ”¶é›†è¯¥è½®æ¬¡ç½‘ç»œä¸­çš„æ‰€æœ‰unique chunks
        network_chunks = set()  # {(source_client, chunk_id, chunk_hash)}
        for client_id in all_chunks[round_num]:
            for chunk_id, chunk_hash in all_chunks[round_num][client_id].items():
                network_chunks.add((client_id, chunk_id, chunk_hash))
        
        print(f"   ç½‘ç»œä¸­æ€»chunkæ•°: {len(network_chunks)} ä¸ª")
        
        # å¯¹æ¯ä¸ªå®¢æˆ·ç«¯è¿›è¡Œæå–æµ‹è¯•
        for target_client in client_dbs.keys():
            print(f"\n   ğŸ  å®¢æˆ·ç«¯ {target_client} çš„æå–æµ‹è¯•:")
            
            extractable_count = 0
            missing_chunks = []
            extraction_details = []
            
            for source_client, chunk_id, chunk_hash in network_chunks:
                # æ¨¡æ‹Ÿæå–ï¼šæ£€æŸ¥target_clientæ˜¯å¦æœ‰è¯¥chunkçš„æ•°æ®
                if chunk_hash in all_chunk_data[target_client]:
                    extractable_count += 1
                    chunk_type = "ğŸ—‚ï¸ç©º" if chunk_hash == empty_hash else "ğŸ“¦éç©º"
                    extraction_details.append(f"     âœ… å®¢æˆ·ç«¯{source_client}_chunk{chunk_id}: {chunk_type} å¯æå–")
                else:
                    missing_chunks.append((source_client, chunk_id, chunk_hash))
                    chunk_type = "ğŸ—‚ï¸ç©º" if chunk_hash == empty_hash else "ğŸ“¦éç©º"
                    extraction_details.append(f"     âŒ å®¢æˆ·ç«¯{source_client}_chunk{chunk_id}: {chunk_type} ç¼ºå¤±")
            
            success_rate = (extractable_count / len(network_chunks)) * 100
            
            print(f"     ğŸ“Š æå–ç»Ÿè®¡: {extractable_count}/{len(network_chunks)} ({success_rate:.1f}%)")
            
            if success_rate == 100:
                print(f"     ğŸ‰ å®Œç¾ï¼å¯æå–ç½‘ç»œä¸­æ‰€æœ‰chunkæ•°æ®")
            else:
                print(f"     âš ï¸ ç¼ºå¤± {len(missing_chunks)} ä¸ªchunk:")
                for source_client, chunk_id, chunk_hash in missing_chunks:
                    chunk_type = "ğŸ—‚ï¸ç©º" if chunk_hash == empty_hash else "ğŸ“¦éç©º"
                    print(f"       - å®¢æˆ·ç«¯{source_client}_chunk{chunk_id}: {chunk_type} {chunk_hash[:16]}...")
            
            # æ˜¾ç¤ºè¯¦ç»†æå–ä¿¡æ¯ï¼ˆåªæ˜¾ç¤ºå‰10ä¸ªï¼‰
            print(f"     ğŸ“‹ è¯¦ç»†æå–ä¿¡æ¯ (æ˜¾ç¤ºå‰10ä¸ª):")
            for detail in extraction_details[:10]:
                print(detail)
            if len(extraction_details) > 10:
                print(f"       ... çœç•¥å…¶ä½™ {len(extraction_details) - 10} æ¡è®°å½•")
            
            test_results[round_num][target_client] = {
                'success_rate': success_rate,
                'extractable': extractable_count,
                'total': len(network_chunks),
                'missing': len(missing_chunks)
            }
    
    # 4. ç”Ÿæˆæµ‹è¯•æ€»ç»“æŠ¥å‘Š
    print(f"\nğŸ“‹ ç¬¬å››é˜¶æ®µï¼šæµ‹è¯•æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    overall_success = True
    
    for round_num in sorted(test_results.keys()):
        print(f"\nğŸ”„ è½®æ¬¡ {round_num} æ€»ç»“:")
        round_perfect = True
        
        for client_id in sorted(test_results[round_num].keys()):
            result = test_results[round_num][client_id]
            status = "âœ… å®Œç¾" if result['success_rate'] == 100 else "âš ï¸ ä¸å®Œæ•´"
            print(f"   å®¢æˆ·ç«¯ {client_id}: {result['extractable']}/{result['total']} ({result['success_rate']:.1f}%) {status}")
            
            if result['success_rate'] != 100:
                round_perfect = False
                overall_success = False
        
        print(f"   è½®æ¬¡{round_num}çŠ¶æ€: {'ğŸ‰ æ‰€æœ‰å®¢æˆ·ç«¯éƒ½èƒ½å®Œæ•´æå–' if round_perfect else 'âš ï¸ å­˜åœ¨æå–ä¸å®Œæ•´çš„å®¢æˆ·ç«¯'}")
    
    print(f"\nğŸ¯ æœ€ç»ˆç»“è®º:")
    if overall_success:
        print("ğŸ‰ è·¨èŠ‚ç‚¹chunkæå–æµ‹è¯• - å®Œå…¨æˆåŠŸï¼")
        print("   âœ… æ‰€æœ‰å®¢æˆ·ç«¯éƒ½èƒ½æå–åˆ°ç½‘ç»œä¸­çš„å…¨éƒ¨chunkæ•°æ®")
        print("   âœ… BitTorrent chunkäº¤æ¢ç³»ç»Ÿè¿è¡Œå®Œç¾")
    else:
        print("âš ï¸ è·¨èŠ‚ç‚¹chunkæå–æµ‹è¯• - å‘ç°é—®é¢˜")
        print("   âŒ éƒ¨åˆ†å®¢æˆ·ç«¯æ— æ³•æå–å®Œæ•´çš„ç½‘ç»œchunkæ•°æ®")
        print("   ğŸ”§ å»ºè®®æ£€æŸ¥BitTorrentäº¤æ¢å’Œæ•°æ®ä¿å­˜æœºåˆ¶")

if __name__ == "__main__":
    cross_node_chunk_extraction_test()