#!/usr/bin/env python3
"""
è°ƒè¯•è½®æ¬¡æ•°æ®å¯ç”¨æ€§
æ£€æŸ¥å®é™…å­˜åœ¨å“ªäº›è½®æ¬¡çš„æ•°æ®
"""

import sqlite3
import os
import hashlib

def check_actual_round_data():
    """æ£€æŸ¥å®é™…å­˜åœ¨çš„è½®æ¬¡æ•°æ®"""
    print("ğŸ” æ£€æŸ¥å®é™…è½®æ¬¡æ•°æ®å¯ç”¨æ€§")
    print("=" * 80)
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_dbs = {
        1: os.path.join(base_path, "client_1", "client_1_chunks.db"),
        2: os.path.join(base_path, "client_2", "client_2_chunks.db"), 
        3: os.path.join(base_path, "client_3", "client_3_chunks.db")
    }
    
    empty_hash = hashlib.sha256(b'').hexdigest()
    
    for client_id, db_path in client_dbs.items():
        print(f"\nğŸ“Š å®¢æˆ·ç«¯ {client_id}:")
        
        if not os.path.exists(db_path):
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        try:
            # æŸ¥çœ‹æ‰€æœ‰è½®æ¬¡çš„æ•°æ®åˆ†å¸ƒ
            cursor.execute("""
                SELECT round_num, source_client_id, 
                       COUNT(*) as total_chunks,
                       SUM(CASE WHEN chunk_hash = ? THEN 1 ELSE 0 END) as empty_chunks,
                       SUM(CASE WHEN chunk_hash != ? THEN 1 ELSE 0 END) as non_empty_chunks
                FROM bt_chunks 
                GROUP BY round_num, source_client_id
                ORDER BY round_num, source_client_id
            """, (empty_hash, empty_hash))
            
            round_data = cursor.fetchall()
            print(f"   bt_chunksè¡¨è½®æ¬¡åˆ†å¸ƒ:")
            
            current_round = None
            for round_num, source_client_id, total, empty, non_empty in round_data:
                if round_num != current_round:
                    print(f"\n     ğŸ”„ è½®æ¬¡ {round_num}:")
                    current_round = round_num
                
                print(f"       æºå®¢æˆ·ç«¯{source_client_id}: æ€»è®¡={total}, ç©º={empty}, éç©º={non_empty}")
            
            # æŸ¥çœ‹chunk_dataä¸è½®æ¬¡çš„å…³ç³»
            print(f"\n   chunk_dataä¸è½®æ¬¡å…³è”:")
            cursor.execute("""
                SELECT cd.chunk_hash, cd.created_at, COUNT(DISTINCT bc.round_num) as round_count
                FROM chunk_data cd
                LEFT JOIN bt_chunks bc ON cd.chunk_hash = bc.chunk_hash
                GROUP BY cd.chunk_hash, cd.created_at
                ORDER BY cd.created_at
            """)
            
            chunk_data_info = cursor.fetchall()
            for i, (chunk_hash, created_at, round_count) in enumerate(chunk_data_info, 1):
                chunk_type = "ç©ºchunk" if chunk_hash == empty_hash else "éç©ºchunk"
                print(f"     {i}. {chunk_type}: åœ¨{round_count}ä¸ªè½®æ¬¡ä¸­å‡ºç° (åˆ›å»º: {created_at})")
                
                # æ˜¾ç¤ºå…·ä½“åœ¨å“ªäº›è½®æ¬¡å‡ºç°
                cursor.execute("""
                    SELECT DISTINCT round_num, source_client_id
                    FROM bt_chunks
                    WHERE chunk_hash = ?
                    ORDER BY round_num, source_client_id
                """, (chunk_hash,))
                
                appearances = cursor.fetchall()
                if appearances:
                    rounds_info = []
                    for round_num, source_client_id in appearances:
                        rounds_info.append(f"è½®æ¬¡{round_num}æº{source_client_id}")
                    print(f"        å‡ºç°ä½ç½®: {', '.join(rounds_info[:5])}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        finally:
            conn.close()

def find_available_cross_node_data():
    """å¯»æ‰¾å¯ç”¨çš„è·¨èŠ‚ç‚¹æ•°æ®"""
    print(f"\n{'='*80}")
    print("ğŸ” å¯»æ‰¾å®é™…å¯ç”¨çš„è·¨èŠ‚ç‚¹chunkæ•°æ®")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_dbs = {
        1: os.path.join(base_path, "client_1", "client_1_chunks.db"),
        2: os.path.join(base_path, "client_2", "client_2_chunks.db"), 
        3: os.path.join(base_path, "client_3", "client_3_chunks.db")
    }
    
    empty_hash = hashlib.sha256(b'').hexdigest()
    
    for target_client in [1, 2, 3]:
        print(f"\nğŸ“‹ å®¢æˆ·ç«¯ {target_client} å¯æå–çš„å…¶ä»–èŠ‚ç‚¹æ•°æ®:")
        
        conn = sqlite3.connect(client_dbs[target_client])
        cursor = conn.cursor()
        
        try:
            other_clients = [c for c in [1, 2, 3] if c != target_client]
            
            # æŸ¥æ‰¾æ‰€æœ‰è½®æ¬¡ä¸­å…¶ä»–å®¢æˆ·ç«¯çš„éç©ºchunkï¼Œå¹¶æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
            for source_client in other_clients:
                print(f"\n   ğŸ“Š æ¥è‡ªå®¢æˆ·ç«¯{source_client}çš„æ•°æ®:")
                
                cursor.execute("""
                    SELECT bc.round_num, bc.chunk_id, bc.chunk_hash, bc.is_verified,
                           CASE WHEN cd.chunk_hash IS NOT NULL THEN 1 ELSE 0 END as has_data
                    FROM bt_chunks bc
                    LEFT JOIN chunk_data cd ON bc.chunk_hash = cd.chunk_hash
                    WHERE bc.source_client_id = ? AND bc.chunk_hash != ?
                    ORDER BY bc.round_num, bc.chunk_id
                """, (source_client, empty_hash))
                
                available_chunks = cursor.fetchall()
                
                if available_chunks:
                    # æŒ‰è½®æ¬¡åˆ†ç»„
                    rounds = {}
                    for round_num, chunk_id, chunk_hash, is_verified, has_data in available_chunks:
                        if round_num not in rounds:
                            rounds[round_num] = []
                        rounds[round_num].append({
                            'chunk_id': chunk_id,
                            'hash': chunk_hash,
                            'verified': is_verified,
                            'has_data': has_data
                        })
                    
                    # æ˜¾ç¤ºæ¯ä¸ªè½®æ¬¡çš„æƒ…å†µ
                    for round_num in sorted(rounds.keys()):
                        chunks = rounds[round_num]
                        available_count = sum(1 for c in chunks if c['has_data'])
                        print(f"     è½®æ¬¡{round_num}: {len(chunks)}ä¸ªéç©ºchunk, {available_count}ä¸ªæœ‰æ•°æ®å¯æå–")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                        for chunk in chunks:
                            status = "âœ…å¯æå–" if chunk['has_data'] else "âŒæ— æ•°æ®"
                            verified = "âœ…å·²éªŒè¯" if chunk['verified'] else "âŒæœªéªŒè¯"
                            print(f"       å—{chunk['chunk_id']}: {status} ({verified})")
                else:
                    print(f"     âŒ æ²¡æœ‰æ¥è‡ªå®¢æˆ·ç«¯{source_client}çš„éç©ºchunk")
        
        finally:
            conn.close()

def test_successful_extraction_scenario():
    """æµ‹è¯•æˆåŠŸæå–åœºæ™¯"""
    print(f"\n{'='*80}")
    print("ğŸ§ª æµ‹è¯•å®é™…å¯æˆåŠŸæå–çš„æ•°æ®")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_1_db = os.path.join(base_path, "client_1", "client_1_chunks.db")
    
    conn = sqlite3.connect(client_1_db)
    cursor = conn.cursor()
    
    empty_hash = hashlib.sha256(b'').hexdigest()
    
    try:
        # æŸ¥æ‰¾å®¢æˆ·ç«¯1ä¸­å­˜åœ¨æ•°æ®çš„æ‰€æœ‰éç©ºchunk
        cursor.execute("""
            SELECT bc.round_num, bc.source_client_id, bc.chunk_id, bc.chunk_hash, cd.created_at
            FROM bt_chunks bc
            INNER JOIN chunk_data cd ON bc.chunk_hash = cd.chunk_hash
            WHERE bc.chunk_hash != ?
            ORDER BY bc.round_num, bc.source_client_id, bc.chunk_id
        """, (empty_hash,))
        
        extractable_chunks = cursor.fetchall()
        
        print(f"\nğŸ“Š å®¢æˆ·ç«¯1ä¸­å¯æå–çš„éç©ºchunkæ•°æ®:")
        print(f"   æ€»æ•°: {len(extractable_chunks)} æ¡")
        
        if extractable_chunks:
            # æŒ‰æ¥æºåˆ†ç»„
            by_source = {}
            for round_num, source_client_id, chunk_id, chunk_hash, created_at in extractable_chunks:
                key = f"å®¢æˆ·ç«¯{source_client_id}"
                if key not in by_source:
                    by_source[key] = []
                by_source[key].append({
                    'round': round_num,
                    'chunk_id': chunk_id,
                    'hash': chunk_hash[:16] + '...',
                    'created_at': created_at
                })
            
            for source, chunks in by_source.items():
                print(f"\n   ğŸ“‹ {source}çš„æ•°æ®:")
                for chunk in chunks:
                    print(f"     è½®æ¬¡{chunk['round']}, å—{chunk['chunk_id']}: {chunk['hash']} (åˆ›å»º: {chunk['created_at']})")
                    
                    # å°è¯•å®é™…æå–æ•°æ®éªŒè¯
                    full_hash = chunk['hash'].replace('...', '')  # éœ€è¦å®Œæ•´å“ˆå¸Œ
                    cursor.execute("SELECT data FROM chunk_data WHERE chunk_hash LIKE ?", (chunk['hash'].replace('...', '%'),))
                    data_result = cursor.fetchone()
                    
                    if data_result:
                        try:
                            import pickle
                            unpickled_data = pickle.loads(data_result[0])
                            print(f"       âœ… æ•°æ®æå–æˆåŠŸ: {type(unpickled_data)}")
                        except Exception as e:
                            print(f"       âŒ æ•°æ®æå–å¤±è´¥: {e}")
    
    finally:
        conn.close()

def main():
    """ä¸»å‡½æ•°"""
    check_actual_round_data()
    find_available_cross_node_data()
    test_successful_extraction_scenario()
    
    print(f"\n{'='*80}")
    print("ğŸ¯ è°ƒè¯•ç»“è®º:")
    print("1. å®é™…å¯ç”¨çš„è½®æ¬¡æ•°æ®åˆ†å¸ƒ")
    print("2. è·¨èŠ‚ç‚¹chunkæ•°æ®çš„çœŸå®å¯æå–æ€§")
    print("3. æ•°æ®ç”Ÿå­˜æœŸç­–ç•¥çš„å®é™…å½±å“")

if __name__ == "__main__":
    main()