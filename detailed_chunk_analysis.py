#!/usr/bin/env python3
"""
è¯¦ç»†çš„chunkæ•°æ®åˆ†æ - éªŒè¯æ•°æ®å†…å®¹å’Œå“ˆå¸Œä¸€è‡´æ€§
"""

import sqlite3
import os
import hashlib
import json
from collections import defaultdict

def analyze_detailed_chunk_content(conn, client_id):
    """è¯¦ç»†åˆ†æchunkå†…å®¹å’Œä¸€è‡´æ€§"""
    try:
        cursor = conn.cursor()
        
        print(f"\nğŸ“Š å®¢æˆ·ç«¯ {client_id} è¯¦ç»†æ•°æ®å†…å®¹åˆ†æ:")
        
        # 1. åˆ†æbt_chunksè¡¨ä¸­çš„å…·ä½“æ•°æ®
        cursor.execute("""
            SELECT round_num, source_client_id, chunk_id, chunk_hash, 
                   holder_client_id, received_time, is_verified
            FROM bt_chunks 
            ORDER BY round_num, source_client_id, chunk_id
            LIMIT 10
        """)
        
        bt_samples = cursor.fetchall()
        print(f"   BitTorrentäº¤æ¢è®°å½•æ ·æœ¬ (å‰10æ¡):")
        for i, record in enumerate(bt_samples, 1):
            round_num, source_client_id, chunk_id, chunk_hash, holder_client_id, received_time, is_verified = record
            print(f"     {i}. è½®æ¬¡={round_num}, æº={source_client_id}, å—ID={chunk_id}")
            print(f"        å“ˆå¸Œ={chunk_hash[:16] if chunk_hash else 'None'}..., æŒæœ‰è€…={holder_client_id}, éªŒè¯={is_verified}")
        
        # 2. åˆ†æchunk_dataä¸­çš„å®é™…æ•°æ®
        cursor.execute("""
            SELECT chunk_hash, LENGTH(data) as data_length, 
                   SUBSTR(data, 1, 50) as data_preview
            FROM chunk_data 
            LIMIT 5
        """)
        
        data_samples = cursor.fetchall()
        print(f"\n   å­˜å‚¨æ•°æ®æ ·æœ¬ (å‰5æ¡):")
        for i, record in enumerate(data_samples, 1):
            chunk_hash, data_length, data_preview = record
            print(f"     {i}. å“ˆå¸Œ={chunk_hash[:16]}..., é•¿åº¦={data_length}bytes")
            print(f"        æ•°æ®é¢„è§ˆ={data_preview[:30] if data_preview else 'None'}...")
        
        # 3. éªŒè¯å“ˆå¸Œä¸€è‡´æ€§
        print(f"\n   å“ˆå¸Œä¸€è‡´æ€§éªŒè¯:")
        cursor.execute("""
            SELECT cd.chunk_hash, cd.data
            FROM chunk_data cd
            LIMIT 3
        """)
        
        hash_samples = cursor.fetchall()
        for i, (stored_hash, data) in enumerate(hash_samples, 1):
            if data:
                # è®¡ç®—å®é™…æ•°æ®çš„å“ˆå¸Œ
                actual_hash = hashlib.sha256(data).hexdigest()
                matches = stored_hash == actual_hash
                print(f"     æ ·æœ¬{i}: å­˜å‚¨å“ˆå¸Œvså®é™…å“ˆå¸Œ = {'âœ…åŒ¹é…' if matches else 'âŒä¸åŒ¹é…'}")
                if not matches:
                    print(f"       å­˜å‚¨: {stored_hash[:16]}...")
                    print(f"       å®é™…: {actual_hash[:16]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯¦ç»†å†…å®¹åˆ†æå¤±è´¥: {e}")
        return False

def analyze_round_distribution(conn, client_id):
    """åˆ†ææ¯è½®çš„æ•°æ®åˆ†å¸ƒè¯¦æƒ…"""
    try:
        cursor = conn.cursor()
        
        print(f"\nğŸ” å®¢æˆ·ç«¯ {client_id} è½®æ¬¡åˆ†å¸ƒè¯¦ç»†åˆ†æ:")
        
        # è·å–æ¯è½®çš„è¯¦ç»†ç»Ÿè®¡
        cursor.execute("""
            SELECT round_num, source_client_id, 
                   COUNT(*) as chunk_count,
                   COUNT(DISTINCT chunk_id) as unique_chunks,
                   COUNT(DISTINCT chunk_hash) as unique_hashes
            FROM bt_chunks 
            GROUP BY round_num, source_client_id
            ORDER BY round_num, source_client_id
        """)
        
        round_details = cursor.fetchall()
        
        current_round = None
        for record in round_details:
            round_num, source_client_id, chunk_count, unique_chunks, unique_hashes = record
            
            if round_num != current_round:
                print(f"\n   è½®æ¬¡ {round_num}:")
                current_round = round_num
            
            print(f"     æ¥æºå®¢æˆ·ç«¯ {source_client_id}: {chunk_count}æ¡è®°å½•, {unique_chunks}ä¸ªå”¯ä¸€chunk, {unique_hashes}ä¸ªå”¯ä¸€å“ˆå¸Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ è½®æ¬¡åˆ†å¸ƒåˆ†æå¤±è´¥: {e}")
        return False

def verify_cross_client_consistency():
    """è·¨å®¢æˆ·ç«¯æ•°æ®ä¸€è‡´æ€§éªŒè¯"""
    print(f"\nğŸŒ è·¨å®¢æˆ·ç«¯æ•°æ®ä¸€è‡´æ€§éªŒè¯:")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_dbs = {
        1: os.path.join(base_path, "client_1", "client_1_chunks.db"),
        2: os.path.join(base_path, "client_2", "client_2_chunks.db"), 
        3: os.path.join(base_path, "client_3", "client_3_chunks.db")
    }
    
    # æ”¶é›†æ¯ä¸ªå®¢æˆ·ç«¯çš„chunkå“ˆå¸Œ
    all_hashes = {}
    
    for client_id, db_path in client_dbs.items():
        if not os.path.exists(db_path):
            continue
            
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # è·å–è¯¥å®¢æˆ·ç«¯çš„æ‰€æœ‰chunkå“ˆå¸Œ
        cursor.execute("""
            SELECT round_num, source_client_id, chunk_id, chunk_hash
            FROM bt_chunks
            ORDER BY round_num, source_client_id, chunk_id
        """)
        
        client_hashes = defaultdict(lambda: defaultdict(dict))
        for record in cursor.fetchall():
            round_num, source_client_id, chunk_id, chunk_hash = record
            client_hashes[round_num][source_client_id][chunk_id] = chunk_hash
        
        all_hashes[client_id] = client_hashes
        conn.close()
    
    # éªŒè¯ä¸€è‡´æ€§
    if len(all_hashes) < 2:
        print("   âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè·¨å®¢æˆ·ç«¯éªŒè¯")
        return
    
    # æ£€æŸ¥åŒä¸€æºå’Œchunkçš„å“ˆå¸Œæ˜¯å¦ä¸€è‡´
    all_rounds = set()
    for client_hashes in all_hashes.values():
        all_rounds.update(client_hashes.keys())
    
    consistency_issues = 0
    total_checks = 0
    
    for round_num in sorted(all_rounds):
        print(f"\n   éªŒè¯è½®æ¬¡ {round_num}:")
        
        # è·å–è¿™è½®æ‰€æœ‰å¯èƒ½çš„æºå®¢æˆ·ç«¯
        all_sources = set()
        for client_hashes in all_hashes.values():
            if round_num in client_hashes:
                all_sources.update(client_hashes[round_num].keys())
        
        for source_client in sorted(all_sources):
            # æ£€æŸ¥æ¯ä¸ªæºå®¢æˆ·ç«¯çš„chunksæ˜¯å¦åœ¨æ‰€æœ‰ç›®æ ‡å®¢æˆ·ç«¯ä¸­ä¸€è‡´
            source_chunks = None
            for client_id, client_hashes in all_hashes.items():
                if round_num in client_hashes and source_client in client_hashes[round_num]:
                    current_chunks = client_hashes[round_num][source_client]
                    
                    if source_chunks is None:
                        source_chunks = current_chunks
                        reference_client = client_id
                    else:
                        # æ¯”è¾ƒchunkå“ˆå¸Œ
                        for chunk_id, chunk_hash in current_chunks.items():
                            total_checks += 1
                            if chunk_id in source_chunks:
                                if source_chunks[chunk_id] != chunk_hash:
                                    consistency_issues += 1
                                    print(f"     âŒ ä¸ä¸€è‡´: æº{source_client}çš„chunk{chunk_id}")
                                    print(f"        å®¢æˆ·ç«¯{reference_client}: {source_chunks[chunk_id][:16]}...")
                                    print(f"        å®¢æˆ·ç«¯{client_id}: {chunk_hash[:16]}...")
        
        # ç»Ÿè®¡è¿™è½®çš„ä¸€è‡´æ€§
        round_chunks = 0
        for client_hashes in all_hashes.values():
            if round_num in client_hashes:
                for source_chunks in client_hashes[round_num].values():
                    round_chunks += len(source_chunks)
        
        if round_chunks > 0:
            print(f"     è¯¥è½®æ¬¡æ€»chunkæ•°: {round_chunks}")
    
    print(f"\n   æ€»ä½“ä¸€è‡´æ€§æ£€æŸ¥: {total_checks - consistency_issues}/{total_checks} é€šè¿‡")
    if consistency_issues == 0:
        print("   âœ… æ‰€æœ‰è·¨å®¢æˆ·ç«¯æ•°æ®å“ˆå¸Œå®Œå…¨ä¸€è‡´ï¼")
    else:
        print(f"   âš ï¸ å‘ç° {consistency_issues} ä¸ªä¸ä¸€è‡´çš„å“ˆå¸Œ")

def test_data_recovery():
    """æµ‹è¯•æ•°æ®æ¢å¤èƒ½åŠ›"""
    print(f"\nğŸ”§ æ•°æ®æ¢å¤èƒ½åŠ›æµ‹è¯•:")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªå®¢æˆ·ç«¯æµ‹è¯•æ•°æ®æ¢å¤
    client_1_db = os.path.join(base_path, "client_1", "client_1_chunks.db")
    
    if not os.path.exists(client_1_db):
        print("   âŒ æµ‹è¯•æ•°æ®åº“ä¸å­˜åœ¨")
        return
    
    conn = sqlite3.connect(client_1_db)
    cursor = conn.cursor()
    
    try:
        # 1. æµ‹è¯•èƒ½å¦é‡æ„å®Œæ•´çš„chunkæ•°æ®
        cursor.execute("""
            SELECT DISTINCT round_num FROM bt_chunks ORDER BY round_num LIMIT 1
        """)
        test_round = cursor.fetchone()
        
        if not test_round:
            print("   âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¾›æµ‹è¯•")
            return
        
        test_round_num = test_round[0]
        print(f"   æµ‹è¯•è½®æ¬¡ {test_round_num} çš„æ•°æ®æ¢å¤èƒ½åŠ›:")
        
        # è·å–è¿™è½®çš„æ‰€æœ‰chunks
        cursor.execute("""
            SELECT source_client_id, chunk_id, chunk_hash
            FROM bt_chunks 
            WHERE round_num = ?
            ORDER BY source_client_id, chunk_id
        """, (test_round_num,))
        
        test_chunks = cursor.fetchall()
        print(f"     è¯¥è½®æ¬¡æ€»chunkæ•°: {len(test_chunks)}")
        
        # ç»Ÿè®¡æ¯ä¸ªæºçš„chunks
        source_counts = defaultdict(int)
        for source_client_id, chunk_id, chunk_hash in test_chunks:
            source_counts[source_client_id] += 1
        
        print(f"     å„æºchunkåˆ†å¸ƒ: {dict(source_counts)}")
        
        # 2. æµ‹è¯•èƒ½å¦è®¿é—®chunkçš„å®é™…æ•°æ®
        cursor.execute("""
            SELECT bc.chunk_hash, cd.data
            FROM bt_chunks bc
            LEFT JOIN chunk_data cd ON bc.chunk_hash = cd.chunk_hash
            WHERE bc.round_num = ?
            LIMIT 5
        """, (test_round_num,))
        
        data_samples = cursor.fetchall()
        accessible_data = sum(1 for _, data in data_samples if data is not None)
        
        print(f"     æ•°æ®å¯è®¿é—®æ€§: {accessible_data}/{len(data_samples)} chunkæœ‰å®é™…æ•°æ®")
        
        if accessible_data > 0:
            print("   âœ… æ•°æ®æ¢å¤æµ‹è¯•é€šè¿‡ - å¯ä»¥è®¿é—®chunkçš„å®é™…å†…å®¹")
        else:
            print("   âš ï¸ æ•°æ®æ¢å¤æµ‹è¯•éƒ¨åˆ†é€šè¿‡ - æœ‰å…ƒæ•°æ®ä½†ç¼ºå°‘å®é™…æ•°æ®")
        
    finally:
        conn.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹è¯¦ç»†çš„chunkæ•°æ®åˆ†æ")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_dbs = {
        1: os.path.join(base_path, "client_1", "client_1_chunks.db"),
        2: os.path.join(base_path, "client_2", "client_2_chunks.db"), 
        3: os.path.join(base_path, "client_3", "client_3_chunks.db")
    }
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªå®¢æˆ·ç«¯
    for client_id, db_path in client_dbs.items():
        if not os.path.exists(db_path):
            continue
        
        print(f"\n{'='*60}")
        print(f"ğŸ” å®¢æˆ·ç«¯ {client_id} è¯¦ç»†åˆ†æ")
        
        conn = sqlite3.connect(db_path)
        try:
            analyze_detailed_chunk_content(conn, client_id)
            analyze_round_distribution(conn, client_id)
        finally:
            conn.close()
    
    # è·¨å®¢æˆ·ç«¯ä¸€è‡´æ€§éªŒè¯
    verify_cross_client_consistency()
    
    # æ•°æ®æ¢å¤æµ‹è¯•
    test_data_recovery()
    
    print(f"\n{'='*60}")
    print("âœ… è¯¦ç»†chunkæ•°æ®åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()