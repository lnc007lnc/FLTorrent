#!/usr/bin/env python3
"""
Ray-Powered FederatedScope V2 Script
===================================

ä¸€é”®å¯åŠ¨åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ ï¼Œå®Œå…¨æ›¿ä»£ä¼ ç»Ÿshellè„šæœ¬
- è‡ªåŠ¨GPUèµ„æºç®¡ç†å’Œåˆ†é…
- åŠ¨æ€IPç«¯å£åˆ†é…
- å®æ—¶ç›‘æ§å’Œæ—¥å¿—
- æ”¯æŒäº‘æœåŠ¡å™¨æ‰©å±•

ç›´æ¥è¿è¡Œå³å¯å¯åŠ¨å®Œæ•´çš„FLç³»ç»Ÿ
"""

import ray
import os
import sys
import time
import yaml
import logging
import psutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass

# ============================================================================
# ğŸ”§ é…ç½®åŒºåŸŸ - æ‰€æœ‰è®¾ç½®é›†ä¸­åœ¨æ­¤å¤„ï¼Œæ–¹ä¾¿ä¿®æ”¹
# ============================================================================

@dataclass
class FLConfig:
    """è”é‚¦å­¦ä¹ é…ç½®å‚æ•°"""
    
    # === åŸºç¡€è®¾ç½® ===
    CLIENT_NUM: int = 3                    # å®¢æˆ·ç«¯æ•°é‡
    TOTAL_ROUNDS: int = 3                  # è®­ç»ƒè½®æ•°
    CHUNK_NUM: int = 10                    # æ¯ä¸ªå®¢æˆ·ç«¯æ¨¡å‹chunkæ•°
    IMPORTANCE_METHOD: str = "snip"         # chunké‡è¦åº¦æ–¹æ³•: magnitude, l2_norm, snip, fisher
    
    # === æ•°æ®é›†è®¾ç½® ===
    DATASET: str = "CIFAR10@torchvision"   # æ•°æ®é›†
    BATCH_SIZE: int = 32                   # æ‰¹å¤„ç†å¤§å°
    DATA_SPLIT_ALPHA: float = 0.1          # LDAæ•°æ®åˆ’åˆ†å‚æ•°
    
    # === æ¨¡å‹è®¾ç½® ===
    MODEL_TYPE: str = "convnet2"          # æ¨¡å‹ç±»å‹
    MODEL_HIDDEN: int = 512               # éšè—å±‚å¤§å°
    MODEL_OUT_CHANNELS: int = 10          # è¾“å‡ºé€šé“æ•°
    MODEL_DROPOUT: float = 0.0            # Dropoutç‡
    
    # === è®­ç»ƒè®¾ç½® ===
    LOCAL_UPDATE_STEPS: int = 5           # æœ¬åœ°è®­ç»ƒæ­¥æ•°
    LEARNING_RATE: float = 0.01           # å­¦ä¹ ç‡
    OPTIMIZER: str = "SGD"                # ä¼˜åŒ–å™¨
    WEIGHT_DECAY: float = 0.0001          # æƒé‡è¡°å‡
    GRAD_CLIP: float = 5.0                # æ¢¯åº¦è£å‰ª
    
    # === BitTorrentè®¾ç½® ===
    BITTORRENT_TIMEOUT: float = 600.0     # BitTorrentè¶…æ—¶
    BT_CHUNK_SELECTION: str = "rarest_first"  # chunké€‰æ‹©ç­–ç•¥
    BT_MIN_COMPLETION_RATIO: float = 0.8   # æœ€å°å®Œæˆæ¯”ç‡
    
    # === æ‹“æ‰‘è®¾ç½® ===
    TOPOLOGY_TYPE: str = "star"           # æ‹“æ‰‘ç±»å‹: star, ring, mesh
    TOPOLOGY_TIMEOUT: float = 600.0       # æ‹“æ‰‘æ„å»ºè¶…æ—¶
    
    # === Rayèµ„æºè®¾ç½® ===
    RAY_AUTO_GPU_DETECTION: bool = True   # è‡ªåŠ¨GPUæ£€æµ‹
    RAY_MAX_CPUS: Optional[int] = None     # æœ€å¤§CPUæ•°ï¼ˆNone=è‡ªåŠ¨ï¼‰
    RAY_MAX_GPUS: Optional[int] = None     # æœ€å¤§GPUæ•°ï¼ˆNone=è‡ªåŠ¨ï¼‰
    
    # === ç›‘æ§è®¾ç½® ===
    MONITOR_DURATION: int = 600           # ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼‰
    LOG_LEVEL: str = "INFO"               # æ—¥å¿—çº§åˆ«
    ENABLE_RAY_DASHBOARD: bool = True     # å¯ç”¨Ray Dashboard
    
    # === è¾“å‡ºè®¾ç½® ===
    OUTPUT_DIR: str = "ray_v2_output"     # è¾“å‡ºç›®å½•
    LOG_DIR: str = "logs"                 # æ—¥å¿—ç›®å½•

# åˆ›å»ºå…¨å±€é…ç½®å®ä¾‹
CONFIG = FLConfig()

# ============================================================================
# ğŸ“Š æ—¥å¿—è®¾ç½®
# ============================================================================

def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs(CONFIG.LOG_DIR, exist_ok=True)
    
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(
        level=getattr(logging, CONFIG.LOG_LEVEL),
        format=log_format,
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f"{CONFIG.LOG_DIR}/run_ray.log")
        ]
    )
    return logging.getLogger(__name__)

# ============================================================================
# ğŸ­ Ray Actorå®šä¹‰
# ============================================================================

@ray.remote
class RayFederatedScopeServer:
    """Rayé©±åŠ¨çš„FederatedScopeæœåŠ¡å™¨Actor"""
    
    def __init__(self, config: Dict[str, Any], gpu_id: Optional[int] = None):
        self.config = config
        self.gpu_id = gpu_id
        self.process = None
        self.node_ip = ray.util.get_node_ip_address()
        self.server_port = None
        self.log_file = None
        
    def start(self) -> Tuple[str, int]:
        """å¯åŠ¨æœåŠ¡å™¨"""
        # åŠ¨æ€åˆ†é…ç«¯å£
        import socket
        sock = socket.socket()
        sock.bind(('', 0))
        self.server_port = sock.getsockname()[1]
        sock.close()
        
        # æ›´æ–°é…ç½®
        self.config['distribute']['server_host'] = self.node_ip
        self.config['distribute']['server_port'] = self.server_port
        
        if self.gpu_id is not None:
            self.config['device'] = self.gpu_id
            self.config['use_gpu'] = True
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config_path = f"{CONFIG.OUTPUT_DIR}/configs/server.yaml"
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        with open(config_path, 'w') as f:
            yaml.safe_dump(self.config, f)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶ - ä½¿ç”¨èŠ‚ç‚¹å
        self.log_file = f"{CONFIG.LOG_DIR}/server.log"
        os.makedirs(CONFIG.LOG_DIR, exist_ok=True)
        
        # å¯åŠ¨è¿›ç¨‹
        cmd = [sys.executable, 'federatedscope/main.py', '--cfg', config_path]
        env = os.environ.copy()
        env['PYTHONPATH'] = '.'
        if self.gpu_id is not None:
            env['CUDA_VISIBLE_DEVICES'] = str(self.gpu_id)
        
        with open(self.log_file, 'w') as log_f:
            self.process = subprocess.Popen(
                cmd, stdout=log_f, stderr=log_f, env=env
            )
        
        return self.node_ip, self.server_port
    
    def get_status(self) -> Dict:
        """è·å–çŠ¶æ€"""
        if self.process is None:
            return {"status": "not_started"}
        
        poll = self.process.poll()
        if poll is None:
            return {
                "status": "running",
                "pid": self.process.pid,
                "node_ip": self.node_ip,
                "server_port": self.server_port,
                "gpu_id": self.gpu_id,
                "log_file": self.log_file
            }
        else:
            return {"status": "stopped", "returncode": poll}
    
    def stop(self):
        """åœæ­¢æœåŠ¡å™¨"""
        if self.process and self.process.poll() is None:
            self.process.terminate()
            try:
                self.process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.process.kill()

@ray.remote
class RayFederatedScopeClient:
    """Rayé©±åŠ¨çš„FederatedScopeå®¢æˆ·ç«¯Actor"""
    
    def __init__(self, client_id: int, config: Dict[str, Any], 
                 server_ip: str, server_port: int, gpu_id: Optional[int] = None):
        self.client_id = client_id
        self.config = config.copy()
        self.server_ip = server_ip
        self.server_port = server_port
        self.gpu_id = gpu_id
        self.process = None
        self.node_ip = ray.util.get_node_ip_address()
        self.client_port = None
        self.log_file = None
        
    def start(self) -> bool:
        """å¯åŠ¨å®¢æˆ·ç«¯"""
        # åŠ¨æ€åˆ†é…ç«¯å£
        import socket
        sock = socket.socket()
        sock.bind(('', 0))
        self.client_port = sock.getsockname()[1]
        sock.close()
        
        # æ›´æ–°å®¢æˆ·ç«¯ç‰¹å®šé…ç½®
        self.config['distribute']['server_host'] = self.server_ip
        self.config['distribute']['server_port'] = self.server_port
        self.config['distribute']['client_host'] = self.node_ip
        self.config['distribute']['client_port'] = self.client_port
        self.config['distribute']['data_idx'] = self.client_id
        
        # å®¢æˆ·ç«¯ä¸“ç”¨ç§å­
        self.config['seed'] = 12345 + self.client_id
        
        if self.gpu_id is not None:
            # å½“ä½¿ç”¨CUDA_VISIBLE_DEVICESæ—¶ï¼ŒFederatedScopeåº”è¯¥ä½¿ç”¨è®¾å¤‡0
            # å› ä¸ºç¯å¢ƒå˜é‡é™åˆ¶äº†å¯è§çš„GPUï¼ŒPyTorchä¼šé‡æ–°ç¼–å·ä¸º0
            self.config['device'] = 0
            self.config['use_gpu'] = True
        
        # å®¢æˆ·ç«¯è¾“å‡ºç›®å½•
        self.config['outdir'] = f"{CONFIG.OUTPUT_DIR}/client_{self.client_id}_output"
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config_path = f"{CONFIG.OUTPUT_DIR}/configs/client_{self.client_id}.yaml"
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        with open(config_path, 'w') as f:
            yaml.safe_dump(self.config, f)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶ - ä½¿ç”¨èŠ‚ç‚¹å
        self.log_file = f"{CONFIG.LOG_DIR}/client_{self.client_id}.log"
        
        # å¯åŠ¨è¿›ç¨‹
        cmd = [sys.executable, 'federatedscope/main.py', '--cfg', config_path]
        env = os.environ.copy()
        env['PYTHONPATH'] = '.'
        if self.gpu_id is not None:
            env['CUDA_VISIBLE_DEVICES'] = str(self.gpu_id)
        
        with open(self.log_file, 'w') as log_f:
            self.process = subprocess.Popen(
                cmd, stdout=log_f, stderr=log_f, env=env
            )
        
        return True
    
    def get_status(self) -> Dict:
        """è·å–çŠ¶æ€"""
        if self.process is None:
            return {"status": "not_started"}
        
        poll = self.process.poll()
        if poll is None:
            return {
                "status": "running",
                "pid": self.process.pid,
                "client_id": self.client_id,
                "node_ip": self.node_ip,
                "client_port": self.client_port,
                "gpu_id": self.gpu_id,
                "log_file": self.log_file
            }
        else:
            return {"status": "stopped", "returncode": poll}
    
    def stop(self):
        """åœæ­¢å®¢æˆ·ç«¯"""
        if self.process and self.process.poll() is None:
            self.process.terminate()
            try:
                self.process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.process.kill()

# ============================================================================
# ğŸš€ ä¸»è¦æ‰§è¡Œç±»
# ============================================================================

class RayV2FederatedLearning:
    """Ray V2 è”é‚¦å­¦ä¹ ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.logger = setup_logging()
        self.server_actor = None
        self.client_actors = []
        self.server_info = None
        self.cleanup_performed = False
        
    def initialize_ray_cluster(self):
        """åˆå§‹åŒ–Rayé›†ç¾¤"""
        # æ£€æµ‹ç¡¬ä»¶èµ„æº
        num_cpus = CONFIG.RAY_MAX_CPUS or psutil.cpu_count()
        num_gpus = CONFIG.RAY_MAX_GPUS
        
        if CONFIG.RAY_AUTO_GPU_DETECTION and num_gpus is None:
            try:
                import torch
                num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
            except ImportError:
                num_gpus = 0
        
        # åˆå§‹åŒ–Ray
        ray_config = {
            "num_cpus": num_cpus,
            "num_gpus": num_gpus or 0,
            "ignore_reinit_error": True
        }
        
        if CONFIG.ENABLE_RAY_DASHBOARD:
            ray_config.update({
                "include_dashboard": True,
                "dashboard_host": "0.0.0.0",
                "dashboard_port": 8265
            })
        
        ray.init(**ray_config)
        
        resources = ray.cluster_resources()
        self.logger.info(f"ğŸš€ Rayé›†ç¾¤åˆå§‹åŒ–å®Œæˆ:")
        self.logger.info(f"   ğŸ“Š èµ„æº: {dict(resources)}")
        
        if CONFIG.ENABLE_RAY_DASHBOARD:
            dashboard_url = f"http://127.0.0.1:8265"
            self.logger.info(f"   ğŸŒ Dashboard: {dashboard_url}")
        
    def generate_base_config(self) -> Dict[str, Any]:
        """ç”ŸæˆåŸºç¡€é…ç½®"""
        return {
            'use_gpu': True,
            'device': 0,  # å°†è¢«åŠ¨æ€è¦†ç›–
            'seed': 12345,  # å°†è¢«åŠ¨æ€è¦†ç›–
            
            'federate': {
                'client_num': CONFIG.CLIENT_NUM,
                'mode': 'distributed',
                'total_round_num': CONFIG.TOTAL_ROUNDS,
                'sample_client_num': CONFIG.CLIENT_NUM
            },
            
            'distribute': {
                'use': True,
                'server_host': '127.0.0.1',  # å°†è¢«åŠ¨æ€è¦†ç›–
                'server_port': 50051,        # å°†è¢«åŠ¨æ€è¦†ç›–
                'client_host': '127.0.0.1',  # å°†è¢«åŠ¨æ€è¦†ç›–
                'client_port': 50052,        # å°†è¢«åŠ¨æ€è¦†ç›–
                'role': 'server',            # å°†è¢«åŠ¨æ€è¦†ç›–
                'data_idx': 0                # å°†è¢«åŠ¨æ€è¦†ç›–
            },
            
            'data': {
                'root': 'data/',
                'type': CONFIG.DATASET,
                'splits': [0.8, 0.1, 0.1],
                'num_workers': 0,
                'transform': [['ToTensor'], ['Normalize', {
                    'mean': [0.4914, 0.4822, 0.4465], 
                    'std': [0.2470, 0.2435, 0.2616]
                }]],
                'test_transform': [['ToTensor'], ['Normalize', {
                    'mean': [0.4914, 0.4822, 0.4465], 
                    'std': [0.2470, 0.2435, 0.2616]
                }]],
                'args': [{'download': True}],
                'splitter': 'lda',
                'splitter_args': [{'alpha': CONFIG.DATA_SPLIT_ALPHA}]
            },
            
            'dataloader': {
                'batch_size': CONFIG.BATCH_SIZE
            },
            
            'model': {
                'type': CONFIG.MODEL_TYPE,
                'hidden': CONFIG.MODEL_HIDDEN,
                'out_channels': CONFIG.MODEL_OUT_CHANNELS,
                'dropout': CONFIG.MODEL_DROPOUT
            },
            
            'train': {
                'local_update_steps': CONFIG.LOCAL_UPDATE_STEPS,
                'batch_or_epoch': 'epoch',
                'optimizer': {
                    'lr': CONFIG.LEARNING_RATE,
                    'type': CONFIG.OPTIMIZER,
                    'weight_decay': CONFIG.WEIGHT_DECAY
                }
            },
            
            'grad': {
                'grad_clip': CONFIG.GRAD_CLIP
            },
            
            'criterion': {
                'type': 'CrossEntropyLoss'
            },
            
            'trainer': {
                'type': 'cvtrainer'
            },
            
            'eval': {
                'freq': 1,
                'metrics': ['acc', 'correct'],
                'best_res_update_round_wise_key': 'test_acc'
            },
            
            'topology': {
                'use': True,
                'type': CONFIG.TOPOLOGY_TYPE,
                'timeout': CONFIG.TOPOLOGY_TIMEOUT,
                'verbose': True
            },
            
            'bittorrent': {
                'enable': True,
                'timeout': CONFIG.BITTORRENT_TIMEOUT,
                'verbose': True,
                'chunk_selection': CONFIG.BT_CHUNK_SELECTION,
                'min_completion_ratio': CONFIG.BT_MIN_COMPLETION_RATIO
            },
            
            'chunk': {
                'num_chunks': CONFIG.CHUNK_NUM,
                'importance_method': CONFIG.IMPORTANCE_METHOD
            },
            
            'chunk_num': CONFIG.CHUNK_NUM,
            'chunk_importance_method': CONFIG.IMPORTANCE_METHOD,
            
            'outdir': f'{CONFIG.OUTPUT_DIR}/server_output'
        }
        
    def allocate_gpu_resources(self) -> List[Optional[int]]:
        """æ™ºèƒ½GPUèµ„æºåˆ†é…"""
        cluster_resources = ray.cluster_resources()
        available_gpus = int(cluster_resources.get('GPU', 0))
        
        if available_gpus == 0:
            self.logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
            return [None] * (CONFIG.CLIENT_NUM + 1)
        
        gpu_allocation = []
        
        # æœåŠ¡å™¨ä½¿ç”¨CPUï¼Œè®©GPUå…¨éƒ¨ç»™å®¢æˆ·ç«¯
        gpu_allocation.append(None)
        
        # å®¢æˆ·ç«¯GPUåˆ†é…ï¼šå¹³å‡åˆ†é…åˆ°æ‰€æœ‰å¯ç”¨GPU
        for i in range(CONFIG.CLIENT_NUM):
            if available_gpus > 0:
                # å®¢æˆ·ç«¯è½®æ¢åˆ†é…åˆ°æ‰€æœ‰å¯ç”¨GPU
                gpu_id = i % available_gpus
                gpu_allocation.append(gpu_id)
            else:
                gpu_allocation.append(None)
        
        self.logger.info(f"ğŸ¯ GPUåˆ†é…: Server=CPU, Clients={gpu_allocation[1:]}")
        return gpu_allocation
    
    def cleanup_environment(self):
        """æ¸…ç†ç¯å¢ƒ"""
        if self.cleanup_performed:
            return
            
        self.logger.info("ğŸ§¹ æ¸…ç†ç¯å¢ƒ...")
        
        # åœæ­¢æ—§è¿›ç¨‹
        subprocess.run(['pkill', '-9', '-f', 'python.*federatedscope'], 
                      capture_output=True, check=False)
        
        # æ¸…ç†æ•°æ®åº“æ–‡ä»¶
        subprocess.run(['rm', '-rf', 'tmp/client_*/client_*_chunks.db'], 
                      shell=True, check=False)
        
        # æ¸…ç†æ—¥å¿—ç›®å½•
        for log_dir in ['connection_logs', 'topology_logs', 'bittorrent_logs']:
            subprocess.run(['rm', '-rf', log_dir], check=False)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(CONFIG.OUTPUT_DIR, exist_ok=True)
        os.makedirs(CONFIG.LOG_DIR, exist_ok=True)
        
        time.sleep(1)
        self.cleanup_performed = True
        self.logger.info("âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ")
    
    def start_federated_learning(self):
        """å¯åŠ¨è”é‚¦å­¦ä¹ """
        self.logger.info(f"ğŸš€ å¯åŠ¨Ray V2è”é‚¦å­¦ä¹ ")
        self.logger.info(f"ğŸ“Š é…ç½®: {CONFIG.CLIENT_NUM}ä¸ªå®¢æˆ·ç«¯, {CONFIG.TOTAL_ROUNDS}è½®è®­ç»ƒ, æ€»èŠ‚ç‚¹æ•°: {CONFIG.CLIENT_NUM + 1}")
        
        # æ¸…ç†ç¯å¢ƒ
        self.cleanup_environment()
        
        # åˆå§‹åŒ–Ray
        self.initialize_ray_cluster()
        
        # GPUèµ„æºåˆ†é…
        gpu_allocation = self.allocate_gpu_resources()
        server_gpu = gpu_allocation[0]
        client_gpus = gpu_allocation[1:]
        
        # ç”Ÿæˆé…ç½®
        base_config = self.generate_base_config()
        
        # å¯åŠ¨æœåŠ¡å™¨
        server_config = base_config.copy()
        server_config['distribute']['role'] = 'server'
        
        server_resources = {"num_cpus": 2}
        if server_gpu is not None:
            server_resources["num_gpus"] = 1
        
        self.server_actor = RayFederatedScopeServer.options(**server_resources).remote(
            server_config, server_gpu
        )
        
        server_ip, server_port = ray.get(self.server_actor.start.remote())
        self.server_info = (server_ip, server_port)
        
        self.logger.info(f"âœ… æœåŠ¡å™¨å·²å¯åŠ¨: {server_ip}:{server_port}")
        time.sleep(3)
        
        # å¯åŠ¨å®¢æˆ·ç«¯
        for i in range(CONFIG.CLIENT_NUM):
            client_id = i + 1
            client_gpu = client_gpus[i]
            
            client_config = base_config.copy()
            client_config['distribute']['role'] = 'client'
            
            client_resources = {"num_cpus": 1}
            if client_gpu is not None:
                # è®¡ç®—æ¯ä¸ªGPUä¸Šçš„å®¢æˆ·ç«¯æ•°é‡ï¼Œåˆç†åˆ†é…GPUèµ„æº
                available_gpus = int(ray.cluster_resources().get('GPU', 0))
                clients_per_gpu = (CONFIG.CLIENT_NUM + available_gpus - 1) // available_gpus  # å‘ä¸Šå–æ•´
                gpu_fraction = 1.0 / clients_per_gpu
                client_resources["num_gpus"] = min(gpu_fraction, 0.8)  # æœ€å¤šä½¿ç”¨80%çš„GPUèµ„æº
            
            client_actor = RayFederatedScopeClient.options(**client_resources).remote(
                client_id, client_config, server_ip, server_port, client_gpu
            )
            
            self.client_actors.append(client_actor)
            ray.get(client_actor.start.remote())
            time.sleep(2)
        
        self.logger.info(f"âœ… æ‰€æœ‰{CONFIG.CLIENT_NUM}ä¸ªå®¢æˆ·ç«¯å·²å¯åŠ¨å®Œæˆ")
        
        # ç›‘æ§è®­ç»ƒ
        self.monitor_training()
        
    def monitor_training(self):
        """ç›‘æ§è®­ç»ƒè¿›åº¦"""
        self.logger.info(f"ğŸ“Š å¼€å§‹ç›‘æ§è®­ç»ƒï¼ˆ{CONFIG.MONITOR_DURATION}ç§’ï¼‰...")
        
        start_time = time.time()
        
        while True:
            elapsed = int(time.time() - start_time)
            
            if elapsed > CONFIG.MONITOR_DURATION:
                self.logger.info("â° ç›‘æ§æ—¶é—´ç»“æŸ")
                break
            
            # æ£€æŸ¥çŠ¶æ€
            server_status = ray.get(self.server_actor.get_status.remote())
            client_statuses = ray.get([
                actor.get_status.remote() for actor in self.client_actors
            ])
            
            running_clients = sum(1 for s in client_statuses if s["status"] == "running")
            
            # èµ„æºä½¿ç”¨æƒ…å†µ
            cluster_resources = ray.cluster_resources()
            available_resources = ray.available_resources()
            gpu_used = cluster_resources.get('GPU', 0) - available_resources.get('GPU', 0)
            
            self.logger.info(
                f"â° {elapsed}s | æœåŠ¡å™¨: {server_status['status']} | "
                f"å®¢æˆ·ç«¯: {running_clients}/{CONFIG.CLIENT_NUM} | "
                f"GPU: {gpu_used:.1f}/{cluster_resources.get('GPU', 0)}"
            )
            
            # æ£€æŸ¥è®­ç»ƒå®Œæˆ
            if server_status["status"] != "running":
                self.logger.info("ğŸ æœåŠ¡å™¨è®­ç»ƒå®Œæˆ")
                break
            
            if running_clients == 0:
                self.logger.info("ğŸ æ‰€æœ‰å®¢æˆ·ç«¯è®­ç»ƒå®Œæˆ")
                break
            
            time.sleep(10)
    
    def stop_all(self):
        """åœæ­¢æ‰€æœ‰è¿›ç¨‹"""
        self.logger.info("ğŸ›‘ åœæ­¢æ‰€æœ‰è”é‚¦å­¦ä¹ è¿›ç¨‹...")
        
        if self.server_actor:
            ray.get(self.server_actor.stop.remote())
        
        if self.client_actors:
            ray.get([actor.stop.remote() for actor in self.client_actors])
        
        self.logger.info("âœ… æ‰€æœ‰è¿›ç¨‹å·²åœæ­¢")
        
    def generate_results_summary(self):
        """ç”Ÿæˆç»“æœæ‘˜è¦"""
        self.logger.info("ğŸ“ˆ ç”Ÿæˆç»“æœæ‘˜è¦...")
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
        log_files = {
            'server': f"{CONFIG.LOG_DIR}/server.log",
            'clients': [f"{CONFIG.LOG_DIR}/client_{i}.log" for i in range(1, CONFIG.CLIENT_NUM + 1)]
        }
        
        summary = {
            'configuration': {
                'client_num': CONFIG.CLIENT_NUM,
                'total_rounds': CONFIG.TOTAL_ROUNDS,
                'dataset': CONFIG.DATASET,
                'model': CONFIG.MODEL_TYPE,
                'chunk_num': CONFIG.CHUNK_NUM,
                'importance_method': CONFIG.IMPORTANCE_METHOD
            },
            'log_files': log_files,
            'output_directories': {
                'server': f"{CONFIG.OUTPUT_DIR}/server_output",
                'clients': [f"{CONFIG.OUTPUT_DIR}/client_{i}_output" for i in range(1, CONFIG.CLIENT_NUM + 1)]
            }
        }
        
        # ä¿å­˜æ‘˜è¦
        summary_file = f"{CONFIG.OUTPUT_DIR}/results_summary.yaml"
        with open(summary_file, 'w') as f:
            yaml.safe_dump(summary, f)
        
        self.logger.info(f"ğŸ“„ ç»“æœæ‘˜è¦å·²ä¿å­˜: {summary_file}")
        
        return summary

# ============================================================================
# ğŸ¬ ä¸»ç¨‹åºå…¥å£
# ============================================================================

def display_banner():
    """æ˜¾ç¤ºå¯åŠ¨æ¨ªå¹…"""
    banner = f"""
{'='*80}
ğŸš€ Ray-Powered FederatedScope V2 Script
{'='*80}
ğŸ“Š é…ç½®ä¿¡æ¯:
   â€¢ å®¢æˆ·ç«¯æ•°é‡: {CONFIG.CLIENT_NUM}
   â€¢ è®­ç»ƒè½®æ•°: {CONFIG.TOTAL_ROUNDS}
   â€¢ æ•°æ®é›†: {CONFIG.DATASET}
   â€¢ æ¨¡å‹: {CONFIG.MODEL_TYPE}
   â€¢ Chunkæ•°: {CONFIG.CHUNK_NUM}
   â€¢ é‡è¦åº¦æ–¹æ³•: {CONFIG.IMPORTANCE_METHOD}
   â€¢ ç›‘æ§æ—¶é•¿: {CONFIG.MONITOR_DURATION}s

ğŸ’¡ è¾“å‡ºç›®å½•: {CONFIG.OUTPUT_DIR}
ğŸ“ æ—¥å¿—ç›®å½•: {CONFIG.LOG_DIR}
{'='*80}
"""
    print(banner)

def main():
    """ä¸»å‡½æ•°"""
    display_banner()
    
    ray_fl = RayV2FederatedLearning()
    
    try:
        # å¯åŠ¨è”é‚¦å­¦ä¹ 
        ray_fl.start_federated_learning()
        
        # ç”Ÿæˆç»“æœæ‘˜è¦
        summary = ray_fl.generate_results_summary()
        
        print("\nğŸ‰ Ray V2 è”é‚¦å­¦ä¹ å®Œæˆï¼")
        print(f"ğŸ“„ ç»“æœæ‘˜è¦: {CONFIG.OUTPUT_DIR}/results_summary.yaml")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {CONFIG.LOG_DIR}/")
        
        if CONFIG.ENABLE_RAY_DASHBOARD:
            print(f"ğŸŒ Ray Dashboard: http://127.0.0.1:8265")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        ray_fl.stop_all()
        ray.shutdown()
        print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

if __name__ == "__main__":
    main()