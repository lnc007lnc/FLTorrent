#!/usr/bin/env python3
"""
æµ‹è¯•chunkæ¢å¤åŠŸèƒ½
éªŒè¯ä¿å­˜çš„chunksèƒ½å¦å®Œå…¨æ¢å¤åŸå§‹æ¨¡å‹
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from federatedscope.core.chunk_manager import ChunkManager
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestModel(nn.Module):
    """æµ‹è¯•ç”¨çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
    def __init__(self):
        super(TestModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.fc1 = nn.Linear(32 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.dropout = nn.Dropout(0.5)
        self.bn1 = nn.BatchNorm2d(16)
        self.bn2 = nn.BatchNorm2d(32)
        
    def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def compare_models(model1, model2, tolerance=1e-6):
    """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°æ˜¯å¦å®Œå…¨ä¸€è‡´"""
    params1 = ChunkManager.model_to_params(model1)
    params2 = ChunkManager.model_to_params(model2)
    
    if set(params1.keys()) != set(params2.keys()):
        print("âŒ æ¨¡å‹å‚æ•°åç§°ä¸åŒ¹é…")
        return False, {}
    
    differences = {}
    all_match = True
    
    for name in params1.keys():
        p1, p2 = params1[name], params2[name]
        
        if p1.shape != p2.shape:
            print(f"âŒ å‚æ•° {name} å½¢çŠ¶ä¸åŒ¹é…: {p1.shape} vs {p2.shape}")
            all_match = False
            differences[name] = {'error': 'shape_mismatch', 'shapes': (p1.shape, p2.shape)}
            continue
        
        # è®¡ç®—ç»å¯¹å·®å¼‚
        abs_diff = np.abs(p1 - p2)
        max_diff = np.max(abs_diff)
        mean_diff = np.mean(abs_diff)
        
        differences[name] = {
            'max_diff': max_diff,
            'mean_diff': mean_diff,
            'shape': p1.shape,
            'total_elements': p1.size
        }
        
        if max_diff > tolerance:
            print(f"âŒ å‚æ•° {name} å·®å¼‚è¶…è¿‡å®¹å¿åº¦: max_diff={max_diff:.2e} > {tolerance:.2e}")
            all_match = False
    
    return all_match, differences


def test_chunk_recovery_accuracy():
    """æµ‹è¯•chunkæ¢å¤çš„å‡†ç¡®æ€§"""
    print("ğŸ§ª æµ‹è¯•chunkæ¢å¤å‡†ç¡®æ€§...")
    print("=" * 60)
    
    try:
        # åˆ›å»ºå¹¶åˆå§‹åŒ–åŸå§‹æ¨¡å‹
        original_model = TestModel()
        
        # ç”¨å›ºå®šçš„éšæœºç§å­åˆå§‹åŒ–ï¼Œç¡®ä¿å¯é‡ç°
        torch.manual_seed(42)
        with torch.no_grad():
            for param in original_model.parameters():
                param.data.normal_(0, 0.1)
        
        print(f"ğŸ“Š åŸå§‹æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        total_params = sum(p.numel() for p in original_model.parameters())
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        
        for name, param in original_model.named_parameters():
            print(f"   {name}: {param.shape} ({param.numel():,} ä¸ªå…ƒç´ )")
        
        # åˆ›å»ºChunkManagerå¹¶ä¿å­˜chunks
        chunk_manager = ChunkManager(client_id=100)
        
        print(f"\nğŸ”ª æµ‹è¯•ä¸åŒchunkæ•°é‡çš„æ¢å¤æ•ˆæœ:")
        
        for num_chunks in [3, 5, 8, 12]:
            print(f"\n--- æµ‹è¯• {num_chunks} ä¸ªchunks ---")
            
            # ä¿å­˜æ¨¡å‹ä¸ºchunks
            saved_hashes = chunk_manager.save_model_chunks(
                model=original_model,
                round_num=num_chunks,  # ä½¿ç”¨chunkæ•°é‡ä½œä¸ºè½®æ¬¡åŒºåˆ†
                num_chunks=num_chunks
            )
            
            print(f"âœ… ä¿å­˜äº† {len(saved_hashes)} ä¸ªchunks")
            
            # åˆ›å»ºæ–°æ¨¡å‹ç”¨äºæ¢å¤
            recovered_model = TestModel()
            
            # ä»chunksæ¢å¤æ¨¡å‹
            success = chunk_manager.reconstruct_model_from_chunks(
                round_num=num_chunks,
                target_model=recovered_model
            )
            
            if not success:
                print(f"âŒ {num_chunks} chunksæ¢å¤å¤±è´¥")
                continue
            
            # æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œæ¢å¤æ¨¡å‹
            models_match, differences = compare_models(original_model, recovered_model)
            
            if models_match:
                print(f"âœ… {num_chunks} chunkså®Œç¾æ¢å¤ - æ‰€æœ‰å‚æ•°å®Œå…¨ä¸€è‡´")
            else:
                print(f"âš ï¸ {num_chunks} chunksæ¢å¤æœ‰å·®å¼‚:")
                for name, diff in differences.items():
                    if 'error' in diff:
                        print(f"   {name}: {diff['error']}")
                    else:
                        print(f"   {name}: max_diff={diff['max_diff']:.2e}, mean_diff={diff['mean_diff']:.2e}")
            
            # éªŒè¯æ¨¡å‹åŠŸèƒ½æ€§ - ä½¿ç”¨ç›¸åŒè¾“å…¥æµ‹è¯•å‰å‘ä¼ æ’­
            test_input = torch.randn(1, 3, 32, 32)
            
            with torch.no_grad():
                original_output = original_model(test_input)
                recovered_output = recovered_model(test_input)
                
                output_diff = torch.abs(original_output - recovered_output).max().item()
                
                if output_diff < 1e-6:
                    print(f"âœ… å‰å‘ä¼ æ’­è¾“å‡ºå®Œå…¨ä¸€è‡´ (diff: {output_diff:.2e})")
                else:
                    print(f"âš ï¸ å‰å‘ä¼ æ’­è¾“å‡ºæœ‰å·®å¼‚ (max_diff: {output_diff:.2e})")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_individual_chunk_integrity():
    """æµ‹è¯•å•ä¸ªchunkçš„å®Œæ•´æ€§"""
    print(f"\nğŸ” æµ‹è¯•å•ä¸ªchunkå®Œæ•´æ€§...")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = TestModel()
        torch.manual_seed(123)
        with torch.no_grad():
            for param in model.parameters():
                param.data.normal_(0, 0.05)
        
        chunk_manager = ChunkManager(client_id=200)
        
        # ä¿å­˜ä¸º6ä¸ªchunks
        num_chunks = 6
        saved_hashes = chunk_manager.save_model_chunks(model, round_num=1, num_chunks=num_chunks)
        
        print(f"âœ… ä¿å­˜äº† {len(saved_hashes)} ä¸ªchunks")
        
        # è·å–åŸå§‹å‚æ•°
        original_params = ChunkManager.model_to_params(model)
        chunk_infos = ChunkManager.split_model(original_params, num_chunks)
        
        print(f"\nğŸ“‹ éªŒè¯æ¯ä¸ªchunkçš„æ•°æ®å®Œæ•´æ€§:")
        
        total_recovered_elements = 0
        
        for i in range(num_chunks):
            # ä»æ•°æ®åº“è·å–chunk
            chunk_data = chunk_manager.get_chunk_by_id(round_num=1, chunk_id=i)
            
            if chunk_data is None:
                print(f"âŒ Chunk {i} æœªæ‰¾åˆ°")
                continue
                
            chunk_info, chunk_array = chunk_data
            
            # éªŒè¯chunkä¿¡æ¯
            expected_info = chunk_infos[i]
            
            if chunk_info['chunk_id'] != expected_info['chunk_id']:
                print(f"âŒ Chunk {i} IDä¸åŒ¹é…")
                continue
                
            if chunk_info['flat_size'] != expected_info['flat_size']:
                print(f"âŒ Chunk {i} å¤§å°ä¸åŒ¹é…: {chunk_info['flat_size']} vs {expected_info['flat_size']}")
                continue
            
            # é‡æ–°æå–åŸå§‹æ•°æ®è¿›è¡Œæ¯”è¾ƒ
            expected_data = chunk_manager.extract_chunk_data(original_params, expected_info)
            
            if np.allclose(chunk_array, expected_data, atol=1e-8):
                print(f"âœ… Chunk {i}: æ•°æ®å®Œæ•´ ({len(chunk_array)} ä¸ªå…ƒç´ )")
                total_recovered_elements += len(chunk_array)
            else:
                max_diff = np.max(np.abs(chunk_array - expected_data))
                print(f"âŒ Chunk {i}: æ•°æ®ä¸åŒ¹é… (max_diff: {max_diff:.2e})")
        
        # éªŒè¯æ€»å…ƒç´ æ•°é‡
        total_original_elements = sum(np.prod(v.shape) for v in original_params.values())
        print(f"\nğŸ“Š å…ƒç´ æ•°é‡éªŒè¯:")
        print(f"   åŸå§‹æ¨¡å‹æ€»å…ƒç´ : {total_original_elements:,}")
        print(f"   æ¢å¤çš„æ€»å…ƒç´ : {total_recovered_elements:,}")
        
        if total_recovered_elements == total_original_elements:
            print(f"âœ… å…ƒç´ æ•°é‡å®Œå…¨åŒ¹é…")
            return True
        else:
            print(f"âŒ å…ƒç´ æ•°é‡ä¸åŒ¹é…")
            return False
            
    except Exception as e:
        print(f"âŒ å•ä¸ªchunkå®Œæ•´æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_storage_persistence():
    """æµ‹è¯•å­˜å‚¨æŒä¹…æ€§ - é‡å¯åèƒ½å¦æ¢å¤"""
    print(f"\nğŸ’¾ æµ‹è¯•å­˜å‚¨æŒä¹…æ€§...")
    print("=" * 60)
    
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šä¿å­˜æ•°æ®
        print("é˜¶æ®µ1: ä¿å­˜æ•°æ®...")
        
        model1 = TestModel()
        torch.manual_seed(456)
        with torch.no_grad():
            for param in model1.parameters():
                param.data.uniform_(-0.1, 0.1)
        
        chunk_manager1 = ChunkManager(client_id=300)
        saved_hashes = chunk_manager1.save_model_chunks(model1, round_num=1, num_chunks=4)
        
        print(f"âœ… ç¬¬ä¸€ä¸ªChunkManagerä¿å­˜äº† {len(saved_hashes)} ä¸ªchunks")
        
        # è·å–å­˜å‚¨ç»Ÿè®¡
        stats1 = chunk_manager1.get_storage_stats()
        print(f"   å­˜å‚¨å¤§å°: {stats1.get('storage_size_mb', 0):.3f} MB")
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ¨¡æ‹Ÿé‡å¯ï¼Œåˆ›å»ºæ–°çš„ChunkManagerå®ä¾‹
        print("\né˜¶æ®µ2: æ¨¡æ‹Ÿé‡å¯...")
        
        chunk_manager2 = ChunkManager(client_id=300)  # ç›¸åŒçš„client_id
        
        # å°è¯•åŠ è½½æ•°æ®
        loaded_chunks = chunk_manager2.load_chunks_by_round(round_num=1)
        print(f"âœ… æ–°çš„ChunkManagerå®ä¾‹åŠ è½½äº† {len(loaded_chunks)} ä¸ªchunks")
        
        # æ¢å¤æ¨¡å‹
        model2 = TestModel()
        success = chunk_manager2.reconstruct_model_from_chunks(round_num=1, target_model=model2)
        
        if not success:
            print("âŒ æ¨¡å‹æ¢å¤å¤±è´¥")
            return False
        
        # æ¯”è¾ƒæ¨¡å‹
        models_match, differences = compare_models(model1, model2)
        
        if models_match:
            print("âœ… é‡å¯åå®Œç¾æ¢å¤ - å­˜å‚¨æŒä¹…æ€§éªŒè¯æˆåŠŸ")
            return True
        else:
            print("âŒ é‡å¯åæ¢å¤æœ‰å·®å¼‚")
            for name, diff in differences.items():
                if 'error' not in diff:
                    print(f"   {name}: max_diff={diff['max_diff']:.2e}")
            return False
            
    except Exception as e:
        print(f"âŒ å­˜å‚¨æŒä¹…æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æ¢å¤æµ‹è¯•"""
    print("ğŸš€ Chunkæ¢å¤åŠŸèƒ½æµ‹è¯•å¥—ä»¶")
    print("=" * 80)
    
    tests = [
        ("Chunkæ¢å¤å‡†ç¡®æ€§", test_chunk_recovery_accuracy),
        ("å•ä¸ªChunkå®Œæ•´æ€§", test_individual_chunk_integrity),
        ("å­˜å‚¨æŒä¹…æ€§", test_storage_persistence)
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        print(f"\nğŸ§ª {test_name}")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} - é€šè¿‡")
            else:
                failed += 1
                print(f"âŒ {test_name} - å¤±è´¥")
        except Exception as e:
            failed += 1
            print(f"âŒ {test_name} - å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 80)
    print(f"ğŸ æµ‹è¯•ç»“æœ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰chunkæ¢å¤æµ‹è¯•é€šè¿‡! ç³»ç»Ÿå¯ä»¥å®Œç¾æ¢å¤æ¨¡å‹!")
    else:
        print("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥chunkæ¢å¤é€»è¾‘")
    
    return failed == 0


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)