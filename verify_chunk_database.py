#!/usr/bin/env python3
"""
éªŒè¯æœ¬åœ°tmpæ•°æ®åº“ä¸­chunkæ•°æ®çš„å®Œæ•´æ€§å’Œå¯è¯»æ€§
æ£€æŸ¥æ¯ä¸ªè½®æ¬¡æ˜¯å¦å­˜å‚¨äº†æ‰€æœ‰èŠ‚ç‚¹çš„chunkä¿¡æ¯
"""

import sqlite3
import os
import json
from collections import defaultdict, Counter

def connect_to_database(db_path):
    """è¿æ¥åˆ°SQLiteæ•°æ®åº“"""
    try:
        conn = sqlite3.connect(db_path)
        return conn
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ {db_path}: {e}")
        return None

def get_database_schema(conn):
    """è·å–æ•°æ®åº“è¡¨ç»“æ„"""
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        
        schema_info = {}
        for table in tables:
            table_name = table[0]
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()
            schema_info[table_name] = columns
            
        return schema_info
    except Exception as e:
        print(f"âŒ è·å–æ•°æ®åº“ç»“æ„å¤±è´¥: {e}")
        return {}

def analyze_chunk_data(conn, client_id):
    """åˆ†æchunkæ•°æ®çš„å®Œæ•´æ€§"""
    try:
        cursor = conn.cursor()
        
        # è·å–æ‰€æœ‰chunkè®°å½•
        cursor.execute("""
            SELECT round_num, client_id, chunk_id, action, chunk_hash, 
                   chunk_size, timestamp, chunk_data 
            FROM chunks 
            ORDER BY round_num, client_id, chunk_id
        """)
        
        records = cursor.fetchall()
        print(f"\nğŸ“Š å®¢æˆ·ç«¯ {client_id} æ•°æ®åº“åˆ†æ:")
        print(f"   æ€»è®°å½•æ•°: {len(records)}")
        
        if not records:
            print("   âš ï¸ æ•°æ®åº“ä¸ºç©ºï¼Œæ²¡æœ‰chunkè®°å½•")
            return {}
        
        # æŒ‰è½®æ¬¡ç»„ç»‡æ•°æ®
        round_data = defaultdict(lambda: defaultdict(list))
        client_distribution = defaultdict(Counter)
        action_distribution = defaultdict(Counter)
        
        for record in records:
            round_num, src_client_id, chunk_id, action, chunk_hash, chunk_size, timestamp, chunk_data = record
            
            round_data[round_num][src_client_id].append({
                'chunk_id': chunk_id,
                'action': action,
                'chunk_hash': chunk_hash,
                'chunk_size': chunk_size,
                'timestamp': timestamp,
                'has_data': chunk_data is not None and len(chunk_data) > 0
            })
            
            client_distribution[round_num][src_client_id] += 1
            action_distribution[round_num][action] += 1
        
        # è¾“å‡ºè½®æ¬¡åˆ†æ
        print(f"\nğŸ” è½®æ¬¡åˆ†æ:")
        for round_num in sorted(round_data.keys()):
            round_chunks = round_data[round_num]
            total_chunks = sum(len(chunks) for chunks in round_chunks.values())
            unique_clients = len(round_chunks.keys())
            
            print(f"   è½®æ¬¡ {round_num}:")
            print(f"     - æ€»chunkæ•°: {total_chunks}")
            print(f"     - æ¶‰åŠå®¢æˆ·ç«¯æ•°: {unique_clients}")
            print(f"     - å®¢æˆ·ç«¯åˆ†å¸ƒ: {dict(client_distribution[round_num])}")
            print(f"     - åŠ¨ä½œåˆ†å¸ƒ: {dict(action_distribution[round_num])}")
            
            # éªŒè¯æ˜¯å¦æœ‰å…¶ä»–å®¢æˆ·ç«¯çš„chunk
            other_clients = [cid for cid in round_chunks.keys() if cid != client_id]
            if other_clients:
                print(f"     âœ… åŒ…å«å…¶ä»–å®¢æˆ·ç«¯æ•°æ®: {other_clients}")
            else:
                print(f"     âš ï¸ åªæœ‰æœ¬å®¢æˆ·ç«¯({client_id})çš„æ•°æ®")
        
        return round_data
        
    except Exception as e:
        print(f"âŒ åˆ†æchunkæ•°æ®å¤±è´¥: {e}")
        return {}

def verify_data_integrity(conn):
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    try:
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        print(f"\nğŸ”§ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
        
        # 1. æ£€æŸ¥NULLå€¼
        cursor.execute("SELECT COUNT(*) FROM chunks WHERE chunk_hash IS NULL")
        null_hashes = cursor.fetchone()[0]
        print(f"   ç©ºchunk_hashæ•°é‡: {null_hashes}")
        
        # 2. æ£€æŸ¥chunkå¤§å°
        cursor.execute("SELECT MIN(chunk_size), MAX(chunk_size), AVG(chunk_size) FROM chunks WHERE chunk_size > 0")
        size_stats = cursor.fetchone()
        if size_stats[0] is not None:
            print(f"   Chunkå¤§å°ç»Ÿè®¡: æœ€å°={size_stats[0]}, æœ€å¤§={size_stats[1]}, å¹³å‡={size_stats[2]:.2f}")
        
        # 3. æ£€æŸ¥timestampèŒƒå›´
        cursor.execute("SELECT MIN(timestamp), MAX(timestamp) FROM chunks")
        time_range = cursor.fetchone()
        print(f"   æ—¶é—´æˆ³èŒƒå›´: {time_range[0]} - {time_range[1]}")
        
        # 4. æ£€æŸ¥é‡å¤è®°å½•
        cursor.execute("""
            SELECT round_num, client_id, chunk_id, COUNT(*) as cnt
            FROM chunks 
            GROUP BY round_num, client_id, chunk_id 
            HAVING COUNT(*) > 1
        """)
        duplicates = cursor.fetchall()
        print(f"   é‡å¤è®°å½•æ•°é‡: {len(duplicates)}")
        if duplicates:
            print(f"     é‡å¤è®°å½•: {duplicates[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_data_readability(conn):
    """æµ‹è¯•æ•°æ®å¯è¯»æ€§"""
    try:
        cursor = conn.cursor()
        
        print(f"\nğŸ“– æ•°æ®å¯è¯»æ€§æµ‹è¯•:")
        
        # è¯»å–ä¸€äº›æ ·æœ¬æ•°æ®
        cursor.execute("""
            SELECT round_num, client_id, chunk_id, action, chunk_hash, 
                   chunk_size, LENGTH(chunk_data) as data_length
            FROM chunks 
            LIMIT 5
        """)
        
        samples = cursor.fetchall()
        print(f"   æ ·æœ¬æ•°æ® (å‰5æ¡è®°å½•):")
        for i, sample in enumerate(samples, 1):
            round_num, client_id, chunk_id, action, chunk_hash, chunk_size, data_length = sample
            print(f"     {i}. è½®æ¬¡={round_num}, å®¢æˆ·ç«¯={client_id}, å—ID={chunk_id}")
            print(f"        åŠ¨ä½œ={action}, å“ˆå¸Œ={chunk_hash[:16] if chunk_hash else 'None'}...")
            print(f"        å¤§å°={chunk_size}, æ•°æ®é•¿åº¦={data_length}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¯è¯»æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def cross_client_analysis(all_data):
    """è·¨å®¢æˆ·ç«¯æ•°æ®åˆ†æ"""
    print(f"\nğŸŒ è·¨å®¢æˆ·ç«¯æ•°æ®åˆ†æ:")
    
    # ç»Ÿè®¡æ¯ä¸ªè½®æ¬¡åœ¨å„å®¢æˆ·ç«¯ä¸­çš„æ•°æ®åˆ†å¸ƒ
    all_rounds = set()
    for client_data in all_data.values():
        all_rounds.update(client_data.keys())
    
    print(f"   å‘ç°çš„è½®æ¬¡: {sorted(all_rounds)}")
    
    for round_num in sorted(all_rounds):
        print(f"\n   è½®æ¬¡ {round_num} è·¨å®¢æˆ·ç«¯åˆ†æ:")
        
        round_summary = {}
        for client_id, client_data in all_data.items():
            if round_num in client_data:
                round_chunks = client_data[round_num]
                total_chunks = sum(len(chunks) for chunks in round_chunks.values())
                unique_sources = list(round_chunks.keys())
                round_summary[f"å®¢æˆ·ç«¯{client_id}"] = {
                    'total_chunks': total_chunks,
                    'source_clients': unique_sources
                }
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        all_source_clients = set()
        for client_info in round_summary.values():
            all_source_clients.update(client_info['source_clients'])
        
        print(f"     æ‰€æœ‰å®¢æˆ·ç«¯éƒ½åº”è¯¥æœ‰æ¥è‡ªè¿™äº›æºçš„æ•°æ®: {sorted(all_source_clients)}")
        
        for client, info in round_summary.items():
            missing_sources = all_source_clients - set(info['source_clients'])
            if missing_sources:
                print(f"     âš ï¸ {client} ç¼ºå°‘æ¥è‡ªå®¢æˆ·ç«¯ {sorted(missing_sources)} çš„æ•°æ®")
            else:
                print(f"     âœ… {client} æ‹¥æœ‰æ‰€æœ‰æºå®¢æˆ·ç«¯çš„æ•°æ®")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹éªŒè¯chunkæ•°æ®åº“å®Œæ•´æ€§å’Œå¯è¯»æ€§")
    
    base_path = "/mnt/g/FLtorrent_combine/FederatedScope-master/tmp"
    client_dbs = {
        1: os.path.join(base_path, "client_1", "client_1_chunks.db"),
        2: os.path.join(base_path, "client_2", "client_2_chunks.db"), 
        3: os.path.join(base_path, "client_3", "client_3_chunks.db")
    }
    
    all_client_data = {}
    
    # åˆ†ææ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®åº“
    for client_id, db_path in client_dbs.items():
        print(f"\n{'='*60}")
        print(f"ğŸ” åˆ†æå®¢æˆ·ç«¯ {client_id} æ•°æ®åº“: {db_path}")
        
        if not os.path.exists(db_path):
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            continue
        
        conn = connect_to_database(db_path)
        if not conn:
            continue
        
        try:
            # è·å–æ•°æ®åº“ç»“æ„
            schema = get_database_schema(conn)
            print(f"ğŸ“‹ æ•°æ®åº“è¡¨ç»“æ„: {list(schema.keys())}")
            for table, columns in schema.items():
                print(f"   è¡¨ {table}: {[col[1] for col in columns]}")
            
            # åˆ†æchunkæ•°æ®
            client_data = analyze_chunk_data(conn, client_id)
            all_client_data[client_id] = client_data
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            verify_data_integrity(conn)
            
            # æµ‹è¯•æ•°æ®å¯è¯»æ€§
            test_data_readability(conn)
            
        finally:
            conn.close()
    
    # è·¨å®¢æˆ·ç«¯åˆ†æ
    if all_client_data:
        cross_client_analysis(all_client_data)
    
    print(f"\n{'='*60}")
    print("âœ… chunkæ•°æ®åº“éªŒè¯å®Œæˆ!")

if __name__ == "__main__":
    main()