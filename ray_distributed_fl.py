#!/usr/bin/env python3
"""
Ray-powered Distributed Federated Learning for FederatedScope
=============================================================

This module uses Ray to replace traditional multi-process FL training with:
- Automatic resource management (CPU/GPU allocation)  
- Dynamic cluster scaling (local + cloud servers)
- Smart task scheduling and fault tolerance
- Simplified network configuration

Key Features:
- Ray Actors for Server/Client processes
- Dynamic GPU assignment based on available resources
- Automatic IP/port management via Ray cluster
- Cloud server scaling support
- Resource monitoring and optimization
"""

import ray
import os
import sys
import time
import yaml
import psutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging

# Rayç›¸å…³å¯¼å…¥
from ray.util.placement_group import placement_group, placement_group_table
from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy

# FederatedScopeç›¸å…³å¯¼å…¥
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@ray.remote
class FederatedScopeServer:
    """
    Ray Actor for FederatedScope Server
    è‡ªåŠ¨ç®¡ç†æœåŠ¡å™¨è¿›ç¨‹ï¼Œæ”¯æŒåŠ¨æ€èµ„æºåˆ†é…
    """
    
    def __init__(self, config_path: str, gpu_id: Optional[int] = None):
        self.config_path = config_path
        self.gpu_id = gpu_id
        self.process = None
        self.node_ip = ray.util.get_node_ip_address()
        self.server_port = None
        
    def start(self) -> Tuple[str, int]:
        """å¯åŠ¨æœåŠ¡å™¨å¹¶è¿”å›IPå’Œç«¯å£"""
        # è¯»å–é…ç½®å¹¶åŠ¨æ€åˆ†é…ç«¯å£
        with open(self.config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Rayè‡ªåŠ¨åˆ†é…å¯ç”¨ç«¯å£
        import socket
        sock = socket.socket()
        sock.bind(('', 0))
        self.server_port = sock.getsockname()[1]
        sock.close()
        
        # æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„æœåŠ¡å™¨åœ°å€
        config['distribute']['server_host'] = self.node_ip
        config['distribute']['server_port'] = self.server_port
        
        # å¦‚æœæŒ‡å®šäº†GPUï¼Œæ›´æ–°è®¾å¤‡é…ç½®
        if self.gpu_id is not None:
            config['device'] = self.gpu_id
            config['use_gpu'] = True
        
        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        updated_config_path = self.config_path.replace('.yaml', f'_ray_server.yaml')
        with open(updated_config_path, 'w') as f:
            yaml.safe_dump(config, f)
        
        # å¯åŠ¨FederatedScopeæœåŠ¡å™¨
        cmd = [
            sys.executable, 'federatedscope/main.py',
            '--cfg', updated_config_path
        ]
        
        env = os.environ.copy()
        env['PYTHONPATH'] = '.'
        if self.gpu_id is not None:
            env['CUDA_VISIBLE_DEVICES'] = str(self.gpu_id)
        
        self.process = subprocess.Popen(
            cmd, 
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, 
            env=env
        )
        
        logger.info(f"ğŸ–¥ï¸ Server started on {self.node_ip}:{self.server_port} (GPU: {self.gpu_id})")
        return self.node_ip, self.server_port
    
    def get_status(self) -> Dict:
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        if self.process is None:
            return {"status": "not_started"}
        
        poll = self.process.poll()
        if poll is None:
            return {
                "status": "running",
                "pid": self.process.pid,
                "node_ip": self.node_ip,
                "server_port": self.server_port,
                "gpu_id": self.gpu_id
            }
        else:
            return {"status": "stopped", "returncode": poll}
    
    def stop(self):
        """åœæ­¢æœåŠ¡å™¨"""
        if self.process and self.process.poll() is None:
            self.process.terminate()
            self.process.wait()
            logger.info(f"ğŸ›‘ Server stopped")

@ray.remote
class FederatedScopeClient:
    """
    Ray Actor for FederatedScope Client  
    è‡ªåŠ¨è¿æ¥åˆ°Rayé›†ç¾¤ä¸­çš„æœåŠ¡å™¨ï¼Œæ”¯æŒåŠ¨æ€GPUåˆ†é…
    """
    
    def __init__(self, client_id: int, config_path: str, 
                 server_ip: str, server_port: int, gpu_id: Optional[int] = None):
        self.client_id = client_id
        self.config_path = config_path
        self.server_ip = server_ip
        self.server_port = server_port
        self.gpu_id = gpu_id
        self.process = None
        self.node_ip = ray.util.get_node_ip_address()
        self.client_port = None
        
    def start(self) -> bool:
        """å¯åŠ¨å®¢æˆ·ç«¯"""
        # è¯»å–é…ç½®
        with open(self.config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Rayè‡ªåŠ¨åˆ†é…å®¢æˆ·ç«¯ç«¯å£
        import socket
        sock = socket.socket()
        sock.bind(('', 0))
        self.client_port = sock.getsockname()[1]
        sock.close()
        
        # æ›´æ–°é…ç½®ï¼šè¿æ¥åˆ°RayæœåŠ¡å™¨
        config['distribute']['server_host'] = self.server_ip
        config['distribute']['server_port'] = self.server_port
        config['distribute']['client_host'] = self.node_ip
        config['distribute']['client_port'] = self.client_port
        config['distribute']['data_idx'] = self.client_id
        
        # GPUé…ç½®
        if self.gpu_id is not None:
            config['device'] = self.gpu_id
            config['use_gpu'] = True
        
        # ä¿å­˜æ›´æ–°é…ç½®
        updated_config_path = self.config_path.replace('.yaml', f'_ray_client_{self.client_id}.yaml')
        with open(updated_config_path, 'w') as f:
            yaml.safe_dump(config, f)
        
        # å¯åŠ¨å®¢æˆ·ç«¯
        cmd = [
            sys.executable, 'federatedscope/main.py',
            '--cfg', updated_config_path
        ]
        
        env = os.environ.copy()
        env['PYTHONPATH'] = '.'
        if self.gpu_id is not None:
            env['CUDA_VISIBLE_DEVICES'] = str(self.gpu_id)
        
        self.process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE,
            env=env
        )
        
        logger.info(f"ğŸ‘¤ Client {self.client_id} started on {self.node_ip}:{self.client_port} (GPU: {self.gpu_id})")
        return True
    
    def get_status(self) -> Dict:
        """è·å–å®¢æˆ·ç«¯çŠ¶æ€"""
        if self.process is None:
            return {"status": "not_started"}
        
        poll = self.process.poll()
        if poll is None:
            return {
                "status": "running",
                "pid": self.process.pid,
                "client_id": self.client_id,
                "node_ip": self.node_ip,
                "client_port": self.client_port,
                "gpu_id": self.gpu_id
            }
        else:
            return {"status": "stopped", "returncode": poll}
    
    def stop(self):
        """åœæ­¢å®¢æˆ·ç«¯"""
        if self.process and self.process.poll() is None:
            self.process.terminate()
            self.process.wait()
            logger.info(f"ğŸ›‘ Client {self.client_id} stopped")

class RayFederatedLearning:
    """
    Ray-powered Federated Learning Orchestrator
    ==========================================
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - è‡ªåŠ¨GPUèµ„æºæ£€æµ‹å’Œåˆ†é…
    - åŠ¨æ€é›†ç¾¤æ‰©å±•ï¼ˆæ”¯æŒäº‘æœåŠ¡å™¨åŠ å…¥ï¼‰
    - æ™ºèƒ½å®¹é”™å’Œé‡å¯æœºåˆ¶
    - å®æ—¶èµ„æºç›‘æ§
    """
    
    def __init__(self, config_dir: str = "multi_process_test_v2/configs"):
        self.config_dir = config_dir
        self.server_actor = None
        self.client_actors = []
        self.server_info = None
        
    def initialize_ray_cluster(self, num_cpus: Optional[int] = None, 
                             num_gpus: Optional[int] = None,
                             address: Optional[str] = None):
        """
        åˆå§‹åŒ–Rayé›†ç¾¤
        
        Args:
            num_cpus: æŒ‡å®šCPUæ•°é‡ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹ï¼‰
            num_gpus: æŒ‡å®šGPUæ•°é‡ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹ï¼‰  
            address: Rayé›†ç¾¤åœ°å€ï¼ˆNone=æœ¬åœ°æ¨¡å¼ï¼Œ"auto"=è‡ªåŠ¨å‘ç°ï¼Œ"ray://host:port"=è¿æ¥è¿œç¨‹ï¼‰
        """
        # è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶èµ„æº
        if num_cpus is None:
            num_cpus = psutil.cpu_count()
        
        if num_gpus is None:
            try:
                import torch
                num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
            except ImportError:
                num_gpus = 0
        
        # åˆå§‹åŒ–Ray
        if address is None:
            # æœ¬åœ°æ¨¡å¼
            ray.init(num_cpus=num_cpus, num_gpus=num_gpus, ignore_reinit_error=True)
            logger.info(f"ğŸš€ Rayæœ¬åœ°é›†ç¾¤åˆå§‹åŒ–: {num_cpus} CPUs, {num_gpus} GPUs")
        else:
            # è¿æ¥åˆ°ç°æœ‰é›†ç¾¤æˆ–è‡ªåŠ¨å‘ç°
            ray.init(address=address, ignore_reinit_error=True)
            logger.info(f"ğŸŒ Rayé›†ç¾¤è¿æ¥: {address}")
        
        # æ˜¾ç¤ºé›†ç¾¤èµ„æºä¿¡æ¯
        resources = ray.cluster_resources()
        logger.info(f"ğŸ“Š é›†ç¾¤èµ„æº: {dict(resources)}")
        
    def allocate_gpu_resources(self, num_clients: int) -> List[Optional[int]]:
        """
        æ™ºèƒ½GPUèµ„æºåˆ†é…
        
        Returns:
            List[Optional[int]]: æ¯ä¸ªè¿›ç¨‹åˆ†é…çš„GPU IDï¼ŒNoneè¡¨ç¤ºä½¿ç”¨CPU
        """
        cluster_resources = ray.cluster_resources()
        available_gpus = int(cluster_resources.get('GPU', 0))
        
        if available_gpus == 0:
            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
            return [None] * (num_clients + 1)  # +1 for server
        
        # GPUåˆ†é…ç­–ç•¥ï¼šæœåŠ¡å™¨ä¼˜å…ˆï¼Œç„¶åå®¢æˆ·ç«¯è½®æ¢
        gpu_allocation = []
        
        if available_gpus >= 1:
            # æœåŠ¡å™¨åˆ†é…GPU 0
            gpu_allocation.append(0)
        else:
            gpu_allocation.append(None)
            
        # å®¢æˆ·ç«¯GPUåˆ†é…
        for i in range(num_clients):
            if available_gpus > 1:
                # å¤šGPUï¼šå®¢æˆ·ç«¯è½®æ¢ä½¿ç”¨GPU 1, 2, 3...
                gpu_id = 1 + (i % (available_gpus - 1))
                gpu_allocation.append(gpu_id)
            elif available_gpus == 1:
                # å•GPUï¼šå®¢æˆ·ç«¯å…±äº«æˆ–ä½¿ç”¨CPU
                gpu_allocation.append(None)  # é¿å…ä¸æœåŠ¡å™¨å†²çª
            else:
                gpu_allocation.append(None)
        
        logger.info(f"ğŸ¯ GPUåˆ†é…ç­–ç•¥: Server=GPU{gpu_allocation[0]}, Clients={gpu_allocation[1:]}")
        return gpu_allocation
    
    def start_federated_learning(self, num_clients: int = 3, 
                                total_rounds: int = 3,
                                monitor_duration: int = 600):
        """
        å¯åŠ¨Rayé©±åŠ¨çš„è”é‚¦å­¦ä¹ 
        
        Args:
            num_clients: å®¢æˆ·ç«¯æ•°é‡
            total_rounds: è®­ç»ƒè½®æ•°  
            monitor_duration: ç›‘æ§æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        logger.info(f"ğŸ¯ å¼€å§‹Rayè”é‚¦å­¦ä¹ : {num_clients}å®¢æˆ·ç«¯, {total_rounds}è½®è®­ç»ƒ")
        
        # 1. GPUèµ„æºåˆ†é…
        gpu_allocation = self.allocate_gpu_resources(num_clients)
        server_gpu = gpu_allocation[0]
        client_gpus = gpu_allocation[1:]
        
        # 2. å¯åŠ¨æœåŠ¡å™¨Actor
        server_config = os.path.join(self.config_dir, "server.yaml")
        
        # æœåŠ¡å™¨èµ„æºè§„æ ¼
        server_resources = {"num_cpus": 2}
        if server_gpu is not None:
            server_resources["num_gpus"] = 1
            
        self.server_actor = FederatedScopeServer.options(
            **server_resources
        ).remote(server_config, server_gpu)
        
        # å¯åŠ¨æœåŠ¡å™¨å¹¶è·å–è¿æ¥ä¿¡æ¯
        server_ip, server_port = ray.get(self.server_actor.start.remote())
        self.server_info = (server_ip, server_port)
        
        logger.info(f"âœ… æœåŠ¡å™¨å·²å¯åŠ¨: {server_ip}:{server_port}")
        
        # ç­‰å¾…æœåŠ¡å™¨åˆå§‹åŒ–
        time.sleep(3)
        
        # 3. å¯åŠ¨å®¢æˆ·ç«¯Actors
        for i in range(num_clients):
            client_id = i + 1
            client_config = os.path.join(self.config_dir, f"client_{client_id}.yaml")
            client_gpu = client_gpus[i]
            
            # å®¢æˆ·ç«¯èµ„æºè§„æ ¼
            client_resources = {"num_cpus": 1}
            if client_gpu is not None:
                client_resources["num_gpus"] = 0.5  # å…±äº«GPUèµ„æº
            
            client_actor = FederatedScopeClient.options(
                **client_resources
            ).remote(client_id, client_config, server_ip, server_port, client_gpu)
            
            self.client_actors.append(client_actor)
            
            # å¯åŠ¨å®¢æˆ·ç«¯
            ray.get(client_actor.start.remote())
            time.sleep(2)  # é”™å³°å¯åŠ¨
        
        logger.info(f"âœ… æ‰€æœ‰{num_clients}ä¸ªå®¢æˆ·ç«¯å·²å¯åŠ¨")
        
        # 4. ç›‘æ§è®­ç»ƒè¿›åº¦
        self.monitor_training_progress(monitor_duration)
        
    def monitor_training_progress(self, monitor_duration: int):
        """å®æ—¶ç›‘æ§è®­ç»ƒè¿›åº¦å’Œèµ„æºä½¿ç”¨"""
        start_time = time.time()
        
        logger.info(f"ğŸ“Š å¼€å§‹ç›‘æ§è®­ç»ƒè¿›åº¦ï¼ˆ{monitor_duration}ç§’ï¼‰...")
        
        while True:
            elapsed = int(time.time() - start_time)
            
            if elapsed > monitor_duration:
                logger.info("â° ç›‘æ§æ—¶é—´ç»“æŸ")
                break
                
            # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
            server_status = ray.get(self.server_actor.get_status.remote())
            
            # æ£€æŸ¥å®¢æˆ·ç«¯çŠ¶æ€  
            client_statuses = ray.get([
                actor.get_status.remote() for actor in self.client_actors
            ])
            
            running_clients = sum(1 for status in client_statuses if status["status"] == "running")
            
            # Rayé›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ
            cluster_resources = ray.cluster_resources()
            available_resources = ray.available_resources()
            
            logger.info(f"â° è¿è¡Œ{elapsed}s | æœåŠ¡å™¨: {server_status['status']} | "
                       f"å®¢æˆ·ç«¯: {running_clients}/{len(self.client_actors)} | "
                       f"GPUä½¿ç”¨: {cluster_resources.get('GPU', 0) - available_resources.get('GPU', 0):.1f}/{cluster_resources.get('GPU', 0)}")
            
            # æ£€æŸ¥è®­ç»ƒæ˜¯å¦å®Œæˆ
            if server_status["status"] != "running":
                logger.info("ğŸ æœåŠ¡å™¨è®­ç»ƒå®Œæˆ")
                break
                
            if running_clients == 0:
                logger.info("ğŸ æ‰€æœ‰å®¢æˆ·ç«¯è®­ç»ƒå®Œæˆ")
                break
                
            time.sleep(10)
    
    def stop_all(self):
        """åœæ­¢æ‰€æœ‰Actor"""
        logger.info("ğŸ§¹ åœæ­¢æ‰€æœ‰è”é‚¦å­¦ä¹ è¿›ç¨‹...")
        
        # åœæ­¢æœåŠ¡å™¨
        if self.server_actor:
            ray.get(self.server_actor.stop.remote())
            
        # åœæ­¢æ‰€æœ‰å®¢æˆ·ç«¯
        if self.client_actors:
            ray.get([actor.stop.remote() for actor in self.client_actors])
        
        logger.info("âœ… æ‰€æœ‰è¿›ç¨‹å·²åœæ­¢")
    
    def get_cluster_info(self) -> Dict:
        """è·å–Rayé›†ç¾¤ä¿¡æ¯"""
        nodes = ray.nodes()
        cluster_resources = ray.cluster_resources()
        available_resources = ray.available_resources()
        
        return {
            "nodes": len(nodes),
            "total_resources": dict(cluster_resources),
            "available_resources": dict(available_resources),
            "node_details": [
                {
                    "node_id": node["NodeID"],
                    "alive": node["Alive"],
                    "node_ip": node.get("NodeManagerAddress", "N/A"),
                    "resources": node.get("Resources", {})
                }
                for node in nodes
            ]
        }

def main():
    """Rayè”é‚¦å­¦ä¹ ä¸»ç¨‹åº"""
    
    print("ğŸš€ Ray-Powered FederatedScope åˆ†å¸ƒå¼è”é‚¦å­¦ä¹ ")
    print("=" * 60)
    
    # åˆå§‹åŒ–Rayè”é‚¦å­¦ä¹ 
    ray_fl = RayFederatedLearning()
    
    try:
        # 1. åˆå§‹åŒ–Rayé›†ç¾¤ï¼ˆæœ¬åœ°æ¨¡å¼ï¼Œè‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶ï¼‰
        ray_fl.initialize_ray_cluster()
        
        # æ˜¾ç¤ºé›†ç¾¤ä¿¡æ¯
        cluster_info = ray_fl.get_cluster_info()
        print(f"ğŸŒ Rayé›†ç¾¤ä¿¡æ¯:")
        print(f"   èŠ‚ç‚¹æ•°: {cluster_info['nodes']}")
        print(f"   æ€»èµ„æº: {cluster_info['total_resources']}")
        print(f"   å¯ç”¨èµ„æº: {cluster_info['available_resources']}")
        
        # 2. å¯åŠ¨è”é‚¦å­¦ä¹ ï¼ˆ3å®¢æˆ·ç«¯ï¼Œ3è½®è®­ç»ƒï¼Œ10åˆ†é’Ÿç›‘æ§ï¼‰
        ray_fl.start_federated_learning(
            num_clients=3,
            total_rounds=3, 
            monitor_duration=600
        )
        
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·")
    except Exception as e:
        logger.error(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # 3. æ¸…ç†èµ„æº
        ray_fl.stop_all()
        ray.shutdown()
        
        print("ğŸ‰ Rayè”é‚¦å­¦ä¹ ç»“æŸï¼")

if __name__ == "__main__":
    main()